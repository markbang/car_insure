In [1]:

    
    
    # This Python 3 environment comes with many helpful analytics libraries installed
    # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
    # For example, here's several helpful packages to load
    
    import numpy as np # linear algebra
    import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
    
    # Input data files are available in the read-only "../input/" directory
    # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory
    
    import os
    for dirname, _, filenames in os.walk('/kaggle/input'):
        for filename in filenames:
            print(os.path.join(dirname, filename))
    
    # You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
    # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
    
    
    
    /kaggle/input/health-insurance-cross-sell-prediction/sample_submission.csv
    /kaggle/input/health-insurance-cross-sell-prediction/test.csv
    /kaggle/input/health-insurance-cross-sell-prediction/train.csv
    

In [2]:

    
    
    # Import useful libraries
    
    import time
    import re
    import string
    from numpy import mean
    from numpy import set_printoptions
    from sklearn.feature_selection import SelectKBest, f_classif
    
    import matplotlib.pyplot as plt
    plt.style.use('seaborn-darkgrid')
    import seaborn as sns
    from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold, KFold, GridSearchCV
    from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix, precision_recall_curve, auc, roc_curve, recall_score, classification_report 
    from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, RobustScaler
    from sklearn.utils.multiclass import type_of_target
    
    from catboost import CatBoostClassifier
    from xgboost import XGBClassifier
    from lightgbm import LGBMClassifier
    
    from collections import Counter
    from imblearn.over_sampling import RandomOverSampler
    from imblearn.under_sampling import RandomUnderSampler
    
    import warnings
    warnings.filterwarnings('ignore')
    

In [3]:

    
    
    # Read dataset
    
    train_data = pd.read_csv('/kaggle/input/health-insurance-cross-sell-prediction/train.csv')
    test_data = pd.read_csv('/kaggle/input/health-insurance-cross-sell-prediction/test.csv')
    train_data.columns = train_data.columns.str.lower().str.strip().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')
    test_data.columns = test_data.columns.str.lower().str.strip().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')
    

In [4]:

    
    
    print('Train Data Shape: ', train_data.shape)
    print('Test Data Shape: ', test_data.shape)
    train_data.head()
    
    
    
    Train Data Shape:  (381109, 12)
    Test Data Shape:  (127037, 11)
    

Out[4]:

| id | gender | age | driving_license | region_code | previously_insured | vehicle_age | vehicle_damage | annual_premium | policy_sales_channel | vintage | response  
---|---|---|---|---|---|---|---|---|---|---|---|---  
0 | 1 | Male | 44 | 1 | 28.0 | 0 | > 2 Years | Yes | 40454.0 | 26.0 | 217 | 1  
1 | 2 | Male | 76 | 1 | 3.0 | 0 | 1-2 Year | No | 33536.0 | 26.0 | 183 | 0  
2 | 3 | Male | 47 | 1 | 28.0 | 0 | > 2 Years | Yes | 38294.0 | 26.0 | 27 | 1  
3 | 4 | Male | 21 | 1 | 11.0 | 1 | < 1 Year | No | 28619.0 | 152.0 | 203 | 0  
4 | 5 | Female | 29 | 1 | 41.0 | 1 | < 1 Year | No | 27496.0 | 152.0 | 39 | 0  
  
In [5]:

    
    
    train_data.isnull().sum()
    

Out[5]:

    
    
    id                      0
    gender                  0
    age                     0
    driving_license         0
    region_code             0
    previously_insured      0
    vehicle_age             0
    vehicle_damage          0
    annual_premium          0
    policy_sales_channel    0
    vintage                 0
    response                0
    dtype: int64

# Exploratory Data Analysis¶

In [6]:

    
    
    train_data['response'].value_counts()
    

Out[6]:

    
    
    0    334399
    1     46710
    Name: response, dtype: int64

In [7]:

    
    
    train_data.nunique()
    

Out[7]:

    
    
    id                      381109
    gender                       2
    age                         66
    driving_license              2
    region_code                 53
    previously_insured           2
    vehicle_age                  3
    vehicle_damage               2
    annual_premium           48838
    policy_sales_channel       155
    vintage                    290
    response                     2
    dtype: int64

In [8]:

    
    
    fig, axes = plt.subplots(ncols = 2, figsize = (13, 3), dpi = 100)
    plt.tight_layout()
    
    train_data.groupby('response').count()['id'].plot(kind = 'pie', ax = axes[0], labels = ['Interested (87.7%)', 'Not Interested (12.1%)'])
    sns.countplot(x = train_data['response'], hue = train_data['response'], ax = axes[1])
    
    axes[0].set_ylabel('')
    axes[1].set_ylabel('')
    axes[1].set_xticklabels(['Interested (87.7%)', 'Not Interested (12.1%)'])
    axes[0].tick_params(axis = 'x', labelsize = 8)
    axes[0].tick_params(axis = 'y', labelsize = 8)
    axes[1].tick_params(axis = 'x', labelsize = 8)
    axes[1].tick_params(axis = 'y', labelsize = 8)
    
    axes[0].set_title('Label Distribution in Training Set', fontsize = 8)
    axes[1].set_title('Label Count in Training Set', fontsize =8)
    
    plt.show()
    

![](__results___files/__results___8_0.png)

In [9]:

    
    
    # looking at the frequency of records by age
    
    plt.rcParams['figure.figsize'] = (18, 7)
    color = plt.cm.copper(np.linspace(0, 1, 66))
    train_data['age'].value_counts().head(66).plot.bar(color = color)
    plt.title('Age distribution (Most policy holders are young. Age is highly skewed)', fontsize = 15)
    plt.xticks(rotation = 90)
    plt.show()
    

![](__results___files/__results___9_0.png)

## Combine Training and Test Data for additional visualizations¶

In [10]:

    
    
    train_data['type'] = 'train'
    test_data['type'] = 'test'
    
    master_data = pd.concat([train_data, test_data])
    

In [11]:

    
    
    plt.figure(figsize = (8, 5))
    sns.distplot(master_data['annual_premium'])
    plt.title('Annual Premium distribution (Highly skewed to the right)', fontsize = 15)
    plt.show()
    

![](__results___files/__results___12_0.png)

In [12]:

    
    
    plt.figure(figsize = (15, 6))
    sns.distplot(master_data.loc[(master_data['gender'] == 'Male'), 'age'], kde_kws = {"color": "b", "lw": 1, "label": "Male"})
    sns.distplot(master_data.loc[(master_data['gender'] == 'Female'), 'age'], kde_kws = {"color": "r", "lw": 1, "label": "Female"})
    plt.title('Age distribution by Gender', fontsize = 15)
    plt.show()
    

![](__results___files/__results___13_0.png)

In [13]:

    
    
    plt.figure(figsize = (15, 6))
    sns.distplot(master_data.loc[(master_data['gender'] == 'Male'), 'annual_premium'], kde_kws = {"color": "b", "lw": 1, "label": "Male"})
    sns.distplot(master_data.loc[(master_data['gender'] == 'Female'), 'annual_premium'], kde_kws = {"color": "r", "lw": 1, "label": "Female"})
    plt.title('Annual Premium distribution by Gender', fontsize = 15)
    plt.show()
    

![](__results___files/__results___14_0.png)

In [14]:

    
    
    plt.figure(figsize = (15, 6))
    sns.distplot(master_data.loc[(master_data['driving_license'] == 0), 'age'], kde_kws = {"color": "b", "lw": 1, "label": "Not Licensed for driving"})
    sns.distplot(master_data.loc[(master_data['driving_license'] == 1), 'age'], kde_kws = {"color": "r", "lw": 1, "label": "Licensed for Driving"})
    plt.title('Age distribution by Driving License', fontsize = 15)
    plt.show()
    

![](__results___files/__results___15_0.png)

In [15]:

    
    
    plt.figure(figsize = (15, 6))
    sns.distplot(master_data.loc[(master_data['driving_license'] == 0), 'annual_premium'], kde_kws = {"color": "b", "lw": 1, "label": "Not Licensed for driving"})
    sns.distplot(master_data.loc[(master_data['driving_license'] == 1), 'annual_premium'], kde_kws = {"color": "r", "lw": 1, "label": "Licensed for Driving"})
    plt.title('Annual Premium distribution by Driving License', fontsize = 15)
    plt.show()
    

![](__results___files/__results___16_0.png)

In [16]:

    
    
    plt.figure(figsize = (18, 5))
    sns.boxplot(master_data['annual_premium'])
    plt.title('Annual Premium distribution (Highly skewed to the right)', fontsize = 15)
    plt.show()
    

![](__results___files/__results___17_0.png)

In [17]:

    
    
    plt.figure(figsize = (8, 5))
    sns.distplot(master_data['vintage'])
    plt.title('No. of days customer was associated with the company', fontsize = 15)
    plt.show()
    

![](__results___files/__results___18_0.png)

In [18]:

    
    
    # looking at the frequency of records by age
    
    plt.rcParams['figure.figsize'] = (18, 7)
    color = plt.cm.copper(np.linspace(0, 1, 50))
    train_data['policy_sales_channel'].value_counts().head(50).plot.bar(color = color)
    plt.title('Top Policy Sales Channels', fontsize = 15)
    plt.xticks(rotation = 90)
    plt.show()
    

![](__results___files/__results___19_0.png)

In [19]:

    
    
    # looking at the frequency of records by sales channel
    
    plt.rcParams['figure.figsize'] = (18, 7)
    color = plt.cm.copper(np.linspace(0, 1, 53))
    train_data['region_code'].value_counts().head(53).plot.bar(color = color)
    plt.title('Customers count by top regions', fontsize = 15)
    plt.xticks(rotation = 90)
    plt.show()
    

![](__results___files/__results___20_0.png)

In [20]:

    
    
    fig, axes = plt.subplots(ncols = 2, figsize = (13, 3), dpi = 100)
    plt.tight_layout()
    
    train_data.groupby('previously_insured').count()['id'].plot(kind = 'pie', ax = axes[0], labels = ['Insured Customers (54.1%)', 'Not Insured Customers (45.9%)'])
    sns.countplot(x = train_data['previously_insured'], hue = train_data['previously_insured'], ax = axes[1])
    
    axes[0].set_ylabel('')
    axes[1].set_ylabel('')
    axes[1].set_xticklabels(['Insured Customers (54.1%)', 'Not Insured Customers (45.9%)'])
    axes[0].tick_params(axis = 'x', labelsize = 8)
    axes[0].tick_params(axis = 'y', labelsize = 8)
    axes[1].tick_params(axis = 'x', labelsize = 8)
    axes[1].tick_params(axis = 'y', labelsize = 8)
    
    axes[0].set_title('Label Distribution in Training Set', fontsize = 8)
    axes[1].set_title('Label Count in Training Set', fontsize =8)
    
    plt.show()
    

![](__results___files/__results___21_0.png)

In [21]:

    
    
    sns.countplot(data = master_data, x = 'driving_license', hue = 'gender')
    plt.ylabel('Count')
    plt.show()
    

![](__results___files/__results___22_0.png)

##### We should oversample the minority class to account for customers without
a driving license¶

# Feature Engineering¶

In [22]:

    
    
    # Unique values for all the columns
    for col in train_data.columns[~(train_data.columns.isin(['age', 'id', 'region_code', 'annual_premium', 'policy_sales_channel', 'vintage']))].tolist():
        print(" Unique Values --> " + col, ':', len(train_data[col].unique()), ': ', train_data[col].unique())
    
    
    
     Unique Values --> gender : 2 :  ['Male' 'Female']
     Unique Values --> driving_license : 2 :  [1 0]
     Unique Values --> previously_insured : 2 :  [0 1]
     Unique Values --> vehicle_age : 3 :  ['> 2 Years' '1-2 Year' '< 1 Year']
     Unique Values --> vehicle_damage : 2 :  ['Yes' 'No']
     Unique Values --> response : 2 :  [1 0]
     Unique Values --> type : 1 :  ['train']
    

In [23]:

    
    
    gender = {'Male': 0, 'Female': 1}
    driving_license = {0: 0, 1: 1}
    previously_insured = {0: 1, 1: 0}
    vehicle_age = {'> 2 Years': 2, '1-2 Year': 1, '< 1 Year': 0}
    vehicle_damage = {'Yes': 1, 'No': 0}
    
    master_data['gender'] = master_data['gender'].map(gender)
    master_data['driving_license'] = master_data['driving_license'].map(driving_license)
    master_data['previously_insured'] = master_data['previously_insured'].map(previously_insured)
    master_data['vehicle_age'] = master_data['vehicle_age'].map(vehicle_age)
    master_data['vehicle_damage'] = master_data['vehicle_damage'].map(vehicle_damage)
    
    master_data['policy_sales_channel'] = master_data['policy_sales_channel'].apply(lambda x: np.int(x))
    master_data['region_code'] = master_data['region_code'].apply(lambda x: np.int(x))
    
    master_data.head()
    

Out[23]:

| id | gender | age | driving_license | region_code | previously_insured | vehicle_age | vehicle_damage | annual_premium | policy_sales_channel | vintage | response | type  
---|---|---|---|---|---|---|---|---|---|---|---|---|---  
0 | 1 | 0 | 44 | 1 | 28 | 1 | 2 | 1 | 40454.0 | 26 | 217 | 1.0 | train  
1 | 2 | 0 | 76 | 1 | 3 | 1 | 1 | 0 | 33536.0 | 26 | 183 | 0.0 | train  
2 | 3 | 0 | 47 | 1 | 28 | 1 | 2 | 1 | 38294.0 | 26 | 27 | 1.0 | train  
3 | 4 | 0 | 21 | 1 | 11 | 0 | 0 | 0 | 28619.0 | 152 | 203 | 0.0 | train  
4 | 5 | 1 | 29 | 1 | 41 | 0 | 0 | 0 | 27496.0 | 152 | 39 | 0.0 | train  
  
# Correlation of features with target variable¶

In [24]:

    
    
    corrMatrix = master_data.corr()
    sns.heatmap(corrMatrix, annot = True)
    plt.show()
    

![](__results___files/__results___28_0.png)

  * It appears **policy sales channel** and **vintage** are more negatively correlated to the target variable **response**

In [25]:

    
    
    # Numerical columns
    numerical_cols = ['age', 'vintage']
    
    # categorical column 
    cat_col = ['gender', 'driving_license', 'region_code', 'previously_insured', 'vehicle_age', 'vehicle_damage', 'policy_sales_channel']
    
    #master_data['policy_sales_channel'] = master_data['policy_sales_channel'].map(master_data['policy_sales_channel'].value_counts())
    #master_data['region_code'] = master_data['region_code'].map(master_data['region_code'].value_counts())
    
    ss = StandardScaler()
    master_data[numerical_cols] = ss.fit_transform(master_data[numerical_cols])
    
    mm = MinMaxScaler()
    master_data[['annual_premium']] = mm.fit_transform(master_data[['annual_premium']])
    
    master_data.head()
    

Out[25]:

| id | gender | age | driving_license | region_code | previously_insured | vehicle_age | vehicle_damage | annual_premium | policy_sales_channel | vintage | response | type  
---|---|---|---|---|---|---|---|---|---|---|---|---|---  
0 | 1 | 0 | 0.334938 | 1 | 28 | 1 | 2 | 1 | 0.070366 | 26 | 0.748905 | 1.0 | train  
1 | 2 | 0 | 2.399432 | 1 | 3 | 1 | 1 | 0 | 0.057496 | 26 | 0.342540 | 0.0 | train  
2 | 3 | 0 | 0.528484 | 1 | 28 | 1 | 2 | 1 | 0.066347 | 26 | -1.521956 | 1.0 | train  
3 | 4 | 0 | -1.148918 | 1 | 11 | 0 | 0 | 0 | 0.048348 | 152 | 0.581578 | 0.0 | train  
4 | 5 | 1 | -0.632794 | 1 | 41 | 0 | 0 | 0 | 0.046259 | 152 | -1.378534 | 0.0 | train  
  
## Splitting bact to train/test set¶

In [26]:

    
    
    train_data = master_data.loc[(master_data['type'] == 'train')]
    test_data = master_data.loc[(master_data['type'] == 'test')]
    
    train_data = train_data.drop(['id', 'type'], axis = 1)
    train_data['response'] = train_data['response'].apply(lambda x: np.int(x))
    
    testIDs = test_data['id']
    test_data = test_data.drop(['id', 'type', 'response'], axis = 1)
    train_data.head()
    

Out[26]:

| gender | age | driving_license | region_code | previously_insured | vehicle_age | vehicle_damage | annual_premium | policy_sales_channel | vintage | response  
---|---|---|---|---|---|---|---|---|---|---|---  
0 | 0 | 0.334938 | 1 | 28 | 1 | 2 | 1 | 0.070366 | 26 | 0.748905 | 1  
1 | 0 | 2.399432 | 1 | 3 | 1 | 1 | 0 | 0.057496 | 26 | 0.342540 | 0  
2 | 0 | 0.528484 | 1 | 28 | 1 | 2 | 1 | 0.066347 | 26 | -1.521956 | 1  
3 | 0 | -1.148918 | 1 | 11 | 0 | 0 | 0 | 0.048348 | 152 | 0.581578 | 0  
4 | 1 | -0.632794 | 1 | 41 | 0 | 0 | 0 | 0.046259 | 152 | -1.378534 | 0  
  
In [27]:

    
    
    for column in cat_col:
        test_data[column] = test_data[column].astype('str')
    

In [28]:

    
    
    for column in cat_col:
        train_data[column] = train_data[column].astype('str')
    
    train_data = train_data.drop(['vintage'], axis = 1)
    test_data = test_data.drop(['vintage'], axis = 1)
    
    X = train_data.drop(['response'], axis = 1)#.values
    y = train_data['response']#.values
    
    cat_cols = [0, 2, 3, 4, 5, 6, 8]
    

# Model Building¶

## 1\. Catboost Model¶

In [29]:

    
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.22, random_state = 22, stratify = y, shuffle = True)
    
    modelC = CatBoostClassifier()
    modelC = modelC.fit(X_train, y_train, cat_features = cat_col, eval_set = (X_test, y_test), early_stopping_rounds = 10, verbose = 100)
    
    predictions = [pred[1] for pred in modelC.predict_proba(X_test)]
    print('Validation ROC AUC Score:', roc_auc_score(y_test, predictions, average = 'weighted'))
    
    
    
    Learning rate set to 0.129353
    0:	learn: 0.4929762	test: 0.4931944	best: 0.4931944 (0)	total: 251ms	remaining: 4m 10s
    100:	learn: 0.2633721	test: 0.2643220	best: 0.2643206 (99)	total: 20.2s	remaining: 2m 59s
    Stopped by overfitting detector  (10 iterations wait)
    
    bestTest = 0.2640151812
    bestIteration = 169
    
    Shrink model to first 170 iterations.
    Validation ROC AUC Score: 0.859642152869408
    

In [30]:

    
    
    cat_pred = [pred[1] for pred in modelC.predict_proba(test_data)]
    submissionC = pd.DataFrame(data = {'id': testIDs, 'Response': cat_pred})
    submissionC.to_csv("catboost_v1.csv", index = False)
    submissionC.head()
    

Out[30]:

| id | Response  
---|---|---  
0 | 381110 | 0.000379  
1 | 381111 | 0.301155  
2 | 381112 | 0.292050  
3 | 381113 | 0.006170  
4 | 381114 | 0.000409  
  
## 2\. LGBM Classifier with kFold¶

In [31]:

    
    
    X = train_data.drop(['response'], axis = 1).values
    y = train_data['response'].values
    

In [32]:

    
    
    kfold, scores = KFold(n_splits = 5, shuffle = True, random_state = 22), list()
    for train, test in kfold.split(X):
        X_train, X_test = X[train], X[test]
        y_train, y_test = y[train], y[test]
    
        model = LGBMClassifier(random_state = 22, max_depth = 7, n_estimators = 110, reg_lambda = 1.2, reg_alpha = 1.2, min_child_weight = 1, 
                               learning_rate = 0.15, gamma = 0.3, colsample_bytree = 0.5, eval_metric = 'auc', is_higher_better = 1, plot = True)
        model.fit(X_train, y_train)
        preds = [pred[1] for pred in model.predict_proba(X_test)]
        score = roc_auc_score(y_test, preds, average = 'weighted')
        scores.append(score)
        print('Validation ROC AUC:', score)
    print("Average Validation ROC AUC: ", sum(scores)/len(scores))
    
    
    
    Validation ROC AUC: 0.8567462748294772
    Validation ROC AUC: 0.858279943171442
    Validation ROC AUC: 0.8581817267487639
    Validation ROC AUC: 0.8604899005796693
    Validation ROC AUC: 0.8595116022428981
    Average Validation ROC AUC:  0.8586418895144501
    

## ROC AUC Curve¶

In [33]:

    
    
    yTest = model.predict(X_test)
    
    fpr, tpr, thresholds = roc_curve(yTest.ravel(), y_test)
    roc_auc = auc(fpr, tpr)
    
    # Plot ROC
    plt.title('Receiver Operating Characteristic')
    plt.plot(fpr, tpr, 'b',label = 'AUC = %0.2f'% roc_auc)
    plt.legend(loc='lower right')
    plt.plot([0,1],[0,1],'r--')
    plt.xlim([-0.1,1.0])
    plt.ylim([-0.1,1.01])
    plt.ylabel('True Positive Rate')
    plt.xlabel('False Positive Rate')
    plt.show()
    

![](__results___files/__results___43_0.png)

### Splitting the dataset¶

In [34]:

    
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 22)
    

## Hyperparameter Tuning¶

In [35]:

    
    
    """
    model = LGBMClassifier(random_state = 22)
    
    param_grid = {"learning_rate"    : [0.01, 0.02, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40],
                  "max_depth"        : [4, 5, 6, 7, 8, 9, 10],
                  "min_child_weight" : [1, 3, 5, 7],
                  "gamma"            : [0.0, 0.1, 0.2 , 0.3, 0.4],
                  "colsample_bytree" : [0.3, 0.4, 0.5 , 0.7],
                  "n_estimators"     : [50, 70, 90, 100, 120, 150, 200, 250, 300, 350, 400, 450],
                  'reg_alpha'        : [1,1.2],
                  'reg_lambda'       : [1,1.2,1.4]
                  }
    
    kfold = KFold(n_splits = 6, shuffle = True, random_state = 22)
    
    grid_search = RandomizedSearchCV(model, param_distributions = param_grid, scoring = "accuracy", n_jobs  = -1, cv = kfold, verbose = 1)
    grid_result = grid_search.fit(X_train, y_train)
    print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
    
    """
    

Out[35]:

    
    
    '\nmodel = LGBMClassifier(random_state = 22)\n\nparam_grid = {"learning_rate"    : [0.01, 0.02, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40],\n              "max_depth"        : [4, 5, 6, 7, 8, 9, 10],\n              "min_child_weight" : [1, 3, 5, 7],\n              "gamma"            : [0.0, 0.1, 0.2 , 0.3, 0.4],\n              "colsample_bytree" : [0.3, 0.4, 0.5 , 0.7],\n              "n_estimators"     : [50, 70, 90, 100, 120, 150, 200, 250, 300, 350, 400, 450],\n              \'reg_alpha\'        : [1,1.2],\n              \'reg_lambda\'       : [1,1.2,1.4]\n              }\n\nkfold = KFold(n_splits = 6, shuffle = True, random_state = 22)\n\ngrid_search = RandomizedSearchCV(model, param_distributions = param_grid, scoring = "accuracy", n_jobs  = -1, cv = kfold, verbose = 1)\ngrid_result = grid_search.fit(X_train, y_train)\nprint("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))\n\n'

In [36]:

    
    
    bestLGB = LGBMClassifier(random_state = 22, max_depth = 7, n_estimators = 110, reg_lambda = 1.2, reg_alpha = 1.2, min_child_weight = 1,
                             learning_rate = 0.15, gamma = 0.3, colsample_bytree = 0.5)
    bestLGB.fit(X_train, y_train)
    y_pred = bestLGB.predict_proba(X_test)
    

# Predictions¶

In [37]:

    
    
    Preds = [predClass[1] for predClass in model.predict_proba(test_data.values)]
    

# Submission¶

In [38]:

    
    
    submission = pd.DataFrame(data = {'id': testIDs, 'Response': Preds})
    submission.to_csv('cross_sell_v8.csv', index = False)
    submission.head()
    

Out[38]:

| id | Response  
---|---|---  
0 | 381110 | 0.000780  
1 | 381111 | 0.290843  
2 | 381112 | 0.302983  
3 | 381113 | 0.005049  
4 | 381114 | 0.000528  
  
## CatBoost gave an ROC AUC score of 85.83 (0.3 improvement over LightGBM)¶

