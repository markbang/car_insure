In [1]:

    
    
    # This Python 3 environment comes with many helpful analytics libraries installed
    # It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
    # For example, here's several helpful packages to load in 
    
    import numpy as np # linear algebra
    import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
    
    # Input data files are available in the "../input/" directory.
    # For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory
    
    import os
    print(os.listdir("../input"))
    
    # Any results you write to the current directory are saved as output.
    
    
    
    ['train.csv', 'sample_submission.csv', 'test.csv']
    

Import Necessary Libraries & Import .csv files into pandas DataFrames

In [2]:

    
    
    import pandas as pd
    import numpy as np
    import matplotlib.pyplot as plt
    import seaborn as sns
    %matplotlib inline 
    
    data_train = pd.read_csv('../input/train.csv')
    data_test = pd.read_csv('../input/test.csv')
    

In [3]:

    
    
    #Lets see data sample
    data_train.sample(10)
    

Out[3]:

| Id | Product_Info_1 | Product_Info_2 | Product_Info_3 | Product_Info_4 | Product_Info_5 | Product_Info_6 | Product_Info_7 | Ins_Age | Ht | Wt | BMI | Employment_Info_1 | Employment_Info_2 | Employment_Info_3 | Employment_Info_4 | Employment_Info_5 | Employment_Info_6 | InsuredInfo_1 | InsuredInfo_2 | InsuredInfo_3 | InsuredInfo_4 | InsuredInfo_5 | InsuredInfo_6 | InsuredInfo_7 | Insurance_History_1 | Insurance_History_2 | Insurance_History_3 | Insurance_History_4 | Insurance_History_5 | Insurance_History_7 | Insurance_History_8 | Insurance_History_9 | Family_Hist_1 | Family_Hist_2 | Family_Hist_3 | Family_Hist_4 | Family_Hist_5 | Medical_History_1 | Medical_History_2 | ... | Medical_Keyword_10 | Medical_Keyword_11 | Medical_Keyword_12 | Medical_Keyword_13 | Medical_Keyword_14 | Medical_Keyword_15 | Medical_Keyword_16 | Medical_Keyword_17 | Medical_Keyword_18 | Medical_Keyword_19 | Medical_Keyword_20 | Medical_Keyword_21 | Medical_Keyword_22 | Medical_Keyword_23 | Medical_Keyword_24 | Medical_Keyword_25 | Medical_Keyword_26 | Medical_Keyword_27 | Medical_Keyword_28 | Medical_Keyword_29 | Medical_Keyword_30 | Medical_Keyword_31 | Medical_Keyword_32 | Medical_Keyword_33 | Medical_Keyword_34 | Medical_Keyword_35 | Medical_Keyword_36 | Medical_Keyword_37 | Medical_Keyword_38 | Medical_Keyword_39 | Medical_Keyword_40 | Medical_Keyword_41 | Medical_Keyword_42 | Medical_Keyword_43 | Medical_Keyword_44 | Medical_Keyword_45 | Medical_Keyword_46 | Medical_Keyword_47 | Medical_Keyword_48 | Response  
---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---  
14700 | 19576 | 1 | D2 | 26 | 0.179487 | 2 | 3 | 1 | 0.343284 | 0.800000 | 0.414226 | 0.566707 | 0.065000 | 12 | 1 | NaN | 2 | 0.000 | 1 | 2 | 3 | 3 | 1 | 1 | 1 | 1 | 1 | 3 | 1 | 0.000667 | 1 | 1 | 2 | 2 | 0.536232 | NaN | 0.464789 | NaN | 1.0 | 434 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 6  
52159 | 69445 | 1 | A7 | 26 | 0.010256 | 2 | 3 | 1 | 0.716418 | 0.672727 | 0.242678 | 0.421090 | 0.040000 | 9 | 1 | 0.000 | 2 | NaN | 2 | 2 | 3 | 3 | 1 | 2 | 1 | 2 | 1 | 3 | 1 | 0.000667 | 1 | 3 | 2 | 3 | NaN | NaN | NaN | NaN | 4.0 | 112 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 5  
44287 | 58886 | 1 | D2 | 26 | 0.487179 | 2 | 3 | 3 | 0.656716 | 0.781818 | 0.466527 | 0.667757 | 0.056000 | 12 | 1 | 0.000 | 2 | 0.750 | 1 | 2 | 8 | 3 | 1 | 1 | 1 | 2 | 1 | 3 | 1 | 0.000333 | 1 | 3 | 2 | 2 | NaN | 0.225490 | 0.718310 | NaN | 4.0 | 112 | ... | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 2  
2064 | 2774 | 1 | A8 | 26 | 0.487179 | 2 | 3 | 1 | 0.432836 | 0.636364 | 0.207113 | 0.390388 | 0.450000 | 9 | 1 | 0.000 | 2 | 1.000 | 2 | 2 | 3 | 3 | 1 | 2 | 1 | 2 | 1 | 3 | 1 | 0.010000 | 1 | 3 | 2 | 3 | 0.521739 | NaN | NaN | 0.517857 | 1.0 | 16 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 8  
28037 | 37332 | 1 | D3 | 26 | 0.230769 | 2 | 1 | 1 | 0.358209 | 0.636364 | 0.278243 | 0.528296 | 0.082524 | 9 | 1 | 0.000 | 2 | 0.000 | 1 | 2 | 6 | 3 | 1 | 2 | 1 | 2 | 1 | 3 | 2 | 0.003333 | 1 | 3 | 2 | 3 | NaN | 0.450980 | 0.577465 | NaN | 1.0 | 387 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1  
21083 | 28081 | 1 | D2 | 26 | 0.487179 | 2 | 1 | 1 | 0.671642 | 0.763636 | 0.263598 | 0.372161 | 0.100000 | 12 | 1 | 0.000 | 2 | 1.000 | 1 | 2 | 6 | 3 | 1 | 1 | 1 | 2 | 1 | 1 | 3 | NaN | 3 | 2 | 3 | 3 | NaN | 0.656863 | NaN | 0.607143 | 8.0 | 387 | ... | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2  
30300 | 40266 | 1 | D3 | 26 | 0.487179 | 2 | 1 | 1 | 0.582090 | 0.818182 | 0.253138 | 0.313008 | 0.150000 | 12 | 1 | 0.005 | 2 | 1.000 | 2 | 2 | 3 | 3 | 1 | 1 | 1 | 2 | 1 | 1 | 3 | NaN | 3 | 2 | 3 | 2 | NaN | 0.323529 | 0.633803 | NaN | 1.0 | 610 | ... | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2  
40410 | 53678 | 1 | D1 | 26 | 0.128205 | 2 | 3 | 1 | 0.791045 | 0.618182 | 0.248954 | 0.492406 | 0.000000 | 1 | 3 | 0.035 | 2 | 0.000 | 2 | 2 | 2 | 3 | 1 | 2 | 1 | 2 | 1 | 3 | 1 | 0.000133 | 1 | 3 | 2 | 3 | NaN | 0.715686 | NaN | 0.508929 | 3.0 | 491 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 7  
22948 | 30601 | 1 | A1 | 26 | 0.076923 | 2 | 3 | 1 | 0.059701 | 0.672727 | 0.288703 | 0.504999 | 0.018000 | 9 | 1 | 0.000 | 2 | 0.005 | 1 | 2 | 6 | 3 | 1 | 1 | 1 | 2 | 1 | 1 | 3 | NaN | 3 | 2 | 3 | 3 | 0.159420 | NaN | 0.098592 | NaN | NaN | 162 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 7  
35625 | 47300 | 1 | D2 | 26 | 0.230769 | 2 | 3 | 1 | 0.522388 | 0.836364 | 0.435146 | 0.556869 | 0.100000 | 9 | 1 | 0.000 | 2 | 0.300 | 1 | 2 | 2 | 3 | 1 | 1 | 1 | 2 | 1 | 3 | 2 | 0.002667 | 1 | 3 | 2 | 2 | NaN | NaN | 0.605634 | NaN | NaN | 162 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1  
  
In [4]:

    
    
    # Lets check Df shape
    data_train.shape
    
    # there are 128 features.
    

Out[4]:

    
    
    (59381, 128)

In [5]:

    
    
    data_test.shape
    

Out[5]:

    
    
    (19765, 127)

**Feature details posted in data overview section -** The following variables
are all categorical (nominal):

Product_Info_1, Product_Info_2, Product_Info_3, Product_Info_5,
Product_Info_6, Product_Info_7, Employment_Info_2, Employment_Info_3,
Employment_Info_5, InsuredInfo_1, InsuredInfo_2, InsuredInfo_3, InsuredInfo_4,
InsuredInfo_5, InsuredInfo_6, InsuredInfo_7, Insurance_History_1,
Insurance_History_2, Insurance_History_3, Insurance_History_4,
Insurance_History_7, Insurance_History_8, Insurance_History_9, Family_Hist_1,
Medical_History_2, Medical_History_3, Medical_History_4, Medical_History_5,
Medical_History_6, Medical_History_7, Medical_History_8, Medical_History_9,
Medical_History_11, Medical_History_12, Medical_History_13,
Medical_History_14, Medical_History_16, Medical_History_17,
Medical_History_18, Medical_History_19, Medical_History_20,
Medical_History_21, Medical_History_22, Medical_History_23,
Medical_History_25, Medical_History_26, Medical_History_27,
Medical_History_28, Medical_History_29, Medical_History_30,
Medical_History_31, Medical_History_33, Medical_History_34,
Medical_History_35, Medical_History_36, Medical_History_37,
Medical_History_38, Medical_History_39, Medical_History_40, Medical_History_41

The following variables are continuous:

Product_Info_4, Ins_Age, Ht, Wt, BMI, Employment_Info_1, Employment_Info_4,
Employment_Info_6, Insurance_History_5, Family_Hist_2, Family_Hist_3,
Family_Hist_4, Family_Hist_5

The following variables are discrete:

Medical_History_1, Medical_History_10, Medical_History_15, Medical_History_24,
Medical_History_32

Medical_Keyword_1-48 are dummy variables.

We will check for missing values .

If a categorical feature has missing values - if required will impute it with
median

if a continous feature has missing values - if required will impute it with
mean

In [6]:

    
    
    data_train.dtypes
    data_train.dtypes.unique()
    #No string data type - all are numerical values which is good.
    

Out[6]:

    
    
    array([dtype('int64'), dtype('O'), dtype('float64')], dtype=object)

# Missing Value imputation¶

In [7]:

    
    
    data_train.isnull().sum()[data_train.isnull().sum() !=0]
    #Below listed columns have missing values in the combined (Train+test) dataset. 
    

Out[7]:

    
    
    Employment_Info_1         19
    Employment_Info_4       6779
    Employment_Info_6      10854
    Insurance_History_5    25396
    Family_Hist_2          28656
    Family_Hist_3          34241
    Family_Hist_4          19184
    Family_Hist_5          41811
    Medical_History_1       8889
    Medical_History_10     58824
    Medical_History_15     44596
    Medical_History_24     55580
    Medical_History_32     58274
    dtype: int64

In [8]:

    
    
    # Lets draw a bar graph to visualize percentage of missing features in train set
    missing= data_train.isnull().sum()[data_train.isnull().sum() !=0]
    missing=pd.DataFrame(missing.reset_index())
    missing.rename(columns={'index':'features',0:'missing_count'},inplace=True)
    missing['missing_count_percentage']=((missing['missing_count'])/59381)*100
    plt.figure(figsize=(20,8))
    sns.barplot(y=missing['features'],x=missing['missing_count_percentage'])
    
    #Looking at below bar grah- 
    #Medical_Hist_32/24/15/10 , Family_hist_5 are top five features with huge amount of missing data ( imputaion to these might not be fruitful - I will drop these features)
    

Out[8]:

    
    
    <matplotlib.axes._subplots.AxesSubplot at 0x7f25c538e630>

![](__results___files/__results___11_1.png)

Employment_Info_1_4_6 Insurance_History_5 Family_Hist_2-3-4-5 are continous
features .

The following variables are discrete: Medical_History_1, Medical_History_10,
Medical_History_15, Medical_History_24, Medical_History_32

  1. remove rows with missing values and see model performance 
  2. impute missing values with mean and median or may be mode.

In [9]:

    
    
    # Lets see spread of data before we impute missing values
    plt.plot(figsize=(15,10))
    sns.boxplot(data_train['Employment_Info_1'])
    # Employment_Info_1 seems to have lots of outliers - Median should be right to impute missing values
    

Out[9]:

    
    
    <matplotlib.axes._subplots.AxesSubplot at 0x7f25c4824208>

![](__results___files/__results___13_1.png)

In [10]:

    
    
    data_train['Employment_Info_1'].isna().sum()
    

Out[10]:

    
    
    19

In [11]:

    
    
    data_train['Employment_Info_1'].fillna(data_train['Employment_Info_1'].median(),inplace=True) 
    # imputing with Meadian , as there are lots of Outliers 
    data_test['Employment_Info_1'].fillna(data_test['Employment_Info_1'].median(),inplace=True) 
    

In [12]:

    
    
    data_train['Employment_Info_1'].isna().sum()
    

Out[12]:

    
    
    0

In [13]:

    
    
    #Outlier Treatment -
    data_train['Employment_Info_1'].describe()
    

Out[13]:

    
    
    count    59381.000000
    mean         0.077576
    std          0.082334
    min          0.000000
    25%          0.035000
    50%          0.060000
    75%          0.100000
    max          1.000000
    Name: Employment_Info_1, dtype: float64

In [14]:

    
    
    sns.boxplot(data_train['Employment_Info_4'])
    # ['Employment_Info_4'] is has most of the values centered close to zero , also huge presence of outliers 
    

Out[14]:

    
    
    <matplotlib.axes._subplots.AxesSubplot at 0x7f25c4758c88>

![](__results___files/__results___18_1.png)

In [15]:

    
    
    data_train['Employment_Info_4'].fillna(data_train['Employment_Info_4'].median(),inplace=True)
    data_test['Employment_Info_4'].fillna(data_test['Employment_Info_4'].median(),inplace=True)
    

In [16]:

    
    
    sns.boxplot(data_train['Employment_Info_6'])
    #No outlieers - mean should be rigth candidate to impute missing values
    

Out[16]:

    
    
    <matplotlib.axes._subplots.AxesSubplot at 0x7f25c4764a90>

![](__results___files/__results___20_1.png)

In [17]:

    
    
    data_train['Employment_Info_6'].fillna(data_train['Employment_Info_6'].mean(),inplace=True)
    data_test['Employment_Info_6'].fillna(data_test['Employment_Info_6'].mean(),inplace=True)
    

In [18]:

    
    
    sns.boxplot(y=data_train['Medical_History_1'])
    

Out[18]:

    
    
    <matplotlib.axes._subplots.AxesSubplot at 0x7f25c4743208>

![](__results___files/__results___22_1.png)

In [19]:

    
    
    data_train['Medical_History_1'].fillna(data_train['Medical_History_1'].median(),inplace=True)
    data_test['Medical_History_1'].fillna(data_test['Medical_History_1'].median(),inplace=True)
    

In [20]:

    
    
    #lets drop features with high number of missing values 
    data_train.drop(['Medical_History_10','Medical_History_15','Medical_History_24','Medical_History_32','Family_Hist_3','Family_Hist_5','Family_Hist_2','Family_Hist_4'],axis=1,inplace=True)
    

In [21]:

    
    
    data_test.drop(['Medical_History_10','Medical_History_15','Medical_History_24','Medical_History_32','Family_Hist_3','Family_Hist_5','Family_Hist_2','Family_Hist_4'],axis=1,inplace=True)
    

In [22]:

    
    
    data_train.isnull().sum()[data_train.isnull().sum()!=0]
    

Out[22]:

    
    
    Insurance_History_5    25396
    dtype: int64

In [23]:

    
    
    #imputing with median 
    data_train['Insurance_History_5'].fillna(data_train['Insurance_History_5'].median(),inplace=True)
    data_test['Insurance_History_5'].fillna(data_test['Insurance_History_5'].median(),inplace=True)
    

In [24]:

    
    
    data_train.isnull().sum()
    #All missing NA values has been treated
    

Out[24]:

    
    
    Id                     0
    Product_Info_1         0
    Product_Info_2         0
    Product_Info_3         0
    Product_Info_4         0
    Product_Info_5         0
    Product_Info_6         0
    Product_Info_7         0
    Ins_Age                0
    Ht                     0
    Wt                     0
    BMI                    0
    Employment_Info_1      0
    Employment_Info_2      0
    Employment_Info_3      0
    Employment_Info_4      0
    Employment_Info_5      0
    Employment_Info_6      0
    InsuredInfo_1          0
    InsuredInfo_2          0
    InsuredInfo_3          0
    InsuredInfo_4          0
    InsuredInfo_5          0
    InsuredInfo_6          0
    InsuredInfo_7          0
    Insurance_History_1    0
    Insurance_History_2    0
    Insurance_History_3    0
    Insurance_History_4    0
    Insurance_History_5    0
                          ..
    Medical_Keyword_20     0
    Medical_Keyword_21     0
    Medical_Keyword_22     0
    Medical_Keyword_23     0
    Medical_Keyword_24     0
    Medical_Keyword_25     0
    Medical_Keyword_26     0
    Medical_Keyword_27     0
    Medical_Keyword_28     0
    Medical_Keyword_29     0
    Medical_Keyword_30     0
    Medical_Keyword_31     0
    Medical_Keyword_32     0
    Medical_Keyword_33     0
    Medical_Keyword_34     0
    Medical_Keyword_35     0
    Medical_Keyword_36     0
    Medical_Keyword_37     0
    Medical_Keyword_38     0
    Medical_Keyword_39     0
    Medical_Keyword_40     0
    Medical_Keyword_41     0
    Medical_Keyword_42     0
    Medical_Keyword_43     0
    Medical_Keyword_44     0
    Medical_Keyword_45     0
    Medical_Keyword_46     0
    Medical_Keyword_47     0
    Medical_Keyword_48     0
    Response               0
    Length: 120, dtype: int64

# Now that we have imputed Missing values - we can move to next step to
convert string type feature data into numric data¶

In [25]:

    
    
    data_train.head()
    #Product_info_2 seems to be the only feature where we should map string values with numeric categorical values
    

Out[25]:

| Id | Product_Info_1 | Product_Info_2 | Product_Info_3 | Product_Info_4 | Product_Info_5 | Product_Info_6 | Product_Info_7 | Ins_Age | Ht | Wt | BMI | Employment_Info_1 | Employment_Info_2 | Employment_Info_3 | Employment_Info_4 | Employment_Info_5 | Employment_Info_6 | InsuredInfo_1 | InsuredInfo_2 | InsuredInfo_3 | InsuredInfo_4 | InsuredInfo_5 | InsuredInfo_6 | InsuredInfo_7 | Insurance_History_1 | Insurance_History_2 | Insurance_History_3 | Insurance_History_4 | Insurance_History_5 | Insurance_History_7 | Insurance_History_8 | Insurance_History_9 | Family_Hist_1 | Medical_History_1 | Medical_History_2 | Medical_History_3 | Medical_History_4 | Medical_History_5 | Medical_History_6 | ... | Medical_Keyword_10 | Medical_Keyword_11 | Medical_Keyword_12 | Medical_Keyword_13 | Medical_Keyword_14 | Medical_Keyword_15 | Medical_Keyword_16 | Medical_Keyword_17 | Medical_Keyword_18 | Medical_Keyword_19 | Medical_Keyword_20 | Medical_Keyword_21 | Medical_Keyword_22 | Medical_Keyword_23 | Medical_Keyword_24 | Medical_Keyword_25 | Medical_Keyword_26 | Medical_Keyword_27 | Medical_Keyword_28 | Medical_Keyword_29 | Medical_Keyword_30 | Medical_Keyword_31 | Medical_Keyword_32 | Medical_Keyword_33 | Medical_Keyword_34 | Medical_Keyword_35 | Medical_Keyword_36 | Medical_Keyword_37 | Medical_Keyword_38 | Medical_Keyword_39 | Medical_Keyword_40 | Medical_Keyword_41 | Medical_Keyword_42 | Medical_Keyword_43 | Medical_Keyword_44 | Medical_Keyword_45 | Medical_Keyword_46 | Medical_Keyword_47 | Medical_Keyword_48 | Response  
---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---  
0 | 2 | 1 | D3 | 10 | 0.076923 | 2 | 1 | 1 | 0.641791 | 0.581818 | 0.148536 | 0.323008 | 0.028 | 12 | 1 | 0.0 | 3 | 0.361469 | 1 | 2 | 6 | 3 | 1 | 2 | 1 | 1 | 1 | 3 | 1 | 0.000667 | 1 | 1 | 2 | 2 | 4.0 | 112 | 2 | 1 | 1 | 3 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 8  
1 | 5 | 1 | A1 | 26 | 0.076923 | 2 | 3 | 1 | 0.059701 | 0.600000 | 0.131799 | 0.272288 | 0.000 | 1 | 3 | 0.0 | 2 | 0.001800 | 1 | 2 | 6 | 3 | 1 | 2 | 1 | 2 | 1 | 3 | 1 | 0.000133 | 1 | 3 | 2 | 2 | 5.0 | 412 | 2 | 1 | 1 | 3 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 4  
2 | 6 | 1 | E1 | 26 | 0.076923 | 2 | 3 | 1 | 0.029851 | 0.745455 | 0.288703 | 0.428780 | 0.030 | 9 | 1 | 0.0 | 2 | 0.030000 | 1 | 2 | 8 | 3 | 1 | 1 | 1 | 2 | 1 | 1 | 3 | 0.000973 | 3 | 2 | 3 | 3 | 10.0 | 3 | 2 | 2 | 1 | 3 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 8  
3 | 7 | 1 | D4 | 10 | 0.487179 | 2 | 3 | 1 | 0.164179 | 0.672727 | 0.205021 | 0.352438 | 0.042 | 9 | 1 | 0.0 | 3 | 0.200000 | 2 | 2 | 8 | 3 | 1 | 2 | 1 | 2 | 1 | 1 | 3 | 0.000973 | 3 | 2 | 3 | 3 | 0.0 | 350 | 2 | 2 | 1 | 3 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 8  
4 | 8 | 1 | D2 | 26 | 0.230769 | 2 | 3 | 1 | 0.417910 | 0.654545 | 0.234310 | 0.424046 | 0.027 | 9 | 1 | 0.0 | 2 | 0.050000 | 1 | 2 | 6 | 3 | 1 | 2 | 1 | 2 | 1 | 1 | 3 | 0.000973 | 3 | 2 | 3 | 2 | 4.0 | 162 | 2 | 2 | 1 | 3 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 8  
  
In [26]:

    
    
    data_train['Product_Info_2'].unique()
    

Out[26]:

    
    
    array(['D3', 'A1', 'E1', 'D4', 'D2', 'A8', 'A2', 'D1', 'A7', 'A6', 'A3',
           'A5', 'C4', 'C1', 'B2', 'C3', 'C2', 'A4', 'B1'], dtype=object)

In [27]:

    
    
    from sklearn.preprocessing import LabelEncoder
    le=LabelEncoder()
    data_train['Product_Info_2']=le.fit_transform(data_train['Product_Info_2'])
    data_test['Product_Info_2']=le.transform(data_test['Product_Info_2'])
    
    #data_train.dtypes
    #Employment_Info_1-4-6  Insurance_History_5
    # I faced an error stating dta types of train columns are not float/numeric ill apply encoder on all column and see what happens
    

In [28]:

    
    
    data_train.head()
    

Out[28]:

| Id | Product_Info_1 | Product_Info_2 | Product_Info_3 | Product_Info_4 | Product_Info_5 | Product_Info_6 | Product_Info_7 | Ins_Age | Ht | Wt | BMI | Employment_Info_1 | Employment_Info_2 | Employment_Info_3 | Employment_Info_4 | Employment_Info_5 | Employment_Info_6 | InsuredInfo_1 | InsuredInfo_2 | InsuredInfo_3 | InsuredInfo_4 | InsuredInfo_5 | InsuredInfo_6 | InsuredInfo_7 | Insurance_History_1 | Insurance_History_2 | Insurance_History_3 | Insurance_History_4 | Insurance_History_5 | Insurance_History_7 | Insurance_History_8 | Insurance_History_9 | Family_Hist_1 | Medical_History_1 | Medical_History_2 | Medical_History_3 | Medical_History_4 | Medical_History_5 | Medical_History_6 | ... | Medical_Keyword_10 | Medical_Keyword_11 | Medical_Keyword_12 | Medical_Keyword_13 | Medical_Keyword_14 | Medical_Keyword_15 | Medical_Keyword_16 | Medical_Keyword_17 | Medical_Keyword_18 | Medical_Keyword_19 | Medical_Keyword_20 | Medical_Keyword_21 | Medical_Keyword_22 | Medical_Keyword_23 | Medical_Keyword_24 | Medical_Keyword_25 | Medical_Keyword_26 | Medical_Keyword_27 | Medical_Keyword_28 | Medical_Keyword_29 | Medical_Keyword_30 | Medical_Keyword_31 | Medical_Keyword_32 | Medical_Keyword_33 | Medical_Keyword_34 | Medical_Keyword_35 | Medical_Keyword_36 | Medical_Keyword_37 | Medical_Keyword_38 | Medical_Keyword_39 | Medical_Keyword_40 | Medical_Keyword_41 | Medical_Keyword_42 | Medical_Keyword_43 | Medical_Keyword_44 | Medical_Keyword_45 | Medical_Keyword_46 | Medical_Keyword_47 | Medical_Keyword_48 | Response  
---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---  
0 | 2 | 1 | 16 | 10 | 0.076923 | 2 | 1 | 1 | 0.641791 | 0.581818 | 0.148536 | 0.323008 | 0.028 | 12 | 1 | 0.0 | 3 | 0.361469 | 1 | 2 | 6 | 3 | 1 | 2 | 1 | 1 | 1 | 3 | 1 | 0.000667 | 1 | 1 | 2 | 2 | 4.0 | 112 | 2 | 1 | 1 | 3 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 8  
1 | 5 | 1 | 0 | 26 | 0.076923 | 2 | 3 | 1 | 0.059701 | 0.600000 | 0.131799 | 0.272288 | 0.000 | 1 | 3 | 0.0 | 2 | 0.001800 | 1 | 2 | 6 | 3 | 1 | 2 | 1 | 2 | 1 | 3 | 1 | 0.000133 | 1 | 3 | 2 | 2 | 5.0 | 412 | 2 | 1 | 1 | 3 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 4  
2 | 6 | 1 | 18 | 26 | 0.076923 | 2 | 3 | 1 | 0.029851 | 0.745455 | 0.288703 | 0.428780 | 0.030 | 9 | 1 | 0.0 | 2 | 0.030000 | 1 | 2 | 8 | 3 | 1 | 1 | 1 | 2 | 1 | 1 | 3 | 0.000973 | 3 | 2 | 3 | 3 | 10.0 | 3 | 2 | 2 | 1 | 3 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 8  
3 | 7 | 1 | 17 | 10 | 0.487179 | 2 | 3 | 1 | 0.164179 | 0.672727 | 0.205021 | 0.352438 | 0.042 | 9 | 1 | 0.0 | 3 | 0.200000 | 2 | 2 | 8 | 3 | 1 | 2 | 1 | 2 | 1 | 1 | 3 | 0.000973 | 3 | 2 | 3 | 3 | 0.0 | 350 | 2 | 2 | 1 | 3 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 8  
4 | 8 | 1 | 15 | 26 | 0.230769 | 2 | 3 | 1 | 0.417910 | 0.654545 | 0.234310 | 0.424046 | 0.027 | 9 | 1 | 0.0 | 2 | 0.050000 | 1 | 2 | 6 | 3 | 1 | 2 | 1 | 2 | 1 | 1 | 3 | 0.000973 | 3 | 2 | 3 | 2 | 4.0 | 162 | 2 | 2 | 1 | 3 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 8  
  
In [29]:

    
    
    # feature meatrix and response vector seperation
    X_train=data_train.iloc[:,0:-1]
    y_train=data_train['Response']
    X_train.drop('Id',axis=1,inplace=True)
    

In [30]:

    
    
    X_train.head()
    

Out[30]:

| Product_Info_1 | Product_Info_2 | Product_Info_3 | Product_Info_4 | Product_Info_5 | Product_Info_6 | Product_Info_7 | Ins_Age | Ht | Wt | BMI | Employment_Info_1 | Employment_Info_2 | Employment_Info_3 | Employment_Info_4 | Employment_Info_5 | Employment_Info_6 | InsuredInfo_1 | InsuredInfo_2 | InsuredInfo_3 | InsuredInfo_4 | InsuredInfo_5 | InsuredInfo_6 | InsuredInfo_7 | Insurance_History_1 | Insurance_History_2 | Insurance_History_3 | Insurance_History_4 | Insurance_History_5 | Insurance_History_7 | Insurance_History_8 | Insurance_History_9 | Family_Hist_1 | Medical_History_1 | Medical_History_2 | Medical_History_3 | Medical_History_4 | Medical_History_5 | Medical_History_6 | Medical_History_7 | ... | Medical_Keyword_9 | Medical_Keyword_10 | Medical_Keyword_11 | Medical_Keyword_12 | Medical_Keyword_13 | Medical_Keyword_14 | Medical_Keyword_15 | Medical_Keyword_16 | Medical_Keyword_17 | Medical_Keyword_18 | Medical_Keyword_19 | Medical_Keyword_20 | Medical_Keyword_21 | Medical_Keyword_22 | Medical_Keyword_23 | Medical_Keyword_24 | Medical_Keyword_25 | Medical_Keyword_26 | Medical_Keyword_27 | Medical_Keyword_28 | Medical_Keyword_29 | Medical_Keyword_30 | Medical_Keyword_31 | Medical_Keyword_32 | Medical_Keyword_33 | Medical_Keyword_34 | Medical_Keyword_35 | Medical_Keyword_36 | Medical_Keyword_37 | Medical_Keyword_38 | Medical_Keyword_39 | Medical_Keyword_40 | Medical_Keyword_41 | Medical_Keyword_42 | Medical_Keyword_43 | Medical_Keyword_44 | Medical_Keyword_45 | Medical_Keyword_46 | Medical_Keyword_47 | Medical_Keyword_48  
---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---  
0 | 1 | 16 | 10 | 0.076923 | 2 | 1 | 1 | 0.641791 | 0.581818 | 0.148536 | 0.323008 | 0.028 | 12 | 1 | 0.0 | 3 | 0.361469 | 1 | 2 | 6 | 3 | 1 | 2 | 1 | 1 | 1 | 3 | 1 | 0.000667 | 1 | 1 | 2 | 2 | 4.0 | 112 | 2 | 1 | 1 | 3 | 2 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0  
1 | 1 | 0 | 26 | 0.076923 | 2 | 3 | 1 | 0.059701 | 0.600000 | 0.131799 | 0.272288 | 0.000 | 1 | 3 | 0.0 | 2 | 0.001800 | 1 | 2 | 6 | 3 | 1 | 2 | 1 | 2 | 1 | 3 | 1 | 0.000133 | 1 | 3 | 2 | 2 | 5.0 | 412 | 2 | 1 | 1 | 3 | 2 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0  
2 | 1 | 18 | 26 | 0.076923 | 2 | 3 | 1 | 0.029851 | 0.745455 | 0.288703 | 0.428780 | 0.030 | 9 | 1 | 0.0 | 2 | 0.030000 | 1 | 2 | 8 | 3 | 1 | 1 | 1 | 2 | 1 | 1 | 3 | 0.000973 | 3 | 2 | 3 | 3 | 10.0 | 3 | 2 | 2 | 1 | 3 | 2 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0  
3 | 1 | 17 | 10 | 0.487179 | 2 | 3 | 1 | 0.164179 | 0.672727 | 0.205021 | 0.352438 | 0.042 | 9 | 1 | 0.0 | 3 | 0.200000 | 2 | 2 | 8 | 3 | 1 | 2 | 1 | 2 | 1 | 1 | 3 | 0.000973 | 3 | 2 | 3 | 3 | 0.0 | 350 | 2 | 2 | 1 | 3 | 2 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0  
4 | 1 | 15 | 26 | 0.230769 | 2 | 3 | 1 | 0.417910 | 0.654545 | 0.234310 | 0.424046 | 0.027 | 9 | 1 | 0.0 | 2 | 0.050000 | 1 | 2 | 6 | 3 | 1 | 2 | 1 | 2 | 1 | 1 | 3 | 0.000973 | 3 | 2 | 3 | 2 | 4.0 | 162 | 2 | 2 | 1 | 3 | 2 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0  
  
In [31]:

    
    
    from sklearn.model_selection import train_test_split
    X_train,X_test,y_train,y_test=train_test_split(X_train,y_train)
    

Machine Learning Model fitting and prediction

In [32]:

    
    
    y_train.unique()
    #there are 8 labels/class in dataset
    

Out[32]:

    
    
    array([8, 2, 1, 5, 7, 6, 3, 4])

In [33]:

    
    
    from sklearn.metrics import accuracy_score,confusion_matrix
    from sklearn.model_selection import cross_val_score,GridSearchCV
    from sklearn.multiclass import OneVsRestClassifier
    

In [34]:

    
    
    # Using a Decision Tree classifier 
    from sklearn.tree import DecisionTreeClassifier
    param_grid={'max_depth':range(1,20,2)}
    DT=DecisionTreeClassifier()
    clf_DT=GridSearchCV(DT,param_grid,cv=10,scoring='accuracy',n_jobs=-1).fit(X_train,y_train)
    y_pred=clf_DT.predict(X_test)
    print(accuracy_score(y_test,y_pred))
    
    
    
    /opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_search.py:735: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.
      DeprecationWarning)
    
    
    
    0.5162333288427859
    

In [35]:

    
    
    #Using a Random Forest tree classifier
    from sklearn.ensemble import RandomForestClassifier
    param_grid={'max_depth':range(1,20,2)}
    RF=RandomForestClassifier()
    clf_rf=GridSearchCV(RF,param_grid,cv=10,scoring='accuracy',n_jobs=-1).fit(X_train,y_train)
    y_pred=clf_rf.predict(X_test)
    accuracy_score(y_test,y_pred)
    
    
    
    /opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_search.py:735: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.
      DeprecationWarning)
    

Out[35]:

    
    
    0.4992590596793749

For now i'll use Decison tree for summission , i'll work on to improve my
predictions, any suggestion/feedback is appreciated.

In [36]:

    
    
    ids = data_test['Id']
    predictions = clf_DT.predict(data_test.drop('Id', axis=1))
    
    
    output = pd.DataFrame({ 'Id' : ids, 'Response': predictions })
    output.to_csv('/Users/adityaprakash/Downloads/predictions.csv', index = False)
    output.head()
    
    
    
    ---------------------------------------------------------------------------
    FileNotFoundError                         Traceback (most recent call last)
    <ipython-input-36-eccd6993f3ed> in <module>()
          4 
          5 output = pd.DataFrame({ 'Id' : ids, 'Response': predictions })
    ----> 6 output.to_csv('/Users/adityaprakash/Downloads/predictions.csv', index = False)
          7 output.head()
    
    /opt/conda/lib/python3.6/site-packages/pandas/core/frame.py in to_csv(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, tupleize_cols, date_format, doublequote, escapechar, decimal)
       1743                                  doublequote=doublequote,
       1744                                  escapechar=escapechar, decimal=decimal)
    -> 1745         formatter.save()
       1746 
       1747         if path_or_buf is None:
    
    /opt/conda/lib/python3.6/site-packages/pandas/io/formats/csvs.py in save(self)
        154             f, handles = _get_handle(self.path_or_buf, self.mode,
        155                                      encoding=encoding,
    --> 156                                      compression=self.compression)
        157             close = True
        158 
    
    /opt/conda/lib/python3.6/site-packages/pandas/io/common.py in _get_handle(path_or_buf, mode, encoding, compression, memory_map, is_text)
        398         elif encoding:
        399             # Python 3 and encoding
    --> 400             f = open(path_or_buf, mode, encoding=encoding)
        401         elif is_text:
        402             # Python 3 and no explicit encoding
    
    FileNotFoundError: [Errno 2] No such file or directory: '/Users/adityaprakash/Downloads/predictions.csv'

