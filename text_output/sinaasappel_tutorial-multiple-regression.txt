# Tutorial - Multiple Regression¶

## 1\. Introduction¶

In regression analysis you try to fit a predictive model to your data and use
that model to predict an outcome variable from one or more independent
predictor variables. With simple regression you try to predict an outcome
variable from a single predictor variable and with multiple regression you try
to predict an outcome variable from multiple predictor variables.

This predictive model uses a straight line to summarize the data and the
method of least squares is used to get the linear line that gives the
description (best fit) of the data.

Lets start with importing some packages and reading in the files

In [1]:

    
    
    ## Importing packages
    library(tidyverse) # metapackage with lots of helpful functions
    library(gridExtra) # to plot multiple ggplots aside
    library(car) # package for regression diagnostics
    
    ## Reading in files
    insurance <- read.csv("../input/insurance.csv")
    
    
    
    ââ Attaching packages âââââââââââââââââââââââââââââââââââââââ tidyverse 1.2.1 ââ
    â ggplot2 3.1.0.9000     â purrr   0.3.0     
    â tibble  2.0.1          â dplyr   0.7.8     
    â tidyr   0.8.2          â stringr 1.3.1     
    â readr   1.3.1          â forcats 0.3.0     
    ââ Conflicts ââââââââââââââââââââââââââââââââââââââââââ tidyverse_conflicts() ââ
    â dplyr::filter() masks stats::filter()
    â dplyr::lag()    masks stats::lag()
    
    Attaching package: âgridExtraâ
    
    The following object is masked from âpackage:dplyrâ:
    
        combine
    
    Loading required package: carData
    
    Attaching package: âcarâ
    
    The following object is masked from âpackage:dplyrâ:
    
        recode
    
    The following object is masked from âpackage:purrrâ:
    
        some
    
    

In [2]:

    
    
    ## have a peak at the data
    head(insurance)
    str(insurance)
    summary(insurance)
    

age| sex| bmi| children| smoker| region| charges  
---|---|---|---|---|---|---  
19 | female | 27.900 | 0 | yes | southwest| 16884.924  
18 | male | 33.770 | 1 | no | southeast|  1725.552  
28 | male | 33.000 | 3 | no | southeast|  4449.462  
33 | male | 22.705 | 0 | no | northwest| 21984.471  
32 | male | 28.880 | 0 | no | northwest|  3866.855  
31 | female | 25.740 | 0 | no | southeast|  3756.622  
      
    
    'data.frame':	1338 obs. of  7 variables:
     $ age     : int  19 18 28 33 32 31 46 37 37 60 ...
     $ sex     : Factor w/ 2 levels "female","male": 1 2 2 2 2 1 1 1 2 1 ...
     $ bmi     : num  27.9 33.8 33 22.7 28.9 ...
     $ children: int  0 1 3 0 0 0 1 3 2 0 ...
     $ smoker  : Factor w/ 2 levels "no","yes": 2 1 1 1 1 1 1 1 1 1 ...
     $ region  : Factor w/ 4 levels "northeast","northwest",..: 4 3 3 2 2 3 3 2 1 2 ...
     $ charges : num  16885 1726 4449 21984 3867 ...
    
    
    
          age            sex           bmi           children     smoker    
     Min.   :18.00   female:662   Min.   :15.96   Min.   :0.000   no :1064  
     1st Qu.:27.00   male  :676   1st Qu.:26.30   1st Qu.:0.000   yes: 274  
     Median :39.00                Median :30.40   Median :1.000             
     Mean   :39.21                Mean   :30.66   Mean   :1.095             
     3rd Qu.:51.00                3rd Qu.:34.69   3rd Qu.:2.000             
     Max.   :64.00                Max.   :53.13   Max.   :5.000             
           region       charges     
     northeast:324   Min.   : 1122  
     northwest:325   1st Qu.: 4740  
     southeast:364   Median : 9382  
     southwest:325   Mean   :13270  
                     3rd Qu.:16640  
                     Max.   :63770  

It is always a good idea to have a peak at the data. Here we see that the
dataset contains 1338 observations of 7 variables. The variable charges is the
one we have to predict using the following predictors: age, sex, bmi,
children, smoker and region. The variable age and bmi are continous variables,
the variables sex, smoker and region are categorical variables. In the summary
statistics we also see that there are no missings in the dataset.

It also a good idea to check how the data is coded. This helps us later with
interpreting the model. Lets start with sex

In [3]:

    
    
    summary(insurance$sex)
    table(as.numeric(insurance$sex))
    
    summary(insurance$smoker)
    table(as.numeric(insurance$smoker))
    

female

    662
male

    676
    
    
      1   2 
    662 676 

no

    1064
yes

    274
    
    
       1    2 
    1064  274 

We see that males are coded with 2 and females with 1. A no for smoking is
coded with 1 and a yes for smoking is coded with 2. Lets make some plots to
have a better look at the data.

In [4]:

    
    
    plot.age <- ggplot(insurance, aes(x = age, y = charges)) +
     geom_point()
    
    plot.bmi <- ggplot(insurance, aes(x = bmi, y = charges)) +
     geom_point()
    
    grid.arrange(plot.age, plot.bmi, ncol=2)
    

![](__results___files/__results___6_0.png)

In the first plot we see that there is a trend that with older age the charges
increase. There are also three groups/lines visible. In the second plot we see
some sort of trend that with increasing bmi the charges increase, however this
is not very clear. Here there might also be two different groups.

In [5]:

    
    
    plot.sex <- ggplot(insurance, aes(x = sex, y = charges)) +
     geom_boxplot()
    
    plot.smoker <- ggplot(insurance, aes(x = smoker, y = charges)) +
     geom_boxplot()
    
    plot.child <- ggplot(insurance, aes(x = as.factor(children), y = charges)) +
     geom_boxplot()
    
    plot.region <- ggplot(insurance, aes(x = region, y = charges)) +
     geom_boxplot()
    
    grid.arrange(plot.sex, plot.smoker, plot.child, plot.region, ncol=2, nrow=2)
    

![](__results___files/__results___8_0.png)

The first boxplot (left upper corner) shows us that females and males pay on
avarage the same charges. When looking at the second boxplot (right upper
corner) we see that smokers pay higher charges compared to non smokers. Also
people with more childres pay more charges and it seems that the region has
not an influence on the charges. In all instances the charges have a skewed
distribution.

## 2\. Simple regression¶

Lets start predicting the outcome variable charges using only the predictor
variable age. This is called simple regression. With simple linear regression
you use the method of least squares to find a line that best fits the data.
With other words finding a line that goes through as many points as possible
(or tries to be very close to those points). However, since the line is linear
it will never cross al datapoints. This means that there is a difference
between the model (the linear line) and the reality (all the points). You can
calculate the difference between the model and the reality by taking the
difference between the line and the datapoints. These differences are called
residuals. Here is the formula:

$deviation = \sum(observed - model)^{2}$

This formula simply means take the difference between each observed value and
the value according to the model, sum these differences and take the square.

### 2.1 Goodness-of-fit, sums of squares, R and R-squared¶

Imaging if we have only information on charges, than if you need to make a
prediction, the best thing you can do is taking the mean charges. This is the
most simple model available. This model is represented by the vertical black
line in the plot below. Using the mean we can calculate the difference between
the mean and all the observed values. This is called the total sum of squares
(SSt). Next you obtain information on age and use this information to make a
linear model to predict charges. This model is represented by the blue
regression line. Here you can calculate the differences between all the
observed values and the regression line. This is called the residual sum of
squares (SSr). Now it is possible to calculate the differences between the
mean value of charges and the regression line. This is called the model sum of
squares (SSm). Now you can use the SSm and the SSt to calculate the percentage
of variation in the outcome (charges) explained by the model using the
following formula:

$R^{2} = \frac{SSm}{SSt}$

This is what you use to assess goodness-of-fit.

In our case only one variable (age) is used to predict the outcome (charge),
so if you take the square root of R-squared you get the Pearson correlation
coefficient. Note that this is not working when you have more than one
predictors.

In [6]:

    
    
    # make plot 
    ggplot(insurance, aes(x = age, y = charges)) +
     geom_point() +
     geom_hline(yintercept = mean(insurance$charges)) + #add line representing the mean charges
     geom_smooth(method='lm') # add regression line
    

![](__results___files/__results___10_0.png)

In [7]:

    
    
    # simple linear regression using age as the predictor variable:
    mod1 = lm(charges ~ age, data = insurance)
    
    # show results:
    summary(mod1)
    
    
    
    Call:
    lm(formula = charges ~ age, data = insurance)
    
    Residuals:
       Min     1Q Median     3Q    Max 
     -8059  -6671  -5939   5440  47829 
    
    Coefficients:
                Estimate Std. Error t value Pr(>|t|)    
    (Intercept)   3165.9      937.1   3.378 0.000751 ***
    age            257.7       22.5  11.453  < 2e-16 ***
    ---
    Signif. codes:  0 â***â 0.001 â**â 0.01 â*â 0.05 â.â 0.1 â â 1
    
    Residual standard error: 11560 on 1336 degrees of freedom
    Multiple R-squared:  0.08941,	Adjusted R-squared:  0.08872 
    F-statistic: 131.2 on 1 and 1336 DF,  p-value: < 2.2e-16
    

The above output the is the summary of our model. It starts with Call: which
shows the function used as input. However we start at almost the bottom. Here
you see Multiple R-squared: 0.08941. This value represents the square R
between age and charges and also tells us that 8.94% of the variation in the
outcome variable charges is explained by the predictor variable age. When you
take the square root of square-R, you also get the correlation coefficient
between age and charges.

In [8]:

    
    
    # to show the correlation between age and age.
    sqrt(0.08941)
    

0.299015049788468

The last line gives us the F-ratio which is 131.2 and its p-value which is
smaller than 0.001. So we know that our model with age as predictor is better
than if we would use the mean charges. The output also shows the coefficients.
The coefficient for age is 257.7, this means that with every increase in age
(in years) the charges increase with 257.7. The intercept is at 3165.9. Thus
our model for predicting charges using age becomes: charges = 3165.9 + (257.7
* age in years).

## 3\. Multiple regression¶

The formula for multiple regression looks as follows:

$Y_i = (b_0 + b_1X_1 + b_2X_2 + ... + b_nX_n) + e_i$

$Y$ is the outcome variable, $b_1$ is the coefficient for the first predictor
($X_1$), $b_2$ is the coefficient for the second predictor ($X_2$) and $b_n$
is the coefficient for the nth predictor ($X_n$). $b_o$ is the intercept, the
point were the regression line crosses the y-axis. Ei is the difference
between the predicted and the observed value of $Y$ for the ith participant.
Lets put this into practice by adding a second variable to our model, for
example body mass index (BMI). Here is how the model looks like:

$Charges = b_0 + b_1*age_i + b_2*bmi_i + e_i$

Lets apply this in R:

In [9]:

    
    
    # multiple linear regression using age and bmi as predictor variables:
    mod2 = lm(charges ~ age + bmi, data = insurance)
    
    # show results:
    summary(mod2)
    
    
    
    Call:
    lm(formula = charges ~ age + bmi, data = insurance)
    
    Residuals:
       Min     1Q Median     3Q    Max 
    -14457  -7045  -5136   7211  48022 
    
    Coefficients:
                Estimate Std. Error t value Pr(>|t|)    
    (Intercept) -6424.80    1744.09  -3.684 0.000239 ***
    age           241.93      22.30  10.850  < 2e-16 ***
    bmi           332.97      51.37   6.481 1.28e-10 ***
    ---
    Signif. codes:  0 â***â 0.001 â**â 0.01 â*â 0.05 â.â 0.1 â â 1
    
    Residual standard error: 11390 on 1335 degrees of freedom
    Multiple R-squared:  0.1172,	Adjusted R-squared:  0.1159 
    F-statistic:  88.6 on 2 and 1335 DF,  p-value: < 2.2e-16
    

In the output we see that we have a multiple R-squared of 0.1172, this means
that our model explains 11.72% of the variation in the outcome variable.
Multiple R-squared can range from 1 to 0, were 1 means that the model
perfectly fits the observed data and explains 100% of the variation in the
outcome variable. In our model with only the predictor age, the R-squared was
0.08941. Thus adding BMI to the model improves the fit. But how do we know if
this improvement in R-square is significant? Well we can compare both models
by calculating the F-ratio. Here is how to do is in R:

In [10]:

    
    
    anova(mod1, mod2)
    

Res.Df| RSS| Df| Sum of Sq| F| Pr(>F)  
---|---|---|---|---|---  
1336 | 178544029385| NA |  NA |  NA |  NA  
1335 | 173097580364|  1 | 5446449021 | 42.00526 | 1.277594e-10  
  
The output shows that the F-value is 42.00, and with a p<0.001 we know that
the change in explained variance is significant. Note that you can compare
only hierarchical models. So the second model should contain all variables
from the first model plus some new and the third model should contain all
variables from the second model plus some new and so on.

### 3.1 Methods of regression¶

When you are building a model with several predictors, how do you know which
predictors to use? The best practice is that the selection of predictors you
want to add to your model are based on previous research. At least you should
not add hundreds of random predictors to you model. Another problem is to
decide in which order you should enter the variables in the model. If all the
predictors are not correlated, than the order is not a problem, however this
usually not the case.

There are several ways to decide on the order about putting variables in the
model

  * Hierarchical or blockwise entry: the predictors are based on past work and the researcher decides in which order the variables are entered in the model. This order should be based on the importance of the variables. The most important variable is entered first and so on. Than the new predictors can be entered.

  * forced entry: all the predictors are put in the model at once

  * stepwise methods: the order in which the predictors are entered in the model are based on mathematical criteria. You can use a forward and a backward method. In a forward method the first variable that in entered in the model, is the one that explains most of the variation of the outcome variable, the next variable entered in the model explains the largest part of the remaining variation and so on. In a backward method all the variables are entered in the model and one by one the variables are removed that explain the smallest part of the variation. To avoid overfitting it is important to cross-validate the model.

  * all sub-sets method: need to explain this further

In this tutorial I assume I did background research (which I didn't) and a
found that smoking was the most important predictor for charges, next came age
and than bmi. I want to know if sex, region and the number of children will
improve the model. First I make a model that includes the known predictors.
Since I don't which of the new predictors (sex, region or number of children
is important, I will add then add once in the new model.

For a following tutorial it would be interesting to compare the different
approaches.

Lets start with the first model containing the predictors smoker, age and bmi

In [11]:

    
    
    mod3 <- lm(charges ~ smoker + age + bmi, data = insurance)
    summary(mod3)
    
    
    
    Call:
    lm(formula = charges ~ smoker + age + bmi, data = insurance)
    
    Residuals:
         Min       1Q   Median       3Q      Max 
    -12415.4  -2970.9   -980.5   1480.0  28971.8 
    
    Coefficients:
                 Estimate Std. Error t value Pr(>|t|)    
    (Intercept) -11676.83     937.57  -12.45   <2e-16 ***
    smokeryes    23823.68     412.87   57.70   <2e-16 ***
    age            259.55      11.93   21.75   <2e-16 ***
    bmi            322.62      27.49   11.74   <2e-16 ***
    ---
    Signif. codes:  0 â***â 0.001 â**â 0.01 â*â 0.05 â.â 0.1 â â 1
    
    Residual standard error: 6092 on 1334 degrees of freedom
    Multiple R-squared:  0.7475,	Adjusted R-squared:  0.7469 
    F-statistic:  1316 on 3 and 1334 DF,  p-value: < 2.2e-16
    

When looking at the R-squared I see that 74.75 percent of the variation in the
outcome variable charge is explained by the three predictor variables smoker,
age and bmi. The adjusted R-square is almost equal to the multiple R-square,
this meaning that the model has good cross-validity. From the F-statistic
(P<0.001) we can conclude that the model is better than just taking the mean
charges. We also notice that the predictor variables are all significant. From
the coefficients we can make our model:

$charges = -11676.83 + 23823.68*smoker + 259.55*age + 322.62*bmi + error$

If we want to predict charges for a person who is 36 years old and smoker and
having a bmi of 25, we simply have to fill the model. Note that a person who
is smoking is coded by 2, thus for smoker we will in "2":

$-11676.83 + 23823.68*2 + 259.55*36 + 322.62*25 = 53.379,83$

Next we make the following model were we also include the predictor variables
sex, children and region.

In [12]:

    
    
    mod4 <- lm(charges ~ smoker + age + bmi + sex + children + region, data = insurance)
    summary(mod4)
    
    
    
    Call:
    lm(formula = charges ~ smoker + age + bmi + sex + children + 
        region, data = insurance)
    
    Residuals:
         Min       1Q   Median       3Q      Max 
    -11304.9  -2848.1   -982.1   1393.9  29992.8 
    
    Coefficients:
                    Estimate Std. Error t value Pr(>|t|)    
    (Intercept)     -11938.5      987.8 -12.086  < 2e-16 ***
    smokeryes        23848.5      413.1  57.723  < 2e-16 ***
    age                256.9       11.9  21.587  < 2e-16 ***
    bmi                339.2       28.6  11.860  < 2e-16 ***
    sexmale           -131.3      332.9  -0.394 0.693348    
    children           475.5      137.8   3.451 0.000577 ***
    regionnorthwest   -353.0      476.3  -0.741 0.458769    
    regionsoutheast  -1035.0      478.7  -2.162 0.030782 *  
    regionsouthwest   -960.0      477.9  -2.009 0.044765 *  
    ---
    Signif. codes:  0 â***â 0.001 â**â 0.01 â*â 0.05 â.â 0.1 â â 1
    
    Residual standard error: 6062 on 1329 degrees of freedom
    Multiple R-squared:  0.7509,	Adjusted R-squared:  0.7494 
    F-statistic: 500.8 on 8 and 1329 DF,  p-value: < 2.2e-16
    

In the new model the multiple R_square is 0.7509, meaning that the new model
explains 75.09 of the variation in charges. When looking at the coefficients
we see that sex is not significant. Lets remove sex from the model. Lets
compare both models

In [13]:

    
    
    mod5 <- lm(charges ~ smoker + age + bmi + children + region, data = insurance)
    summary(mod5)
    
    
    
    Call:
    lm(formula = charges ~ smoker + age + bmi + children + region, 
        data = insurance)
    
    Residuals:
         Min       1Q   Median       3Q      Max 
    -11367.2  -2835.4   -979.7   1361.9  29935.5 
    
    Coefficients:
                     Estimate Std. Error t value Pr(>|t|)    
    (Intercept)     -11990.27     978.76 -12.250  < 2e-16 ***
    smokeryes        23836.30     411.86  57.875  < 2e-16 ***
    age                256.97      11.89  21.610  < 2e-16 ***
    bmi                338.66      28.56  11.858  < 2e-16 ***
    children           474.57     137.74   3.445 0.000588 ***
    regionnorthwest   -352.18     476.12  -0.740 0.459618    
    regionsoutheast  -1034.36     478.54  -2.162 0.030834 *  
    regionsouthwest   -959.37     477.78  -2.008 0.044846 *  
    ---
    Signif. codes:  0 â***â 0.001 â**â 0.01 â*â 0.05 â.â 0.1 â â 1
    
    Residual standard error: 6060 on 1330 degrees of freedom
    Multiple R-squared:  0.7509,	Adjusted R-squared:  0.7496 
    F-statistic: 572.7 on 7 and 1330 DF,  p-value: < 2.2e-16
    

After removing sex from the model we see that the model stil explains 75.09
percent of the variation in charges.

In [14]:

    
    
    anova(mod3, mod5)
    

Res.Df| RSS| Df| Sum of Sq| F| Pr(>F)  
---|---|---|---|---|---  
1334 | 49513219514| NA |  NA |  NA |  NA  
1330 | 48845249273|  4 | 667970241 | 4.547015 | 0.001191318  
  
We see a p-value of 0.002, meaning that our new model gives a better
prediction than the old model which uses smoker, age and bmi.

### 3.2 Using categorical variables¶

One of the assumption of multiple regression is that the predictor variables
are numeric or are categorical with maximal two categories, for example sex
(male, female) or smoker(yes, no). However in our dataset we have the variable
region containing four categories. normally you should use dummy variables.
However this is something the lm function in R does automatically. Remember
that R does not know which category to use as a reference.

## 4\. The accuracy of the model and checking assumptions¶

It is always important to assess the accuracy of the model.

### 4.1 Outliers and influential cases¶

An outlier is a case that differs from the main trend in the data. These
outliers can have a very large effect on the model. If this outlier is caused
for example by a measurement error you better remove this observation. One way
of detecting outliers is to look at the differences between the observered and
predicted values (aka residuals). An outlier would show a large difference
between the predicted and observerd value. Thus to spot outliers we obtain the
standardized residuals.

An influential case is a case that has a substantial effect on the regression
analysis. Thus if the influential case is removed from the analysis, than we
would have a very different regression line. However these influential are not
outliers perse, this means that they cannot be detected by looking at the
residuals.

In [15]:

    
    
    # obtain residuals, cooks distance for model 5 and add to dataframe
    insurance$standardized.residuals <- rstandard(mod5)
    insurance$cooks.distance <- cooks.distance(mod5) # to spot influential cases
    
    # indicate which standardized residuals are > 2 and add to dataframe
    insurance$large.residual2 <- insurance$standardized.residuals > 2 | insurance$standardized.residuals < -2
    
    # indicate which standardized residuals are > 2.5 and add to dataframe
    insurance$large.residual2.5 <- insurance$standardized.residuals > 2.5 | insurance$standardized.residuals < -2.5
    
    # indicate which cases are outliers (standardized residual > 3)
    insurance$outlier.residual <- insurance$standardized.residuals > 3 | insurance$standardized.residuals < -3
    
    # percentage of cases with residual > 2 (this should 5% or lower)
    paste("The percentage of cases with a standardized residual > 2 is", 
          round(sum(insurance$large.residual2)/nrow(insurance)*100, 2), "percent.")
    
    # percentage of cases with residual > 2.5 (this should be 1% or lower)
    paste("The percentage of cases with a standardized residual > 2.5 is", 
          round(sum(insurance$large.residual2.5)/nrow(insurance)*100, 2), "percent.")
    
    # potential outliers
    nrow(insurance[which(insurance$outlier.residual==T),])
    

'The percentage of cases with a standardized residual > 2 is 5.01 percent.'

'The percentage of cases with a standardized residual > 2.5 is 3.29 percent.'

28

The percentage of cases with a standardized residual larger than 2 is 5.01
percent. It is expected that 95% of the cases are within the boundaries of -2
and 2. So, 5.01 percent of cases with a standardized residual larger than 2 is
what you would expect.

The percentage of cases with a standardized resisuals larger than 2.5 is 3.29
percent. This is a bit to much. Lets have a look at these cases:

In [16]:

    
    
    # get cases with standardized residuals larger than 2.5
    insurance %>% filter(large.residual2.5==T)
    

age| sex| bmi| children| smoker| region| charges| standardized.residuals|
cooks.distance| large.residual2| large.residual2.5| outlier.residual  
---|---|---|---|---|---|---|---|---|---|---|---  
33 | male | 22.705 | 0 | no | northwest | 21984.47 | 3.003498 | 0.005511057| TRUE | TRUE |  TRUE   
60 | female | 25.840 | 0 | no | northwest | 28923.14 | 2.829583 | 0.006061828| TRUE | TRUE | FALSE   
28 | male | 36.400 | 1 | yes | southwest | 51194.56 | 3.364270 | 0.010802184| TRUE | TRUE |  TRUE   
64 | male | 24.700 | 1 | no | northwest | 30166.62 | 2.850994 | 0.006422368| TRUE | TRUE | FALSE   
18 | female | 30.115 | 0 | no | northeast | 21344.85 | 3.063062 | 0.006597718| TRUE | TRUE |  TRUE   
60 | male | 28.595 | 0 | no | northeast | 30260.00 | 2.837505 | 0.005648275| TRUE | TRUE | FALSE   
34 | male | 22.420 | 2 | no | northeast | 27375.90 | 3.653512 | 0.008145827| TRUE | TRUE |  TRUE   
24 | female | 23.210 | 0 | no | southeast | 25081.77 | 3.985971 | 0.012732783| TRUE | TRUE |  TRUE   
55 | female | 26.800 | 1 | no | southwest | 35160.13 | 4.039796 | 0.009436536| TRUE | TRUE |  TRUE   
52 | male | 26.400 | 3 | no | southeast | 25992.82 | 2.531821 | 0.005554385| TRUE | TRUE | FALSE   
28 | female | 27.500 | 2 | no | southwest | 20177.67 | 2.591268 | 0.003635288| TRUE | TRUE | FALSE   
26 | female | 29.640 | 4 | no | northeast | 24671.66 | 2.990506 | 0.009768325| TRUE | TRUE | FALSE   
24 | female | 27.600 | 0 | no | southwest | 18955.22 | 2.711192 | 0.004484614| TRUE | TRUE | FALSE   
46 | male | 27.600 | 0 | no | southwest | 24603.05 | 2.709523 | 0.003972426| TRUE | TRUE | FALSE   
50 | male | 25.365 | 2 | no | northwest | 30284.64 | 3.347117 | 0.006243464| TRUE | TRUE |  TRUE   
19 | male | 33.100 | 0 | no | southwest | 23082.96 | 3.299789 | 0.007792970| TRUE | TRUE |  TRUE   
28 | female | 24.320 | 1 | no | northeast | 23288.93 | 3.203512 | 0.005409111| TRUE | TRUE |  TRUE   
20 | male | 35.310 | 1 | no | southeast | 27724.29 | 3.833250 | 0.008496341| TRUE | TRUE |  TRUE   
50 | female | 27.360 | 0 | no | northeast | 25656.58 | 2.568699 | 0.003665077| TRUE | TRUE | FALSE   
19 | female | 30.590 | 2 | no | northwest | 24059.68 | 3.343979 | 0.007529078| TRUE | TRUE |  TRUE   
53 | male | 31.350 | 0 | no | southeast | 27346.04 | 2.668346 | 0.004055232| TRUE | TRUE | FALSE   
54 | female | 47.410 | 0 | yes | southeast | 63770.43 | 3.820638 | 0.020287139| TRUE | TRUE |  TRUE   
31 | female | 38.095 | 1 | yes | northeast | 58571.07 | 4.205737 | 0.018268180| TRUE | TRUE |  TRUE   
52 | female | 37.525 | 2 | no | northwest | 33471.97 | 3.109778 | 0.006638454| TRUE | TRUE |  TRUE   
47 | female | 24.100 | 1 | no | southwest | 26236.58 | 3.054985 | 0.005269040| TRUE | TRUE |  TRUE   
53 | female | 32.300 | 2 | no | northeast | 29186.48 | 2.591474 | 0.003823039| TRUE | TRUE | FALSE   
40 | female | 41.420 | 1 | no | northwest | 28476.73 | 2.655160 | 0.005784369| TRUE | TRUE | FALSE   
33 | female | 35.530 | 0 | yes | northwest | 55135.40 | 3.831920 | 0.014861094| TRUE | TRUE |  TRUE   
49 | female | 27.100 | 1 | no | southwest | 26140.36 | 2.785245 | 0.003820740| TRUE | TRUE | FALSE   
44 | male | 29.735 | 2 | no | northeast | 32108.66 | 3.599649 | 0.006216543| TRUE | TRUE |  TRUE   
48 | male | 36.670 | 1 | no | northwest | 28468.92 | 2.577377 | 0.003853845| TRUE | TRUE | FALSE   
45 | female | 27.645 | 1 | no | northwest | 28340.19 | 3.187214 | 0.004376861| TRUE | TRUE |  TRUE   
25 | male | 24.985 | 2 | no | northeast | 23241.47 | 3.208545 | 0.006326010| TRUE | TRUE |  TRUE   
61 | female | 33.330 | 4 | no | southeast | 36580.28 | 3.438724 | 0.013671755| TRUE | TRUE |  TRUE   
21 | female | 32.680 | 2 | no | northwest | 26018.95 | 3.465987 | 0.008072640| TRUE | TRUE |  TRUE   
23 | male | 18.715 | 0 | no | northwest | 21595.38 | 3.591431 | 0.011225529| TRUE | TRUE |  TRUE   
19 | male | 27.265 | 2 | no | northwest | 22493.66 | 3.271036 | 0.007081307| TRUE | TRUE |  TRUE   
52 | female | 24.860 | 0 | no | southeast | 27117.99 | 3.039047 | 0.007099334| TRUE | TRUE |  TRUE   
60 | male | 32.800 | 0 | yes | southwest | 52590.83 | 2.515499 | 0.006979210| TRUE | TRUE | FALSE   
19 | female | 27.930 | 3 | no | northwest | 18838.70 | 2.552427 | 0.005514411| TRUE | TRUE | FALSE   
59 | female | 34.800 | 2 | no | southwest | 36910.61 | 3.633799 | 0.008626227| TRUE | TRUE |  TRUE   
52 | male | 34.485 | 3 | yes | northwest | 60021.40 | 3.657047 | 0.015185934| TRUE | TRUE |  TRUE   
45 | male | 30.360 | 0 | yes | southeast | 62592.87 | 4.955464 | 0.019607397| TRUE | TRUE |  TRUE   
23 | female | 24.225 | 2 | no | northeast | 22395.74 | 3.196813 | 0.006741456| TRUE | TRUE |  TRUE   
  
Put some explanation here:

In [17]:

    
    
    # cases with cooks distance larger than 1
    insurance %>% filter(cooks.distance>=1)
    

age| sex| bmi| children| smoker| region| charges| standardized.residuals|
cooks.distance| large.residual2| large.residual2.5| outlier.residual  
---|---|---|---|---|---|---|---|---|---|---|---  
  
There are no cases with a cooks distance larger than 1, meaning that there are
no influential cases.

### 4.2 Checking the assumption of indepence¶

This can be done with the Durbin Watsons test

In [18]:

    
    
    dwt(mod5)
    
    
    
     lag Autocorrelation D-W Statistic p-value
       1     -0.04582739      2.088964   0.104
     Alternative hypothesis: rho != 0

The D-W statistic is very close to 2 and we see a p-value larger than 0.5.
This means that the assumption of indepence is met.

### 4.3 Checking the assumption of multicollinearity¶

When checking the assumption of multicollinearity you check if one or more
variables used in the model are not to strongly correlated amongst eachother.
At least the correlation between the predictors should not be to high (that is
higher than 0.80). One way to check is this, is to make a correlation matrix
with all predictor variables. An easier way is to look at the variance
inflation factor (VIF). The VIF should not be higher than 10.

In [19]:

    
    
    # VIF
    vif(mod5)
    # tolerance
    1/vif(mod5)
    # mean VIF
    mean(vif(mod5))
    

| GVIF| Df| GVIF^(1/(2*Df))  
---|---|---|---  
smoker| 1.006369| 1 | 1.003179  
age| 1.016188| 1 | 1.008061  
bmi| 1.104197| 1 | 1.050808  
children| 1.003714| 1 | 1.001855  
region| 1.098870| 3 | 1.015838  
  
| GVIF| Df| GVIF^(1/(2*Df))  
---|---|---|---  
smoker| 0.9936716| 1.0000000| 0.9968308  
age| 0.9840702| 1.0000000| 0.9920031  
bmi| 0.9056351| 1.0000000| 0.9516486  
children| 0.9962997| 1.0000000| 0.9981481  
region| 0.9100262| 0.3333333| 0.9844092  
  
1.15393860894511

A VIF larger than 10 indicates multicolinearity. We have no VIF larger than
10, so this indicates no multicollinearity. A tolerance lower 0.2 indicates
problems. In our cases the tolerance levels are all higher than 0.2, thus no
problems here. Also the mean VIF is around 1, so no bias here.

### 4.4 Checking assumptions about residuals.¶

To check this, you need to plot the standardized residuals (on the y-axis)
against and the predicted residuals (on the x-axis). This plot should look
like random dots evenly distributed amongst the zero line.

In [20]:

    
    
    plot(mod5)
    

![](__results___files/__results___42_0.png)

![](__results___files/__results___42_1.png)

![](__results___files/__results___42_2.png)

![](__results___files/__results___42_3.png)

We have plotted for graphs. The first graph shows the fitted values against
the observed resididuals. In this plot the dots should be randomly placed
around the horizontal zero line, which is clearly not the case. It seems that
there are three groups present. Thus per group there is difference in variance
across the residuals.

There is increasing variance across the residuals (heteroscedasticity), and
there migth also be a non-linear relationship between the outcome and the
predictor. This non-linear relationship is also reflected by the second plot
(Q-Q plot), since the dots deviate from the dotted line.

#### tackling the non-linear relationship¶

The outcome variable was charges, the continuous predictor variables were age
and bmi. When looking at the first plot we made (charges versus age) you see
indeed een small curve. A method of solving the problem is to transform the
data.

In [21]:

    
    
    plot.age2 <- ggplot(insurance, aes(x = age^2, y = charges)) +
     geom_point()
    
    grid.arrange(plot.age, plot.age2, ncol=2)
    

![](__results___files/__results___45_0.png)

In the second plot we see there is a straigth line. Thus we should use age
square in our model.

In [22]:

    
    
    # make new variable age.square
    insurance <- insurance %>% 
    mutate(age.square = age^2) 
    
    mod6 <- lm(charges ~ smoker + age + age.square + bmi + children + region, data = insurance)
    summary(mod6)
    
    
    
    Call:
    lm(formula = charges ~ smoker + age + age.square + bmi + children + 
        region, data = insurance)
    
    Residuals:
         Min       1Q   Median       3Q      Max 
    -11726.0  -2905.4   -926.3   1306.3  30765.1 
    
    Coefficients:
                     Estimate Std. Error t value Pr(>|t|)    
    (Intercept)     -6654.371   1683.270  -3.953 8.12e-05 ***
    smokeryes       23846.842    409.699  58.206  < 2e-16 ***
    age               -54.266     80.962  -0.670 0.502807    
    age.square          3.925      1.010   3.886 0.000107 ***
    bmi               334.656     28.427  11.772  < 2e-16 ***
    children          640.940    143.549   4.465 8.69e-06 ***
    regionnorthwest  -366.979    473.632  -0.775 0.438584    
    regionsoutheast -1030.808    476.021  -2.165 0.030530 *  
    regionsouthwest  -956.835    475.266  -2.013 0.044288 *  
    ---
    Signif. codes:  0 â***â 0.001 â**â 0.01 â*â 0.05 â.â 0.1 â â 1
    
    Residual standard error: 6028 on 1329 degrees of freedom
    Multiple R-squared:  0.7537,	Adjusted R-squared:  0.7522 
    F-statistic: 508.3 on 8 and 1329 DF,  p-value: < 2.2e-16
    

When we added age.square in the model, 75.23 percent of the variation is
explained. Let's have a lpo

In [23]:

    
    
    ggplot(insurance, aes(x = bmi, y = charges, col = smoker)) +
     geom_point()
    

![](__results___files/__results___49_0.png)

It seems that there is an interaction between smoke and bmi. Lets put this
interaction in the model.

In [24]:

    
    
    mod7 <- lm(charges ~ smoker + age.square + bmi + children + region + smoker*bmi, data = insurance)
    summary(mod7)
    plot(mod7)
    
    
    
    Call:
    lm(formula = charges ~ smoker + age.square + bmi + children + 
        region + smoker * bmi, data = insurance)
    
    Residuals:
         Min       1Q   Median       3Q      Max 
    -14915.4  -1620.6  -1314.4   -875.3  31032.7 
    
    Coefficients:
                      Estimate Std. Error t value Pr(>|t|)    
    (Intercept)      2.052e+03  8.117e+02   2.528  0.01158 *  
    smokeryes       -2.025e+04  1.636e+03 -12.377  < 2e-16 ***
    age.square       3.338e+00  1.179e-01  28.326  < 2e-16 ***
    bmi              1.940e+01  2.543e+01   0.763  0.44566    
    children         6.539e+02  1.093e+02   5.982 2.84e-09 ***
    regionnorthwest -5.939e+02  3.782e+02  -1.570  0.11658    
    regionsoutheast -1.203e+03  3.801e+02  -3.164  0.00159 ** 
    regionsouthwest -1.225e+03  3.796e+02  -3.227  0.00128 ** 
    smokeryes:bmi    1.436e+03  5.221e+01  27.510  < 2e-16 ***
    ---
    Signif. codes:  0 â***â 0.001 â**â 0.01 â*â 0.05 â.â 0.1 â â 1
    
    Residual standard error: 4813 on 1329 degrees of freedom
    Multiple R-squared:  0.843,	Adjusted R-squared:  0.8421 
    F-statistic:   892 on 8 and 1329 DF,  p-value: < 2.2e-16
    

![](__results___files/__results___51_1.png)

![](__results___files/__results___51_2.png)

![](__results___files/__results___51_3.png)

![](__results___files/__results___51_4.png)

