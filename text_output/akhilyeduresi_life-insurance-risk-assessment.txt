In [1]:

    
    
    # This Python 3 environment comes with many helpful analytics libraries installed
    # It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
    # For example, here's several helpful packages to load in 
    #loading libraries
    
    import os
    import gc
    import numpy as np # linear algebra
    import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
    import seaborn as sns
    import matplotlib.pyplot as plt
    %matplotlib inline
    
    from sklearn import metrics
    from sklearn.metrics import roc_auc_score
    from sklearn.impute import SimpleImputer
    from sklearn.preprocessing import MinMaxScaler
    from sklearn.metrics import accuracy_score
    from sklearn.model_selection import train_test_split
    from sklearn.ensemble import ExtraTreesClassifier
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import confusion_matrix, precision_recall_curve
    from sklearn.metrics import recall_score, classification_report, auc, roc_curve
    from sklearn.metrics import precision_recall_fscore_support, f1_score
    from sklearn.preprocessing import StandardScaler
    from sklearn.metrics import accuracy_score
    from sklearn.metrics import cohen_kappa_score
    from sklearn.metrics import precision_score
    from sklearn.metrics import recall_score
    
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.tree import DecisionTreeClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.naive_bayes import GaussianNB
    from sklearn.svm import SVC
    
    from sklearn.model_selection import KFold
    from sklearn.model_selection import cross_val_score
    k_fold = KFold(n_splits=10, shuffle=True, random_state=0)
    
    from keras import losses
    from keras.utils import to_categorical
    from keras.layers import Input, Dense, Dropout
    from keras.models import Model, Sequential 
    from keras.optimizers import Adam
    from keras import optimizers
    from keras import backend as K
    from keras.callbacks import Callback
    from keras.models import Model, load_model
    from keras.layers import Input, Dense
    from keras.callbacks import ModelCheckpoint, TensorBoard
    from keras import regularizers
    
    
    import warnings
    warnings.filterwarnings('ignore')
    
    from scipy import stats
    import tensorflow as tf
    import pickle
    
    from pylab import rcParams
    
    
    
    Using TensorFlow backend.
    

In [2]:

    
    
    #Loading train data
    train=pd.read_csv("../input/train.csv")
    

Data Exploration

In [3]:

    
    
    #Data
    train.head()
    

Out[3]:

| Id | Product_Info_1 | Product_Info_2 | Product_Info_3 | Product_Info_4 | Product_Info_5 | Product_Info_6 | Product_Info_7 | Ins_Age | Ht | Wt | BMI | Employment_Info_1 | Employment_Info_2 | Employment_Info_3 | Employment_Info_4 | Employment_Info_5 | Employment_Info_6 | InsuredInfo_1 | InsuredInfo_2 | InsuredInfo_3 | InsuredInfo_4 | InsuredInfo_5 | InsuredInfo_6 | InsuredInfo_7 | Insurance_History_1 | Insurance_History_2 | Insurance_History_3 | Insurance_History_4 | Insurance_History_5 | Insurance_History_7 | Insurance_History_8 | Insurance_History_9 | Family_Hist_1 | Family_Hist_2 | Family_Hist_3 | Family_Hist_4 | Family_Hist_5 | Medical_History_1 | Medical_History_2 | ... | Medical_Keyword_10 | Medical_Keyword_11 | Medical_Keyword_12 | Medical_Keyword_13 | Medical_Keyword_14 | Medical_Keyword_15 | Medical_Keyword_16 | Medical_Keyword_17 | Medical_Keyword_18 | Medical_Keyword_19 | Medical_Keyword_20 | Medical_Keyword_21 | Medical_Keyword_22 | Medical_Keyword_23 | Medical_Keyword_24 | Medical_Keyword_25 | Medical_Keyword_26 | Medical_Keyword_27 | Medical_Keyword_28 | Medical_Keyword_29 | Medical_Keyword_30 | Medical_Keyword_31 | Medical_Keyword_32 | Medical_Keyword_33 | Medical_Keyword_34 | Medical_Keyword_35 | Medical_Keyword_36 | Medical_Keyword_37 | Medical_Keyword_38 | Medical_Keyword_39 | Medical_Keyword_40 | Medical_Keyword_41 | Medical_Keyword_42 | Medical_Keyword_43 | Medical_Keyword_44 | Medical_Keyword_45 | Medical_Keyword_46 | Medical_Keyword_47 | Medical_Keyword_48 | Response  
---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---  
0 | 2 | 1 | D3 | 10 | 0.076923 | 2 | 1 | 1 | 0.641791 | 0.581818 | 0.148536 | 0.323008 | 0.028 | 12 | 1 | 0.0 | 3 | NaN | 1 | 2 | 6 | 3 | 1 | 2 | 1 | 1 | 1 | 3 | 1 | 0.000667 | 1 | 1 | 2 | 2 | NaN | 0.598039 | NaN | 0.526786 | 4.0 | 112 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 8  
1 | 5 | 1 | A1 | 26 | 0.076923 | 2 | 3 | 1 | 0.059701 | 0.600000 | 0.131799 | 0.272288 | 0.000 | 1 | 3 | 0.0 | 2 | 0.0018 | 1 | 2 | 6 | 3 | 1 | 2 | 1 | 2 | 1 | 3 | 1 | 0.000133 | 1 | 3 | 2 | 2 | 0.188406 | NaN | 0.084507 | NaN | 5.0 | 412 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 4  
2 | 6 | 1 | E1 | 26 | 0.076923 | 2 | 3 | 1 | 0.029851 | 0.745455 | 0.288703 | 0.428780 | 0.030 | 9 | 1 | 0.0 | 2 | 0.0300 | 1 | 2 | 8 | 3 | 1 | 1 | 1 | 2 | 1 | 1 | 3 | NaN | 3 | 2 | 3 | 3 | 0.304348 | NaN | 0.225352 | NaN | 10.0 | 3 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 8  
3 | 7 | 1 | D4 | 10 | 0.487179 | 2 | 3 | 1 | 0.164179 | 0.672727 | 0.205021 | 0.352438 | 0.042 | 9 | 1 | 0.0 | 3 | 0.2000 | 2 | 2 | 8 | 3 | 1 | 2 | 1 | 2 | 1 | 1 | 3 | NaN | 3 | 2 | 3 | 3 | 0.420290 | NaN | 0.352113 | NaN | 0.0 | 350 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 8  
4 | 8 | 1 | D2 | 26 | 0.230769 | 2 | 3 | 1 | 0.417910 | 0.654545 | 0.234310 | 0.424046 | 0.027 | 9 | 1 | 0.0 | 2 | 0.0500 | 1 | 2 | 6 | 3 | 1 | 2 | 1 | 2 | 1 | 1 | 3 | NaN | 3 | 2 | 3 | 2 | 0.463768 | NaN | 0.408451 | NaN | NaN | 162 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 8  
  
In [4]:

    
    
    #Shape of data
    train.shape
    

Out[4]:

    
    
    (59381, 128)

In [5]:

    
    
    #Exploring missing values
    train.isnull().sum()[train.isnull().sum() !=0]
    

Out[5]:

    
    
    Employment_Info_1         19
    Employment_Info_4       6779
    Employment_Info_6      10854
    Insurance_History_5    25396
    Family_Hist_2          28656
    Family_Hist_3          34241
    Family_Hist_4          19184
    Family_Hist_5          41811
    Medical_History_1       8889
    Medical_History_10     58824
    Medical_History_15     44596
    Medical_History_24     55580
    Medical_History_32     58274
    dtype: int64

In [6]:

    
    
    #Exploring missing values
    train_missing= train.isnull().sum()[train.isnull().sum() !=0]
    train_missing=pd.DataFrame(train_missing.reset_index())
    train_missing.rename(columns={'index':'features',0:'missing_count'},inplace=True)
    train_missing['missing_count_percentage']=((train_missing['missing_count'])/59381)*100
    plt.figure(figsize=(20,8))
    sns.barplot(y=train_missing['features'],x=train_missing['missing_count_percentage'])
    train_missing
    

Out[6]:

| features | missing_count | missing_count_percentage  
---|---|---|---  
0 | Employment_Info_1 | 19 | 0.031997  
1 | Employment_Info_4 | 6779 | 11.416110  
2 | Employment_Info_6 | 10854 | 18.278574  
3 | Insurance_History_5 | 25396 | 42.767889  
4 | Family_Hist_2 | 28656 | 48.257860  
5 | Family_Hist_3 | 34241 | 57.663226  
6 | Family_Hist_4 | 19184 | 32.306630  
7 | Family_Hist_5 | 41811 | 70.411411  
8 | Medical_History_1 | 8889 | 14.969435  
9 | Medical_History_10 | 58824 | 99.061990  
10 | Medical_History_15 | 44596 | 75.101463  
11 | Medical_History_24 | 55580 | 93.598963  
12 | Medical_History_32 | 58274 | 98.135767  
  
![](__results___files/__results___6_1.png)

In [7]:

    
    
    #checking data types
    train.dtypes.unique()
    

Out[7]:

    
    
    array([dtype('int64'), dtype('O'), dtype('float64')], dtype=object)

In [8]:

    
    
    #Outliers detection
    train.describe()
    

Out[8]:

| Id | Product_Info_1 | Product_Info_3 | Product_Info_4 | Product_Info_5 | Product_Info_6 | Product_Info_7 | Ins_Age | Ht | Wt | BMI | Employment_Info_1 | Employment_Info_2 | Employment_Info_3 | Employment_Info_4 | Employment_Info_5 | Employment_Info_6 | InsuredInfo_1 | InsuredInfo_2 | InsuredInfo_3 | InsuredInfo_4 | InsuredInfo_5 | InsuredInfo_6 | InsuredInfo_7 | Insurance_History_1 | Insurance_History_2 | Insurance_History_3 | Insurance_History_4 | Insurance_History_5 | Insurance_History_7 | Insurance_History_8 | Insurance_History_9 | Family_Hist_1 | Family_Hist_2 | Family_Hist_3 | Family_Hist_4 | Family_Hist_5 | Medical_History_1 | Medical_History_2 | Medical_History_3 | ... | Medical_Keyword_10 | Medical_Keyword_11 | Medical_Keyword_12 | Medical_Keyword_13 | Medical_Keyword_14 | Medical_Keyword_15 | Medical_Keyword_16 | Medical_Keyword_17 | Medical_Keyword_18 | Medical_Keyword_19 | Medical_Keyword_20 | Medical_Keyword_21 | Medical_Keyword_22 | Medical_Keyword_23 | Medical_Keyword_24 | Medical_Keyword_25 | Medical_Keyword_26 | Medical_Keyword_27 | Medical_Keyword_28 | Medical_Keyword_29 | Medical_Keyword_30 | Medical_Keyword_31 | Medical_Keyword_32 | Medical_Keyword_33 | Medical_Keyword_34 | Medical_Keyword_35 | Medical_Keyword_36 | Medical_Keyword_37 | Medical_Keyword_38 | Medical_Keyword_39 | Medical_Keyword_40 | Medical_Keyword_41 | Medical_Keyword_42 | Medical_Keyword_43 | Medical_Keyword_44 | Medical_Keyword_45 | Medical_Keyword_46 | Medical_Keyword_47 | Medical_Keyword_48 | Response  
---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---  
count | 59381.000000 | 59381.000000 | 59381.000000 | 59381.000000 | 59381.000000 | 59381.000000 | 59381.000000 | 59381.000000 | 59381.000000 | 59381.000000 | 59381.000000 | 59362.000000 | 59381.000000 | 59381.000000 | 52602.000000 | 59381.000000 | 48527.000000 | 59381.000000 | 59381.000000 | 59381.000000 | 59381.000000 | 59381.000000 | 59381.000000 | 59381.000000 | 59381.000000 | 59381.000000 | 59381.000000 | 59381.000000 | 33985.000000 | 59381.000000 | 59381.000000 | 59381.000000 | 59381.000000 | 30725.000000 | 25140.000000 | 40197.000000 | 17570.000000 | 50492.000000 | 59381.000000 | 59381.000000 | ... | 59381.000000 | 59381.000000 | 59381.000000 | 59381.000000 | 59381.000000 | 59381.000000 | 59381.000000 | 59381.000000 | 59381.000000 | 59381.000000 | 59381.000000 | 59381.000000 | 59381.000000 | 59381.000000 | 59381.000000 | 59381.000000 | 59381.000000 | 59381.000000 | 59381.000000 | 59381.000000 | 59381.000000 | 59381.000000 | 59381.000000 | 59381.000000 | 59381.000000 | 59381.000000 | 59381.000000 | 59381.000000 | 59381.000000 | 59381.000000 | 59381.000000 | 59381.000000 | 59381.000000 | 59381.000000 | 59381.000000 | 59381.000000 | 59381.000000 | 59381.000000 | 59381.000000 | 59381.000000  
mean | 39507.211515 | 1.026355 | 24.415655 | 0.328952 | 2.006955 | 2.673599 | 1.043583 | 0.405567 | 0.707283 | 0.292587 | 0.469462 | 0.077582 | 8.641821 | 1.300904 | 0.006283 | 2.142958 | 0.361469 | 1.209326 | 2.007427 | 5.835840 | 2.883666 | 1.027180 | 1.409188 | 1.038531 | 1.727606 | 1.055792 | 2.146983 | 1.958707 | 0.001733 | 1.901989 | 2.048484 | 2.419360 | 2.686230 | 0.474550 | 0.497737 | 0.444890 | 0.484635 | 7.962172 | 253.987100 | 2.102171 | ... | 0.036459 | 0.058015 | 0.010003 | 0.005962 | 0.007848 | 0.190465 | 0.012715 | 0.009161 | 0.007494 | 0.009296 | 0.008134 | 0.014601 | 0.037167 | 0.097775 | 0.018895 | 0.089456 | 0.013439 | 0.011856 | 0.014937 | 0.011755 | 0.025042 | 0.010896 | 0.021168 | 0.022836 | 0.020646 | 0.006938 | 0.010407 | 0.066587 | 0.006837 | 0.013658 | 0.056954 | 0.010054 | 0.045536 | 0.010710 | 0.007528 | 0.013691 | 0.008488 | 0.019905 | 0.054496 | 5.636837  
std | 22815.883089 | 0.160191 | 5.072885 | 0.282562 | 0.083107 | 0.739103 | 0.291949 | 0.197190 | 0.074239 | 0.089037 | 0.122213 | 0.082347 | 4.227082 | 0.715034 | 0.032816 | 0.350033 | 0.349551 | 0.417939 | 0.085858 | 2.674536 | 0.320627 | 0.231566 | 0.491688 | 0.274915 | 0.445195 | 0.329328 | 0.989139 | 0.945739 | 0.007338 | 0.971223 | 0.755149 | 0.509577 | 0.483159 | 0.154959 | 0.140187 | 0.163012 | 0.129200 | 13.027697 | 178.621154 | 0.303098 | ... | 0.187432 | 0.233774 | 0.099515 | 0.076981 | 0.088239 | 0.392671 | 0.112040 | 0.095275 | 0.086244 | 0.095967 | 0.089821 | 0.119949 | 0.189172 | 0.297013 | 0.136155 | 0.285404 | 0.115145 | 0.108237 | 0.121304 | 0.107780 | 0.156253 | 0.103813 | 0.143947 | 0.149380 | 0.142198 | 0.083007 | 0.101485 | 0.249307 | 0.082405 | 0.116066 | 0.231757 | 0.099764 | 0.208479 | 0.102937 | 0.086436 | 0.116207 | 0.091737 | 0.139676 | 0.226995 | 2.456833  
min | 2.000000 | 1.000000 | 1.000000 | 0.000000 | 2.000000 | 1.000000 | 1.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 1.000000 | 1.000000 | 0.000000 | 2.000000 | 0.000000 | 1.000000 | 2.000000 | 1.000000 | 2.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 0.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 1.000000 | 1.000000 | ... | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 1.000000  
25% | 19780.000000 | 1.000000 | 26.000000 | 0.076923 | 2.000000 | 3.000000 | 1.000000 | 0.238806 | 0.654545 | 0.225941 | 0.385517 | 0.035000 | 9.000000 | 1.000000 | 0.000000 | 2.000000 | 0.060000 | 1.000000 | 2.000000 | 3.000000 | 3.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 0.000400 | 1.000000 | 1.000000 | 2.000000 | 2.000000 | 0.362319 | 0.401961 | 0.323944 | 0.401786 | 2.000000 | 112.000000 | 2.000000 | ... | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 4.000000  
50% | 39487.000000 | 1.000000 | 26.000000 | 0.230769 | 2.000000 | 3.000000 | 1.000000 | 0.402985 | 0.709091 | 0.288703 | 0.451349 | 0.060000 | 9.000000 | 1.000000 | 0.000000 | 2.000000 | 0.250000 | 1.000000 | 2.000000 | 6.000000 | 3.000000 | 1.000000 | 1.000000 | 1.000000 | 2.000000 | 1.000000 | 3.000000 | 2.000000 | 0.000973 | 1.000000 | 2.000000 | 2.000000 | 3.000000 | 0.463768 | 0.519608 | 0.422535 | 0.508929 | 4.000000 | 162.000000 | 2.000000 | ... | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 6.000000  
75% | 59211.000000 | 1.000000 | 26.000000 | 0.487179 | 2.000000 | 3.000000 | 1.000000 | 0.567164 | 0.763636 | 0.345188 | 0.532858 | 0.100000 | 9.000000 | 1.000000 | 0.000000 | 2.000000 | 0.550000 | 1.000000 | 2.000000 | 8.000000 | 3.000000 | 1.000000 | 2.000000 | 1.000000 | 2.000000 | 1.000000 | 3.000000 | 3.000000 | 0.002000 | 3.000000 | 3.000000 | 3.000000 | 3.000000 | 0.579710 | 0.598039 | 0.563380 | 0.580357 | 9.000000 | 418.000000 | 2.000000 | ... | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 8.000000  
max | 79146.000000 | 2.000000 | 38.000000 | 1.000000 | 3.000000 | 3.000000 | 3.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 38.000000 | 3.000000 | 1.000000 | 3.000000 | 1.000000 | 3.000000 | 3.000000 | 11.000000 | 3.000000 | 3.000000 | 2.000000 | 3.000000 | 2.000000 | 3.000000 | 3.000000 | 3.000000 | 1.000000 | 3.000000 | 3.000000 | 3.000000 | 3.000000 | 1.000000 | 1.000000 | 0.943662 | 1.000000 | 240.000000 | 648.000000 | 3.000000 | ... | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 8.000000  
  
In [9]:

    
    
    #Responce variable
    aixs1 = plt.subplots(1,1,figsize=(10,5))
    sns.countplot(x='Response',data=train)
    

Out[9]:

    
    
    <matplotlib.axes._subplots.AxesSubplot at 0x7f6c0ef59358>

![](__results___files/__results___9_1.png)

Data PreProcessing

In [10]:

    
    
    #Categorical codes
    train['Product_Info_2'] = train['Product_Info_2'].astype('category').cat.codes
    

Missing Value Treatment

In [11]:

    
    
    # missing values
    train_missing
    

Out[11]:

| features | missing_count | missing_count_percentage  
---|---|---|---  
0 | Employment_Info_1 | 19 | 0.031997  
1 | Employment_Info_4 | 6779 | 11.416110  
2 | Employment_Info_6 | 10854 | 18.278574  
3 | Insurance_History_5 | 25396 | 42.767889  
4 | Family_Hist_2 | 28656 | 48.257860  
5 | Family_Hist_3 | 34241 | 57.663226  
6 | Family_Hist_4 | 19184 | 32.306630  
7 | Family_Hist_5 | 41811 | 70.411411  
8 | Medical_History_1 | 8889 | 14.969435  
9 | Medical_History_10 | 58824 | 99.061990  
10 | Medical_History_15 | 44596 | 75.101463  
11 | Medical_History_24 | 55580 | 93.598963  
12 | Medical_History_32 | 58274 | 98.135767  
  
In [12]:

    
    
    #dropping columns containing missing values more than 80%
    train = train.drop(['Medical_History_10','Medical_History_24','Medical_History_32'], axis=1)
    

In [13]:

    
    
    #missing values AGAIN
    train_missing= train.isnull().sum()[train.isnull().sum() !=0]
    train_missing=pd.DataFrame(train_missing.reset_index())
    train_missing.rename(columns={'index':'features',0:'missing_count'},inplace=True)
    train_missing['missing_count_percentage']=((train_missing['missing_count'])/59381)*100
    train_missing
    

Out[13]:

| features | missing_count | missing_count_percentage  
---|---|---|---  
0 | Employment_Info_1 | 19 | 0.031997  
1 | Employment_Info_4 | 6779 | 11.416110  
2 | Employment_Info_6 | 10854 | 18.278574  
3 | Insurance_History_5 | 25396 | 42.767889  
4 | Family_Hist_2 | 28656 | 48.257860  
5 | Family_Hist_3 | 34241 | 57.663226  
6 | Family_Hist_4 | 19184 | 32.306630  
7 | Family_Hist_5 | 41811 | 70.411411  
8 | Medical_History_1 | 8889 | 14.969435  
9 | Medical_History_15 | 44596 | 75.101463  
  
In [14]:

    
    
    #Mean Imputation fro continous variables
    Continuos = ['Employment_Info_1','Employment_Info_4', 'Employment_Info_6', 'Insurance_History_5',
                        'Family_Hist_2', 'Family_Hist_3', 'Family_Hist_4', 'Family_Hist_5']
    train[Continuos] = train[Continuos].fillna(train[Continuos].mean())
    

In [15]:

    
    
    #Mode Imputation fro continous variables
    Categorical = ['Medical_History_1', 'Medical_History_15']
    train[Categorical] = train[Categorical].apply(lambda x:x.fillna(x.value_counts().index[0]))
    

In [16]:

    
    
    #Missing values again
    train_missing= train.isnull().sum()[train.isnull().sum() !=0]
    train_missing=pd.DataFrame(train_missing.reset_index())
    train_missing.rename(columns={'index':'features',0:'missing_count'},inplace=True)
    train_missing['missing_count_percentage']=((train_missing['missing_count'])/59381)*100
    train_missing
    

Out[16]:

| features | missing_count | missing_count_percentage  
---|---|---|---  
  
In [17]:

    
    
    #train data
    train.head()
    

Out[17]:

| Id | Product_Info_1 | Product_Info_2 | Product_Info_3 | Product_Info_4 | Product_Info_5 | Product_Info_6 | Product_Info_7 | Ins_Age | Ht | Wt | BMI | Employment_Info_1 | Employment_Info_2 | Employment_Info_3 | Employment_Info_4 | Employment_Info_5 | Employment_Info_6 | InsuredInfo_1 | InsuredInfo_2 | InsuredInfo_3 | InsuredInfo_4 | InsuredInfo_5 | InsuredInfo_6 | InsuredInfo_7 | Insurance_History_1 | Insurance_History_2 | Insurance_History_3 | Insurance_History_4 | Insurance_History_5 | Insurance_History_7 | Insurance_History_8 | Insurance_History_9 | Family_Hist_1 | Family_Hist_2 | Family_Hist_3 | Family_Hist_4 | Family_Hist_5 | Medical_History_1 | Medical_History_2 | ... | Medical_Keyword_10 | Medical_Keyword_11 | Medical_Keyword_12 | Medical_Keyword_13 | Medical_Keyword_14 | Medical_Keyword_15 | Medical_Keyword_16 | Medical_Keyword_17 | Medical_Keyword_18 | Medical_Keyword_19 | Medical_Keyword_20 | Medical_Keyword_21 | Medical_Keyword_22 | Medical_Keyword_23 | Medical_Keyword_24 | Medical_Keyword_25 | Medical_Keyword_26 | Medical_Keyword_27 | Medical_Keyword_28 | Medical_Keyword_29 | Medical_Keyword_30 | Medical_Keyword_31 | Medical_Keyword_32 | Medical_Keyword_33 | Medical_Keyword_34 | Medical_Keyword_35 | Medical_Keyword_36 | Medical_Keyword_37 | Medical_Keyword_38 | Medical_Keyword_39 | Medical_Keyword_40 | Medical_Keyword_41 | Medical_Keyword_42 | Medical_Keyword_43 | Medical_Keyword_44 | Medical_Keyword_45 | Medical_Keyword_46 | Medical_Keyword_47 | Medical_Keyword_48 | Response  
---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---  
0 | 2 | 1 | 16 | 10 | 0.076923 | 2 | 1 | 1 | 0.641791 | 0.581818 | 0.148536 | 0.323008 | 0.028 | 12 | 1 | 0.0 | 3 | 0.361469 | 1 | 2 | 6 | 3 | 1 | 2 | 1 | 1 | 1 | 3 | 1 | 0.000667 | 1 | 1 | 2 | 2 | 0.474550 | 0.598039 | 0.444890 | 0.526786 | 4.0 | 112 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 8  
1 | 5 | 1 | 0 | 26 | 0.076923 | 2 | 3 | 1 | 0.059701 | 0.600000 | 0.131799 | 0.272288 | 0.000 | 1 | 3 | 0.0 | 2 | 0.001800 | 1 | 2 | 6 | 3 | 1 | 2 | 1 | 2 | 1 | 3 | 1 | 0.000133 | 1 | 3 | 2 | 2 | 0.188406 | 0.497737 | 0.084507 | 0.484635 | 5.0 | 412 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 4  
2 | 6 | 1 | 18 | 26 | 0.076923 | 2 | 3 | 1 | 0.029851 | 0.745455 | 0.288703 | 0.428780 | 0.030 | 9 | 1 | 0.0 | 2 | 0.030000 | 1 | 2 | 8 | 3 | 1 | 1 | 1 | 2 | 1 | 1 | 3 | 0.001733 | 3 | 2 | 3 | 3 | 0.304348 | 0.497737 | 0.225352 | 0.484635 | 10.0 | 3 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 8  
3 | 7 | 1 | 17 | 10 | 0.487179 | 2 | 3 | 1 | 0.164179 | 0.672727 | 0.205021 | 0.352438 | 0.042 | 9 | 1 | 0.0 | 3 | 0.200000 | 2 | 2 | 8 | 3 | 1 | 2 | 1 | 2 | 1 | 1 | 3 | 0.001733 | 3 | 2 | 3 | 3 | 0.420290 | 0.497737 | 0.352113 | 0.484635 | 0.0 | 350 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 8  
4 | 8 | 1 | 15 | 26 | 0.230769 | 2 | 3 | 1 | 0.417910 | 0.654545 | 0.234310 | 0.424046 | 0.027 | 9 | 1 | 0.0 | 2 | 0.050000 | 1 | 2 | 6 | 3 | 1 | 2 | 1 | 2 | 1 | 1 | 3 | 0.001733 | 3 | 2 | 3 | 2 | 0.463768 | 0.497737 | 0.408451 | 0.484635 | 1.0 | 162 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 8  
  
Modelling

1)Dataset split

In [18]:

    
    
    #Dataset split
    train_data, test_data = train_test_split(train, test_size = 0.15)
    print(train_data.shape)
    print(test_data.shape)
    
    
    
    (50473, 125)
    (8908, 125)
    

In [19]:

    
    
    #traindata
    train_data.head()
    

Out[19]:

| Id | Product_Info_1 | Product_Info_2 | Product_Info_3 | Product_Info_4 | Product_Info_5 | Product_Info_6 | Product_Info_7 | Ins_Age | Ht | Wt | BMI | Employment_Info_1 | Employment_Info_2 | Employment_Info_3 | Employment_Info_4 | Employment_Info_5 | Employment_Info_6 | InsuredInfo_1 | InsuredInfo_2 | InsuredInfo_3 | InsuredInfo_4 | InsuredInfo_5 | InsuredInfo_6 | InsuredInfo_7 | Insurance_History_1 | Insurance_History_2 | Insurance_History_3 | Insurance_History_4 | Insurance_History_5 | Insurance_History_7 | Insurance_History_8 | Insurance_History_9 | Family_Hist_1 | Family_Hist_2 | Family_Hist_3 | Family_Hist_4 | Family_Hist_5 | Medical_History_1 | Medical_History_2 | ... | Medical_Keyword_10 | Medical_Keyword_11 | Medical_Keyword_12 | Medical_Keyword_13 | Medical_Keyword_14 | Medical_Keyword_15 | Medical_Keyword_16 | Medical_Keyword_17 | Medical_Keyword_18 | Medical_Keyword_19 | Medical_Keyword_20 | Medical_Keyword_21 | Medical_Keyword_22 | Medical_Keyword_23 | Medical_Keyword_24 | Medical_Keyword_25 | Medical_Keyword_26 | Medical_Keyword_27 | Medical_Keyword_28 | Medical_Keyword_29 | Medical_Keyword_30 | Medical_Keyword_31 | Medical_Keyword_32 | Medical_Keyword_33 | Medical_Keyword_34 | Medical_Keyword_35 | Medical_Keyword_36 | Medical_Keyword_37 | Medical_Keyword_38 | Medical_Keyword_39 | Medical_Keyword_40 | Medical_Keyword_41 | Medical_Keyword_42 | Medical_Keyword_43 | Medical_Keyword_44 | Medical_Keyword_45 | Medical_Keyword_46 | Medical_Keyword_47 | Medical_Keyword_48 | Response  
---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---  
37724 | 50086 | 1 | 15 | 26 | 0.179487 | 2 | 1 | 1 | 0.582090 | 0.672727 | 0.261506 | 0.455416 | 0.100 | 9 | 1 | 0.0 | 2 | 0.500000 | 1 | 2 | 2 | 3 | 1 | 2 | 1 | 1 | 1 | 3 | 1 | 0.001000 | 1 | 1 | 2 | 2 | 0.474550 | 0.401961 | 0.444890 | 0.339286 | 1.0 | 478 | ... | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 6  
33550 | 44541 | 1 | 14 | 26 | 0.487179 | 2 | 3 | 1 | 0.477612 | 0.745455 | 0.236402 | 0.344016 | 0.100 | 11 | 1 | 0.0 | 2 | 0.500000 | 1 | 2 | 6 | 3 | 1 | 1 | 1 | 2 | 3 | 1 | 3 | 0.001733 | 3 | 2 | 3 | 2 | 0.565217 | 0.497737 | 0.535211 | 0.484635 | 1.0 | 112 | ... | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 3  
31838 | 42260 | 1 | 16 | 26 | 0.076923 | 2 | 3 | 1 | 0.522388 | 0.563636 | 0.236402 | 0.533794 | 0.080 | 9 | 1 | 0.0 | 2 | 0.020000 | 2 | 2 | 8 | 3 | 1 | 1 | 1 | 2 | 1 | 3 | 1 | 0.000900 | 1 | 3 | 2 | 3 | 0.474550 | 0.539216 | 0.444890 | 0.580357 | 3.0 | 112 | ... | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 7  
8351 | 11130 | 1 | 16 | 26 | 0.230769 | 2 | 3 | 1 | 0.507463 | 0.781818 | 0.497908 | 0.715829 | 0.085 | 14 | 1 | 0.0 | 2 | 0.361469 | 1 | 2 | 11 | 3 | 1 | 1 | 1 | 1 | 1 | 3 | 1 | 0.003333 | 1 | 1 | 2 | 3 | 0.474550 | 0.382353 | 0.563380 | 0.484635 | 3.0 | 491 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 5  
36028 | 47841 | 1 | 15 | 26 | 0.076923 | 2 | 3 | 1 | 0.447761 | 0.800000 | 0.309623 | 0.410825 | 0.060 | 12 | 1 | 0.0 | 2 | 0.361469 | 1 | 2 | 8 | 3 | 1 | 1 | 1 | 2 | 1 | 1 | 3 | 0.001733 | 3 | 2 | 3 | 3 | 0.474550 | 0.480392 | 0.521127 | 0.484635 | 1.0 | 112 | ... | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 2  
  
In [20]:

    
    
    #traindata
    test_data.head()
    

Out[20]:

| Id | Product_Info_1 | Product_Info_2 | Product_Info_3 | Product_Info_4 | Product_Info_5 | Product_Info_6 | Product_Info_7 | Ins_Age | Ht | Wt | BMI | Employment_Info_1 | Employment_Info_2 | Employment_Info_3 | Employment_Info_4 | Employment_Info_5 | Employment_Info_6 | InsuredInfo_1 | InsuredInfo_2 | InsuredInfo_3 | InsuredInfo_4 | InsuredInfo_5 | InsuredInfo_6 | InsuredInfo_7 | Insurance_History_1 | Insurance_History_2 | Insurance_History_3 | Insurance_History_4 | Insurance_History_5 | Insurance_History_7 | Insurance_History_8 | Insurance_History_9 | Family_Hist_1 | Family_Hist_2 | Family_Hist_3 | Family_Hist_4 | Family_Hist_5 | Medical_History_1 | Medical_History_2 | ... | Medical_Keyword_10 | Medical_Keyword_11 | Medical_Keyword_12 | Medical_Keyword_13 | Medical_Keyword_14 | Medical_Keyword_15 | Medical_Keyword_16 | Medical_Keyword_17 | Medical_Keyword_18 | Medical_Keyword_19 | Medical_Keyword_20 | Medical_Keyword_21 | Medical_Keyword_22 | Medical_Keyword_23 | Medical_Keyword_24 | Medical_Keyword_25 | Medical_Keyword_26 | Medical_Keyword_27 | Medical_Keyword_28 | Medical_Keyword_29 | Medical_Keyword_30 | Medical_Keyword_31 | Medical_Keyword_32 | Medical_Keyword_33 | Medical_Keyword_34 | Medical_Keyword_35 | Medical_Keyword_36 | Medical_Keyword_37 | Medical_Keyword_38 | Medical_Keyword_39 | Medical_Keyword_40 | Medical_Keyword_41 | Medical_Keyword_42 | Medical_Keyword_43 | Medical_Keyword_44 | Medical_Keyword_45 | Medical_Keyword_46 | Medical_Keyword_47 | Medical_Keyword_48 | Response  
---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---  
42541 | 56547 | 1 | 5 | 10 | 0.487179 | 2 | 3 | 1 | 0.268657 | 0.563636 | 0.215481 | 0.487645 | 0.000 | 1 | 3 | 0.040000 | 3 | 0.600000 | 3 | 2 | 8 | 3 | 1 | 2 | 1 | 1 | 1 | 3 | 1 | 0.003333 | 1 | 1 | 2 | 3 | 0.376812 | 0.497737 | 0.338028 | 0.484635 | 13.0 | 112 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 8  
37399 | 49660 | 1 | 16 | 10 | 0.487179 | 2 | 3 | 1 | 0.358209 | 0.636364 | 0.205021 | 0.386332 | 0.030 | 12 | 1 | 0.000000 | 3 | 0.100000 | 1 | 2 | 3 | 3 | 1 | 2 | 1 | 1 | 1 | 3 | 1 | 0.003333 | 1 | 1 | 2 | 3 | 0.608696 | 0.497737 | 0.492958 | 0.484635 | 5.0 | 112 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 6  
37016 | 49150 | 1 | 17 | 26 | 0.384615 | 2 | 3 | 1 | 0.164179 | 0.727273 | 0.194561 | 0.289568 | 0.100 | 14 | 1 | 0.006283 | 2 | 0.350000 | 1 | 2 | 3 | 3 | 1 | 2 | 1 | 1 | 1 | 3 | 2 | 0.000667 | 1 | 3 | 2 | 3 | 0.333333 | 0.497737 | 0.295775 | 0.484635 | 10.0 | 161 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 8  
58288 | 77693 | 1 | 16 | 10 | 0.487179 | 2 | 3 | 1 | 0.462687 | 0.654545 | 0.215481 | 0.388655 | 0.700 | 1 | 3 | 0.000000 | 3 | 0.361469 | 1 | 2 | 8 | 3 | 1 | 2 | 1 | 2 | 1 | 1 | 3 | 0.001733 | 3 | 2 | 3 | 3 | 0.536232 | 0.497737 | 0.492958 | 0.484635 | 30.0 | 491 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 8  
54857 | 73049 | 1 | 15 | 26 | 0.230769 | 2 | 3 | 1 | 0.388060 | 0.709091 | 0.516736 | 0.856842 | 0.025 | 1 | 3 | 0.000000 | 2 | 0.361469 | 2 | 2 | 8 | 3 | 1 | 2 | 1 | 2 | 1 | 1 | 3 | 0.001733 | 3 | 2 | 3 | 3 | 0.681159 | 0.497737 | 0.507042 | 0.484635 | 1.0 | 112 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2  
  
In [21]:

    
    
    #Predictor and responce variables
    train_x = train_data.drop(['Id', 'Response'], axis=1)
    train_y = train_data['Response']
    test_x = test_data.drop(['Id', 'Response'], axis=1)
    test_y = test_data['Response']
    print(train_x.shape)
    print(train_y.shape)
    print(test_x.shape)
    print(test_y.shape)
    
    
    
    (50473, 123)
    (50473,)
    (8908, 123)
    (8908,)
    

In [22]:

    
    
    #train responce
    train_y.head()
    

Out[22]:

    
    
    37724    6
    33550    3
    31838    7
    8351     5
    36028    2
    Name: Response, dtype: int64

In [23]:

    
    
    #test responce
    test_y.head()
    

Out[23]:

    
    
    42541    8
    37399    6
    37016    8
    58288    8
    54857    2
    Name: Response, dtype: int64

In [24]:

    
    
    #converting to responce categorical class labels(0-7)
    train_y = train_y-1
    train_y = to_categorical(train_y, num_classes= 8)
    
    print(train_x.shape)
    print(train_y.shape)
    print(test_x.shape)
    print(test_y.shape)
    
    
    
    (50473, 123)
    (50473, 8)
    (8908, 123)
    (8908,)
    

2)Normalization

In [25]:

    
    
    #Function for normalization
    def normalization(data):
        return (data - data.min())/(data.max() - data.min())
    

In [26]:

    
    
    #normalizing data
    train_x = normalization(train_x)
    test_x = normalization(test_x)
    

In [27]:

    
    
    #traindata
    train_x.head()
    

Out[27]:

| Product_Info_1 | Product_Info_2 | Product_Info_3 | Product_Info_4 | Product_Info_5 | Product_Info_6 | Product_Info_7 | Ins_Age | Ht | Wt | BMI | Employment_Info_1 | Employment_Info_2 | Employment_Info_3 | Employment_Info_4 | Employment_Info_5 | Employment_Info_6 | InsuredInfo_1 | InsuredInfo_2 | InsuredInfo_3 | InsuredInfo_4 | InsuredInfo_5 | InsuredInfo_6 | InsuredInfo_7 | Insurance_History_1 | Insurance_History_2 | Insurance_History_3 | Insurance_History_4 | Insurance_History_5 | Insurance_History_7 | Insurance_History_8 | Insurance_History_9 | Family_Hist_1 | Family_Hist_2 | Family_Hist_3 | Family_Hist_4 | Family_Hist_5 | Medical_History_1 | Medical_History_2 | Medical_History_3 | ... | Medical_Keyword_9 | Medical_Keyword_10 | Medical_Keyword_11 | Medical_Keyword_12 | Medical_Keyword_13 | Medical_Keyword_14 | Medical_Keyword_15 | Medical_Keyword_16 | Medical_Keyword_17 | Medical_Keyword_18 | Medical_Keyword_19 | Medical_Keyword_20 | Medical_Keyword_21 | Medical_Keyword_22 | Medical_Keyword_23 | Medical_Keyword_24 | Medical_Keyword_25 | Medical_Keyword_26 | Medical_Keyword_27 | Medical_Keyword_28 | Medical_Keyword_29 | Medical_Keyword_30 | Medical_Keyword_31 | Medical_Keyword_32 | Medical_Keyword_33 | Medical_Keyword_34 | Medical_Keyword_35 | Medical_Keyword_36 | Medical_Keyword_37 | Medical_Keyword_38 | Medical_Keyword_39 | Medical_Keyword_40 | Medical_Keyword_41 | Medical_Keyword_42 | Medical_Keyword_43 | Medical_Keyword_44 | Medical_Keyword_45 | Medical_Keyword_46 | Medical_Keyword_47 | Medical_Keyword_48  
---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---  
37724 | 0.0 | 0.833333 | 0.675676 | 0.179487 | 0.0 | 0.0 | 0.0 | 0.582090 | 0.672727 | 0.261506 | 0.455416 | 0.100 | 0.216216 | 0.0 | 0.0 | 0.0 | 0.500000 | 0.0 | 0.0 | 0.1 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.001000 | 0.0 | 0.0 | 0.5 | 0.5 | 0.474550 | 0.401961 | 0.471451 | 0.339286 | 0.004167 | 0.737249 | 0.5 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0  
33550 | 0.0 | 0.777778 | 0.675676 | 0.487179 | 0.0 | 1.0 | 0.0 | 0.477612 | 0.745455 | 0.236402 | 0.344016 | 0.100 | 0.270270 | 0.0 | 0.0 | 0.0 | 0.500000 | 0.0 | 0.0 | 0.5 | 1.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | 1.0 | 0.001733 | 1.0 | 0.5 | 1.0 | 0.5 | 0.565217 | 0.497737 | 0.567164 | 0.484635 | 0.004167 | 0.171561 | 0.5 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0  
31838 | 0.0 | 0.888889 | 0.675676 | 0.076923 | 0.0 | 1.0 | 0.0 | 0.522388 | 0.563636 | 0.236402 | 0.533794 | 0.080 | 0.216216 | 0.0 | 0.0 | 0.0 | 0.020000 | 0.5 | 0.0 | 0.7 | 1.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.000900 | 0.0 | 1.0 | 0.5 | 1.0 | 0.474550 | 0.539216 | 0.471451 | 0.580357 | 0.012500 | 0.171561 | 0.5 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0  
8351 | 0.0 | 0.888889 | 0.675676 | 0.230769 | 0.0 | 1.0 | 0.0 | 0.507463 | 0.781818 | 0.497908 | 0.715829 | 0.085 | 0.351351 | 0.0 | 0.0 | 0.0 | 0.361469 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.003333 | 0.0 | 0.0 | 0.5 | 1.0 | 0.474550 | 0.382353 | 0.597015 | 0.484635 | 0.012500 | 0.757342 | 0.5 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0  
36028 | 0.0 | 0.833333 | 0.675676 | 0.076923 | 0.0 | 1.0 | 0.0 | 0.447761 | 0.800000 | 0.309623 | 0.410825 | 0.060 | 0.297297 | 0.0 | 0.0 | 0.0 | 0.361469 | 0.0 | 0.0 | 0.7 | 1.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 1.0 | 0.001733 | 1.0 | 0.5 | 1.0 | 1.0 | 0.474550 | 0.480392 | 0.552239 | 0.484635 | 0.004167 | 0.171561 | 0.5 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0  
  
In [28]:

    
    
    #testdata
    test_x.head()
    

Out[28]:

| Product_Info_1 | Product_Info_2 | Product_Info_3 | Product_Info_4 | Product_Info_5 | Product_Info_6 | Product_Info_7 | Ins_Age | Ht | Wt | BMI | Employment_Info_1 | Employment_Info_2 | Employment_Info_3 | Employment_Info_4 | Employment_Info_5 | Employment_Info_6 | InsuredInfo_1 | InsuredInfo_2 | InsuredInfo_3 | InsuredInfo_4 | InsuredInfo_5 | InsuredInfo_6 | InsuredInfo_7 | Insurance_History_1 | Insurance_History_2 | Insurance_History_3 | Insurance_History_4 | Insurance_History_5 | Insurance_History_7 | Insurance_History_8 | Insurance_History_9 | Family_Hist_1 | Family_Hist_2 | Family_Hist_3 | Family_Hist_4 | Family_Hist_5 | Medical_History_1 | Medical_History_2 | Medical_History_3 | ... | Medical_Keyword_9 | Medical_Keyword_10 | Medical_Keyword_11 | Medical_Keyword_12 | Medical_Keyword_13 | Medical_Keyword_14 | Medical_Keyword_15 | Medical_Keyword_16 | Medical_Keyword_17 | Medical_Keyword_18 | Medical_Keyword_19 | Medical_Keyword_20 | Medical_Keyword_21 | Medical_Keyword_22 | Medical_Keyword_23 | Medical_Keyword_24 | Medical_Keyword_25 | Medical_Keyword_26 | Medical_Keyword_27 | Medical_Keyword_28 | Medical_Keyword_29 | Medical_Keyword_30 | Medical_Keyword_31 | Medical_Keyword_32 | Medical_Keyword_33 | Medical_Keyword_34 | Medical_Keyword_35 | Medical_Keyword_36 | Medical_Keyword_37 | Medical_Keyword_38 | Medical_Keyword_39 | Medical_Keyword_40 | Medical_Keyword_41 | Medical_Keyword_42 | Medical_Keyword_43 | Medical_Keyword_44 | Medical_Keyword_45 | Medical_Keyword_46 | Medical_Keyword_47 | Medical_Keyword_48  
---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---  
42541 | 0.0 | 0.277778 | 0.228571 | 0.487179 | 0.0 | 1.0 | 0.0 | 0.295082 | 0.314286 | 0.204545 | 0.391839 | 0.000 | 0.000000 | 1.0 | 0.040000 | 1.0 | 0.600000 | 1.0 | 0.0 | 0.7 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.100000 | 0.0 | 0.0 | 0.5 | 1.0 | 0.393443 | 0.547969 | 0.363636 | 0.653965 | 0.054393 | 0.172360 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0  
37399 | 0.0 | 0.888889 | 0.228571 | 0.487179 | 0.0 | 1.0 | 0.0 | 0.393443 | 0.428571 | 0.190341 | 0.271582 | 0.030 | 0.305556 | 0.0 | 0.000000 | 1.0 | 0.100000 | 0.0 | 0.0 | 0.2 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.100000 | 0.0 | 0.0 | 0.5 | 1.0 | 0.655738 | 0.547969 | 0.530303 | 0.653965 | 0.020921 | 0.172360 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0  
37016 | 0.0 | 0.944444 | 0.685714 | 0.384615 | 0.0 | 1.0 | 0.0 | 0.180328 | 0.571429 | 0.176136 | 0.156723 | 0.100 | 0.361111 | 0.0 | 0.006283 | 0.0 | 0.350000 | 0.0 | 0.0 | 0.2 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.5 | 0.020000 | 0.0 | 1.0 | 0.5 | 1.0 | 0.344262 | 0.547969 | 0.318182 | 0.653965 | 0.041841 | 0.248447 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0  
58288 | 0.0 | 0.888889 | 0.228571 | 0.487179 | 0.0 | 1.0 | 0.0 | 0.508197 | 0.457143 | 0.204545 | 0.274339 | 0.700 | 0.000000 | 1.0 | 0.000000 | 1.0 | 0.361469 | 0.0 | 0.0 | 0.7 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 1.0 | 0.051992 | 1.0 | 0.5 | 1.0 | 1.0 | 0.573770 | 0.547969 | 0.530303 | 0.653965 | 0.125523 | 0.760870 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0  
54857 | 0.0 | 0.833333 | 0.685714 | 0.230769 | 0.0 | 1.0 | 0.0 | 0.426230 | 0.542857 | 0.613636 | 0.830073 | 0.025 | 0.000000 | 1.0 | 0.000000 | 0.0 | 0.361469 | 0.5 | 0.0 | 0.7 | 1.0 | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 1.0 | 0.051992 | 1.0 | 0.5 | 1.0 | 1.0 | 0.737705 | 0.547969 | 0.545455 | 0.653965 | 0.004184 | 0.172360 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0  
  
3)Models and evaluation

In [29]:

    
    
    #Train and test data shapes
    print(train_x.shape)
    print(test_x.shape)
    
    
    
    (50473, 123)
    (8908, 123)
    

In [30]:

    
    
    #assigning static parameter
    nb_epoch = 20
    batch_size = 512
    input_dim = train_x.shape[1]
    hidden_dim1 = 64 
    hidden_dim2 = 32
    hidden_dim3 = 16
    learning_rate = 1e-7
    

In [31]:

    
    
    #Function for auto encoder to get and fit model
    def get_fit_encoder(xs_train,xs_cv,test_x):
        input_layer = Input(shape=(input_dim, ))
        encoder = Dense(input_dim, activation="relu",activity_regularizer=regularizers.l1(learning_rate))(input_layer)
        
        encoder = Dense(hidden_dim1, activation="relu")(encoder)
        encoder = Dense(hidden_dim2, activation="relu")(encoder)
        encoder = Dense(hidden_dim3, activation="relu", name="encoder")(encoder)
        
        decoder = Dense(hidden_dim3, activation="relu")(encoder)
        decoder = Dense(hidden_dim2, activation='relu')(decoder)
        decoder = Dense(hidden_dim1, activation='relu')(decoder)
        
        decoder = Dense(input_dim, activation='relu')(decoder)
        decoder = Dense(input_dim, activation='sigmoid')(decoder)
        autoencoder = Model(inputs=input_layer, outputs=decoder)
        #autoencoder.summary()
        autoencoder.compile(optimizer='adam',
                            loss='binary_crossentropy')
        
        history = autoencoder.fit(x=xs_train, y=xs_train,
                              epochs=nb_epoch,
                              batch_size=batch_size,
                              shuffle=True,
                              validation_data=(xs_cv, xs_cv),
                              verbose=1)
        encoder = Model(autoencoder.input, autoencoder.get_layer('encoder').output)
        x_auto_train= encoder.predict(xs_train)
        x_auto_cv= encoder.predict(xs_cv)
        x_auto_test= encoder.predict(test_x)
        return x_auto_train,x_auto_cv,x_auto_test
        
    

In [32]:

    
    
    #Function for Neural network to get and fit model
    def get_fit_neuralnetwork(xs_encoder_train,xs_encoder_cv,xs_encoder_test,ys_train,ys_cv):
        classifier = Sequential()
        classifier.add(Dense(output_dim = input_dim , init = 'uniform', activation = 'relu', input_dim = 16))
        classifier.add(Dense(output_dim = 16 , init = 'uniform', activation = 'relu'))
        classifier.add(Dense(output_dim = 8 , init = 'uniform', activation = 'relu'))
        classifier.add(Dense(output_dim = 8, init = 'uniform', activation = 'softmax'))
        classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])
        cp = ModelCheckpoint(filepath="autoencoder_data.h5",
                             save_best_only=True,
                             verbose=0)
        tb = TensorBoard(log_dir='./logs',
                         histogram_freq=0,
                         write_graph=True,
                         write_images=True)
        history = classifier.fit(xs_encoder_train, ys_train,
                                 batch_size=batch_size ,
                                 epochs=nb_epoch ,
                                 shuffle=True,
                                 validation_data=(xs_encoder_cv,ys_cv),
                                 verbose=1,
                                callbacks=[cp, tb]).history
        y_pred_NN = classifier.predict(xs_encoder_test, batch_size=batch_size, verbose=1)
        y_pred_NN = np.argmax(y_pred_NN,axis = 1) + 1
        return y_pred_NN
    

In [33]:

    
    
    #Function for State of art model to get and fit model
    def get_fit_SOA_Models(x_sampletrain,y_sampletrain,test_x):
        model1 = RandomForestClassifier()
        
        inside_train_y = np.argmax(y_sampletrain, axis = 1) + 1   
        
        model1.fit(x_sampletrain, inside_train_y)
        
        y_pred1 = model1.predict(test_x) 
        return y_pred1
    

In [34]:

    
    
    #function for model evaluation
    def model_evaluation (test_y,y_pred_NN,y_pred1):
       
        accuracy_NN = accuracy_score(test_y,y_pred_NN)
        F1_score_NN=f1_score(test_y, y_pred_NN,average='weighted')
        Precision_NN=precision_score(test_y, y_pred_NN,average='weighted')
        Recall_score_NN=recall_score(test_y, y_pred_NN,average='weighted')
        
        accuracy_SOAM1 = accuracy_score(test_y, y_pred1)
        F1_score_SOAM1=f1_score(test_y, y_pred1,average='weighted')
        Precision_SOAM1=precision_score(test_y, y_pred1,average='weighted')
        Recall_score_SOAM1=recall_score(test_y, y_pred1,average='weighted')
        
        
        print("Classification score for NN:", classification_report(test_y,y_pred_NN))
        print("Classification score for SOAM1:", classification_report(test_y, y_pred1))
           
        return accuracy_NN,F1_score_NN,Precision_NN,Recall_score_NN,accuracy_SOAM1,F1_score_SOAM1,Precision_SOAM1,Recall_score_SOAM1
        
    

In [35]:

    
    
    #Function to pass sample data to autoencoder and neural network functions
    def data_sampling(train_x, train_y, test_x, test_y):
        accuracy_list_NN= []
        F1_score_list_NN=[]
        Precision_list_NN=[]
        Recall_list_NN=[]
        
        accuracy_list_SOAM1= []
        F1_score_list_SOAM1=[]
        Precision_list_SOAM1=[]
        Recall_list_SOAM1=[]
        
        
        for i in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6,0.7, 0.8, 0.9, 0.99]:
            print("data sample {}".format(i*100))
            x_sampletrain, _, y_sampletrain, _ = train_test_split(train_x, train_y, stratify= train_y, train_size=i)
            xs_train, xs_cv, ys_train, ys_cv = train_test_split(x_sampletrain, y_sampletrain, stratify=y_sampletrain, train_size=0.9)
            xs_train.shape, xs_cv.shape, ys_train.shape, ys_cv.shape
            xs_encoder_train,xs_encoder_cv,xs_encoder_test=get_fit_encoder(xs_train,xs_cv,test_x)
            y_pred_NN=get_fit_neuralnetwork(xs_encoder_train,xs_encoder_cv,xs_encoder_test,ys_train,ys_cv)
            
            y_pred1=get_fit_SOA_Models(x_sampletrain,y_sampletrain,test_x)
            
            accuracy_NN,F1_score_NN,Precision_NN,Recall_NN,accuracy_SOAM1,F1_score_SOAM1,Precision_SOAM1,Recall_SOAM1=model_evaluation(test_y,y_pred_NN,y_pred1)
            
            
            accuracy_list_NN.append(accuracy_NN)
            F1_score_list_NN.append(F1_score_NN)
            Precision_list_NN.append(Precision_NN)
            Recall_list_NN.append(Recall_NN)
            
            accuracy_list_SOAM1.append(accuracy_SOAM1)
            F1_score_list_SOAM1.append(F1_score_SOAM1)
            Precision_list_SOAM1.append(Precision_SOAM1)
            Recall_list_SOAM1.append(Recall_SOAM1)
            
            
        return accuracy_list_NN,F1_score_list_NN,Precision_list_NN,Recall_list_NN,accuracy_list_SOAM1,F1_score_list_SOAM1,Precision_list_SOAM1,Recall_list_SOAM1
    

In [36]:

    
    
    #main code to run all functions to reach objective
    accuracy_list_NN,F1_score_list_NN,Precision_list_NN,Recall_list_NN,accuracy_list_SOAM1,F1_score_list_SOAM1,Precision_list_SOAM1,Recall_list_SOAM1=data_sampling(train_x, train_y, test_x, test_y)
    
    
    
    data sample 10.0
    WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
    Instructions for updating:
    Colocations handled automatically by placer.
    WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
    Instructions for updating:
    Use tf.cast instead.
    Train on 4542 samples, validate on 505 samples
    Epoch 1/20
    4542/4542 [==============================] - 2s 395us/step - loss: 0.6817 - val_loss: 0.6498
    Epoch 2/20
    4542/4542 [==============================] - 0s 12us/step - loss: 0.5763 - val_loss: 0.4489
    Epoch 3/20
    4542/4542 [==============================] - 0s 12us/step - loss: 0.3834 - val_loss: 0.3497
    Epoch 4/20
    4542/4542 [==============================] - 0s 12us/step - loss: 0.3169 - val_loss: 0.3068
    Epoch 5/20
    4542/4542 [==============================] - 0s 12us/step - loss: 0.2999 - val_loss: 0.2999
    Epoch 6/20
    4542/4542 [==============================] - 0s 12us/step - loss: 0.2935 - val_loss: 0.2970
    Epoch 7/20
    4542/4542 [==============================] - 0s 12us/step - loss: 0.2905 - val_loss: 0.2947
    Epoch 8/20
    4542/4542 [==============================] - 0s 12us/step - loss: 0.2887 - val_loss: 0.2932
    Epoch 9/20
    4542/4542 [==============================] - 0s 12us/step - loss: 0.2872 - val_loss: 0.2920
    Epoch 10/20
    4542/4542 [==============================] - 0s 12us/step - loss: 0.2860 - val_loss: 0.2910
    Epoch 11/20
    4542/4542 [==============================] - 0s 12us/step - loss: 0.2853 - val_loss: 0.2904
    Epoch 12/20
    4542/4542 [==============================] - 0s 12us/step - loss: 0.2848 - val_loss: 0.2900
    Epoch 13/20
    4542/4542 [==============================] - 0s 12us/step - loss: 0.2844 - val_loss: 0.2896
    Epoch 14/20
    4542/4542 [==============================] - 0s 13us/step - loss: 0.2841 - val_loss: 0.2894
    Epoch 15/20
    4542/4542 [==============================] - 0s 12us/step - loss: 0.2837 - val_loss: 0.2889
    Epoch 16/20
    4542/4542 [==============================] - 0s 12us/step - loss: 0.2833 - val_loss: 0.2885
    Epoch 17/20
    4542/4542 [==============================] - 0s 12us/step - loss: 0.2827 - val_loss: 0.2876
    Epoch 18/20
    4542/4542 [==============================] - 0s 12us/step - loss: 0.2814 - val_loss: 0.2857
    Epoch 19/20
    4542/4542 [==============================] - 0s 12us/step - loss: 0.2795 - val_loss: 0.2837
    Epoch 20/20
    4542/4542 [==============================] - 0s 12us/step - loss: 0.2770 - val_loss: 0.2808
    Train on 4542 samples, validate on 505 samples
    Epoch 1/20
    4542/4542 [==============================] - 0s 54us/step - loss: 2.0755 - acc: 0.3283 - val_loss: 2.0693 - val_acc: 0.3267
    Epoch 2/20
    4542/4542 [==============================] - 0s 9us/step - loss: 2.0601 - acc: 0.3276 - val_loss: 2.0434 - val_acc: 0.3267
    Epoch 3/20
    4542/4542 [==============================] - 0s 8us/step - loss: 2.0180 - acc: 0.3276 - val_loss: 1.9763 - val_acc: 0.3267
    Epoch 4/20
    4542/4542 [==============================] - 0s 8us/step - loss: 1.9281 - acc: 0.3276 - val_loss: 1.8653 - val_acc: 0.3267
    Epoch 5/20
    4542/4542 [==============================] - 0s 8us/step - loss: 1.8433 - acc: 0.3276 - val_loss: 1.8217 - val_acc: 0.3267
    Epoch 6/20
    4542/4542 [==============================] - 0s 8us/step - loss: 1.8155 - acc: 0.3276 - val_loss: 1.7945 - val_acc: 0.3267
    Epoch 7/20
    4542/4542 [==============================] - 0s 8us/step - loss: 1.7943 - acc: 0.3276 - val_loss: 1.7874 - val_acc: 0.3267
    Epoch 8/20
    4542/4542 [==============================] - 0s 8us/step - loss: 1.7864 - acc: 0.3276 - val_loss: 1.7793 - val_acc: 0.3267
    Epoch 9/20
    4542/4542 [==============================] - 0s 8us/step - loss: 1.7818 - acc: 0.3276 - val_loss: 1.7761 - val_acc: 0.3267
    Epoch 10/20
    4542/4542 [==============================] - 0s 8us/step - loss: 1.7802 - acc: 0.3276 - val_loss: 1.7747 - val_acc: 0.3267
    Epoch 11/20
    4542/4542 [==============================] - 0s 8us/step - loss: 1.7790 - acc: 0.3276 - val_loss: 1.7741 - val_acc: 0.3267
    Epoch 12/20
    4542/4542 [==============================] - 0s 7us/step - loss: 1.7783 - acc: 0.3276 - val_loss: 1.7733 - val_acc: 0.3267
    Epoch 13/20
    4542/4542 [==============================] - 0s 7us/step - loss: 1.7776 - acc: 0.3276 - val_loss: 1.7727 - val_acc: 0.3267
    Epoch 14/20
    4542/4542 [==============================] - 0s 8us/step - loss: 1.7772 - acc: 0.3276 - val_loss: 1.7725 - val_acc: 0.3267
    Epoch 15/20
    4542/4542 [==============================] - 0s 7us/step - loss: 1.7765 - acc: 0.3276 - val_loss: 1.7718 - val_acc: 0.3267
    Epoch 16/20
    4542/4542 [==============================] - 0s 7us/step - loss: 1.7761 - acc: 0.3276 - val_loss: 1.7712 - val_acc: 0.3267
    Epoch 17/20
    4542/4542 [==============================] - 0s 7us/step - loss: 1.7758 - acc: 0.3276 - val_loss: 1.7711 - val_acc: 0.3267
    Epoch 18/20
    4542/4542 [==============================] - 0s 7us/step - loss: 1.7759 - acc: 0.3276 - val_loss: 1.7712 - val_acc: 0.3267
    Epoch 19/20
    4542/4542 [==============================] - 0s 7us/step - loss: 1.7754 - acc: 0.3276 - val_loss: 1.7702 - val_acc: 0.3267
    Epoch 20/20
    4542/4542 [==============================] - 0s 7us/step - loss: 1.7745 - acc: 0.3276 - val_loss: 1.7706 - val_acc: 0.3267
    8908/8908 [==============================] - 0s 9us/step
    Classification score for NN:               precision    recall  f1-score   support
    
               1       0.00      0.00      0.00       925
               2       0.00      0.00      0.00       975
               3       0.00      0.00      0.00       150
               4       0.00      0.00      0.00       199
               5       0.00      0.00      0.00       828
               6       0.00      0.00      0.00      1625
               7       0.00      0.00      0.00      1247
               8       0.33      1.00      0.50      2959
    
       micro avg       0.33      0.33      0.33      8908
       macro avg       0.04      0.12      0.06      8908
    weighted avg       0.11      0.33      0.17      8908
    
    Classification score for SOAM1:               precision    recall  f1-score   support
    
               1       0.21      0.22      0.21       925
               2       0.20      0.20      0.20       975
               3       0.33      0.29      0.31       150
               4       0.45      0.37      0.41       199
               5       0.46      0.22      0.30       828
               6       0.29      0.27      0.28      1625
               7       0.30      0.24      0.26      1247
               8       0.60      0.77      0.68      2959
    
       micro avg       0.42      0.42      0.42      8908
       macro avg       0.35      0.32      0.33      8908
    weighted avg       0.40      0.42      0.40      8908
    
    data sample 20.0
    Train on 9084 samples, validate on 1010 samples
    Epoch 1/20
    9084/9084 [==============================] - 1s 86us/step - loss: 0.5991 - val_loss: 0.4027
    Epoch 2/20
    9084/9084 [==============================] - 0s 12us/step - loss: 0.3400 - val_loss: 0.3040
    Epoch 3/20
    9084/9084 [==============================] - 0s 11us/step - loss: 0.2951 - val_loss: 0.2937
    Epoch 4/20
    9084/9084 [==============================] - 0s 11us/step - loss: 0.2893 - val_loss: 0.2903
    Epoch 5/20
    9084/9084 [==============================] - 0s 11us/step - loss: 0.2862 - val_loss: 0.2879
    Epoch 6/20
    9084/9084 [==============================] - 0s 11us/step - loss: 0.2838 - val_loss: 0.2852
    Epoch 7/20
    9084/9084 [==============================] - 0s 12us/step - loss: 0.2788 - val_loss: 0.2775
    Epoch 8/20
    9084/9084 [==============================] - 0s 12us/step - loss: 0.2706 - val_loss: 0.2704
    Epoch 9/20
    9084/9084 [==============================] - 0s 12us/step - loss: 0.2646 - val_loss: 0.2658
    Epoch 10/20
    9084/9084 [==============================] - 0s 11us/step - loss: 0.2606 - val_loss: 0.2617
    Epoch 11/20
    9084/9084 [==============================] - 0s 11us/step - loss: 0.2567 - val_loss: 0.2578
    Epoch 12/20
    9084/9084 [==============================] - 0s 12us/step - loss: 0.2533 - val_loss: 0.2545
    Epoch 13/20
    9084/9084 [==============================] - 0s 11us/step - loss: 0.2505 - val_loss: 0.2519
    Epoch 14/20
    9084/9084 [==============================] - 0s 11us/step - loss: 0.2481 - val_loss: 0.2496
    Epoch 15/20
    9084/9084 [==============================] - 0s 11us/step - loss: 0.2460 - val_loss: 0.2478
    Epoch 16/20
    9084/9084 [==============================] - 0s 12us/step - loss: 0.2443 - val_loss: 0.2460
    Epoch 17/20
    9084/9084 [==============================] - 0s 12us/step - loss: 0.2428 - val_loss: 0.2447
    Epoch 18/20
    9084/9084 [==============================] - 0s 11us/step - loss: 0.2411 - val_loss: 0.2428
    Epoch 19/20
    9084/9084 [==============================] - 0s 12us/step - loss: 0.2391 - val_loss: 0.2404
    Epoch 20/20
    9084/9084 [==============================] - 0s 11us/step - loss: 0.2363 - val_loss: 0.2376
    Train on 9084 samples, validate on 1010 samples
    Epoch 1/20
    9084/9084 [==============================] - 0s 31us/step - loss: 2.0717 - acc: 0.3108 - val_loss: 2.0567 - val_acc: 0.3277
    Epoch 2/20
    9084/9084 [==============================] - 0s 7us/step - loss: 2.0087 - acc: 0.3275 - val_loss: 1.9221 - val_acc: 0.3277
    Epoch 3/20
    9084/9084 [==============================] - 0s 7us/step - loss: 1.8525 - acc: 0.3275 - val_loss: 1.8207 - val_acc: 0.3277
    Epoch 4/20
    9084/9084 [==============================] - 0s 7us/step - loss: 1.8004 - acc: 0.3275 - val_loss: 1.7926 - val_acc: 0.3277
    Epoch 5/20
    9084/9084 [==============================] - 0s 7us/step - loss: 1.7862 - acc: 0.3275 - val_loss: 1.7829 - val_acc: 0.3277
    Epoch 6/20
    9084/9084 [==============================] - 0s 7us/step - loss: 1.7788 - acc: 0.3275 - val_loss: 1.7758 - val_acc: 0.3277
    Epoch 7/20
    9084/9084 [==============================] - 0s 7us/step - loss: 1.7714 - acc: 0.3275 - val_loss: 1.7683 - val_acc: 0.3277
    Epoch 8/20
    9084/9084 [==============================] - 0s 7us/step - loss: 1.7625 - acc: 0.3275 - val_loss: 1.7600 - val_acc: 0.3277
    Epoch 9/20
    9084/9084 [==============================] - 0s 7us/step - loss: 1.7530 - acc: 0.3275 - val_loss: 1.7508 - val_acc: 0.3277
    Epoch 10/20
    9084/9084 [==============================] - 0s 7us/step - loss: 1.7431 - acc: 0.3290 - val_loss: 1.7428 - val_acc: 0.3307
    Epoch 11/20
    9084/9084 [==============================] - 0s 7us/step - loss: 1.7337 - acc: 0.3326 - val_loss: 1.7354 - val_acc: 0.3416
    Epoch 12/20
    9084/9084 [==============================] - 0s 7us/step - loss: 1.7251 - acc: 0.3421 - val_loss: 1.7265 - val_acc: 0.3485
    Epoch 13/20
    9084/9084 [==============================] - 0s 7us/step - loss: 1.7142 - acc: 0.3544 - val_loss: 1.7179 - val_acc: 0.3634
    Epoch 14/20
    9084/9084 [==============================] - 0s 8us/step - loss: 1.6997 - acc: 0.3702 - val_loss: 1.7025 - val_acc: 0.3832
    Epoch 15/20
    9084/9084 [==============================] - 0s 7us/step - loss: 1.6831 - acc: 0.3866 - val_loss: 1.6903 - val_acc: 0.3881
    Epoch 16/20
    9084/9084 [==============================] - 0s 7us/step - loss: 1.6710 - acc: 0.3939 - val_loss: 1.6815 - val_acc: 0.3901
    Epoch 17/20
    9084/9084 [==============================] - 0s 8us/step - loss: 1.6642 - acc: 0.3949 - val_loss: 1.6773 - val_acc: 0.3901
    Epoch 18/20
    9084/9084 [==============================] - 0s 7us/step - loss: 1.6616 - acc: 0.3920 - val_loss: 1.6748 - val_acc: 0.3970
    Epoch 19/20
    9084/9084 [==============================] - 0s 7us/step - loss: 1.6573 - acc: 0.3934 - val_loss: 1.6717 - val_acc: 0.3901
    Epoch 20/20
    9084/9084 [==============================] - 0s 7us/step - loss: 1.6566 - acc: 0.3928 - val_loss: 1.6750 - val_acc: 0.3931
    8908/8908 [==============================] - 0s 15us/step
    Classification score for NN:               precision    recall  f1-score   support
    
               1       0.00      0.00      0.00       925
               2       0.00      0.00      0.00       975
               3       0.00      0.00      0.00       150
               4       0.00      0.00      0.00       199
               5       0.00      0.00      0.00       828
               6       0.23      0.47      0.31      1625
               7       0.00      0.00      0.00      1247
               8       0.48      0.91      0.62      2959
    
       micro avg       0.39      0.39      0.39      8908
       macro avg       0.09      0.17      0.12      8908
    weighted avg       0.20      0.39      0.26      8908
    
    Classification score for SOAM1:               precision    recall  f1-score   support
    
               1       0.23      0.24      0.24       925
               2       0.20      0.20      0.20       975
               3       0.40      0.14      0.21       150
               4       0.48      0.48      0.48       199
               5       0.51      0.25      0.33       828
               6       0.32      0.31      0.32      1625
               7       0.33      0.21      0.26      1247
               8       0.60      0.81      0.69      2959
    
       micro avg       0.44      0.44      0.44      8908
       macro avg       0.38      0.33      0.34      8908
    weighted avg       0.42      0.44      0.42      8908
    
    data sample 30.0
    Train on 13626 samples, validate on 1515 samples
    Epoch 1/20
    13626/13626 [==============================] - 1s 73us/step - loss: 0.5305 - val_loss: 0.3339
    Epoch 2/20
    13626/13626 [==============================] - 0s 13us/step - loss: 0.3016 - val_loss: 0.2948
    Epoch 3/20
    13626/13626 [==============================] - 0s 12us/step - loss: 0.2895 - val_loss: 0.2902
    Epoch 4/20
    13626/13626 [==============================] - 0s 12us/step - loss: 0.2863 - val_loss: 0.2884
    Epoch 5/20
    13626/13626 [==============================] - 0s 12us/step - loss: 0.2851 - val_loss: 0.2875
    Epoch 6/20
    13626/13626 [==============================] - 0s 12us/step - loss: 0.2842 - val_loss: 0.2867
    Epoch 7/20
    13626/13626 [==============================] - 0s 12us/step - loss: 0.2831 - val_loss: 0.2854
    Epoch 8/20
    13626/13626 [==============================] - 0s 12us/step - loss: 0.2790 - val_loss: 0.2760
    Epoch 9/20
    13626/13626 [==============================] - 0s 12us/step - loss: 0.2670 - val_loss: 0.2658
    Epoch 10/20
    13626/13626 [==============================] - 0s 12us/step - loss: 0.2598 - val_loss: 0.2598
    Epoch 11/20
    13626/13626 [==============================] - 0s 13us/step - loss: 0.2543 - val_loss: 0.2549
    Epoch 12/20
    13626/13626 [==============================] - 0s 12us/step - loss: 0.2503 - val_loss: 0.2517
    Epoch 13/20
    13626/13626 [==============================] - 0s 12us/step - loss: 0.2460 - val_loss: 0.2457
    Epoch 14/20
    13626/13626 [==============================] - 0s 12us/step - loss: 0.2398 - val_loss: 0.2409
    Epoch 15/20
    13626/13626 [==============================] - 0s 12us/step - loss: 0.2362 - val_loss: 0.2382
    Epoch 16/20
    13626/13626 [==============================] - 0s 12us/step - loss: 0.2339 - val_loss: 0.2366
    Epoch 17/20
    13626/13626 [==============================] - 0s 13us/step - loss: 0.2324 - val_loss: 0.2355
    Epoch 18/20
    13626/13626 [==============================] - 0s 12us/step - loss: 0.2312 - val_loss: 0.2342
    Epoch 19/20
    13626/13626 [==============================] - 0s 12us/step - loss: 0.2298 - val_loss: 0.2330
    Epoch 20/20
    13626/13626 [==============================] - 0s 12us/step - loss: 0.2281 - val_loss: 0.2305
    Train on 13626 samples, validate on 1515 samples
    Epoch 1/20
    13626/13626 [==============================] - 0s 26us/step - loss: 2.0588 - acc: 0.1876 - val_loss: 2.0017 - val_acc: 0.1901
    Epoch 2/20
    13626/13626 [==============================] - 0s 7us/step - loss: 1.8932 - acc: 0.2757 - val_loss: 1.8271 - val_acc: 0.3274
    Epoch 3/20
    13626/13626 [==============================] - 0s 7us/step - loss: 1.8099 - acc: 0.3275 - val_loss: 1.8047 - val_acc: 0.3274
    Epoch 4/20
    13626/13626 [==============================] - 0s 7us/step - loss: 1.7958 - acc: 0.3275 - val_loss: 1.7943 - val_acc: 0.3274
    Epoch 5/20
    13626/13626 [==============================] - 0s 8us/step - loss: 1.7843 - acc: 0.3275 - val_loss: 1.7795 - val_acc: 0.3274
    Epoch 6/20
    13626/13626 [==============================] - 0s 7us/step - loss: 1.7673 - acc: 0.3275 - val_loss: 1.7588 - val_acc: 0.3274
    Epoch 7/20
    13626/13626 [==============================] - 0s 7us/step - loss: 1.7448 - acc: 0.3275 - val_loss: 1.7317 - val_acc: 0.3274
    Epoch 8/20
    13626/13626 [==============================] - 0s 7us/step - loss: 1.7259 - acc: 0.3281 - val_loss: 1.7162 - val_acc: 0.3300
    Epoch 9/20
    13626/13626 [==============================] - 0s 7us/step - loss: 1.7179 - acc: 0.3319 - val_loss: 1.7111 - val_acc: 0.3340
    Epoch 10/20
    13626/13626 [==============================] - 0s 7us/step - loss: 1.7138 - acc: 0.3374 - val_loss: 1.7061 - val_acc: 0.3380
    Epoch 11/20
    13626/13626 [==============================] - 0s 7us/step - loss: 1.7101 - acc: 0.3402 - val_loss: 1.7042 - val_acc: 0.3492
    Epoch 12/20
    13626/13626 [==============================] - 0s 7us/step - loss: 1.7073 - acc: 0.3418 - val_loss: 1.7015 - val_acc: 0.3426
    Epoch 13/20
    13626/13626 [==============================] - 0s 7us/step - loss: 1.7053 - acc: 0.3460 - val_loss: 1.6974 - val_acc: 0.3538
    Epoch 14/20
    13626/13626 [==============================] - 0s 7us/step - loss: 1.7029 - acc: 0.3496 - val_loss: 1.6958 - val_acc: 0.3525
    Epoch 15/20
    13626/13626 [==============================] - 0s 7us/step - loss: 1.7012 - acc: 0.3533 - val_loss: 1.6935 - val_acc: 0.3558
    Epoch 16/20
    13626/13626 [==============================] - 0s 7us/step - loss: 1.6998 - acc: 0.3562 - val_loss: 1.6908 - val_acc: 0.3696
    Epoch 17/20
    13626/13626 [==============================] - 0s 8us/step - loss: 1.6980 - acc: 0.3635 - val_loss: 1.6886 - val_acc: 0.3729
    Epoch 18/20
    13626/13626 [==============================] - 0s 7us/step - loss: 1.6965 - acc: 0.3658 - val_loss: 1.6904 - val_acc: 0.3683
    Epoch 19/20
    13626/13626 [==============================] - 0s 7us/step - loss: 1.6959 - acc: 0.3663 - val_loss: 1.6865 - val_acc: 0.3835
    Epoch 20/20
    13626/13626 [==============================] - 0s 7us/step - loss: 1.6944 - acc: 0.3729 - val_loss: 1.6856 - val_acc: 0.3769
    8908/8908 [==============================] - 0s 23us/step
    Classification score for NN:               precision    recall  f1-score   support
    
               1       0.00      0.00      0.00       925
               2       0.00      0.00      0.00       975
               3       0.00      0.00      0.00       150
               4       0.00      0.00      0.00       199
               5       0.00      0.00      0.00       828
               6       0.25      0.32      0.28      1625
               7       0.00      0.00      0.00      1247
               8       0.40      0.93      0.56      2959
    
       micro avg       0.37      0.37      0.37      8908
       macro avg       0.08      0.16      0.11      8908
    weighted avg       0.18      0.37      0.24      8908
    
    Classification score for SOAM1:               precision    recall  f1-score   support
    
               1       0.26      0.26      0.26       925
               2       0.24      0.20      0.22       975
               3       0.39      0.16      0.23       150
               4       0.41      0.35      0.38       199
               5       0.55      0.26      0.35       828
               6       0.33      0.40      0.36      1625
               7       0.32      0.26      0.29      1247
               8       0.62      0.74      0.67      2959
    
       micro avg       0.44      0.44      0.44      8908
       macro avg       0.39      0.33      0.34      8908
    weighted avg       0.43      0.44      0.42      8908
    
    data sample 40.0
    Train on 18170 samples, validate on 2019 samples
    Epoch 1/20
    18170/18170 [==============================] - 1s 69us/step - loss: 0.5248 - val_loss: 0.3040
    Epoch 2/20
    18170/18170 [==============================] - 0s 13us/step - loss: 0.2969 - val_loss: 0.2891
    Epoch 3/20
    18170/18170 [==============================] - 0s 13us/step - loss: 0.2879 - val_loss: 0.2842
    Epoch 4/20
    18170/18170 [==============================] - 0s 13us/step - loss: 0.2852 - val_loss: 0.2829
    Epoch 5/20
    18170/18170 [==============================] - 0s 13us/step - loss: 0.2834 - val_loss: 0.2792
    Epoch 6/20
    18170/18170 [==============================] - 0s 13us/step - loss: 0.2753 - val_loss: 0.2669
    Epoch 7/20
    18170/18170 [==============================] - 0s 13us/step - loss: 0.2651 - val_loss: 0.2601
    Epoch 8/20
    18170/18170 [==============================] - 0s 13us/step - loss: 0.2604 - val_loss: 0.2563
    Epoch 9/20
    18170/18170 [==============================] - 0s 13us/step - loss: 0.2570 - val_loss: 0.2537
    Epoch 10/20
    18170/18170 [==============================] - 0s 13us/step - loss: 0.2547 - val_loss: 0.2519
    Epoch 11/20
    18170/18170 [==============================] - 0s 13us/step - loss: 0.2531 - val_loss: 0.2504
    Epoch 12/20
    18170/18170 [==============================] - 0s 13us/step - loss: 0.2513 - val_loss: 0.2481
    Epoch 13/20
    18170/18170 [==============================] - 0s 13us/step - loss: 0.2484 - val_loss: 0.2445
    Epoch 14/20
    18170/18170 [==============================] - 0s 13us/step - loss: 0.2441 - val_loss: 0.2405
    Epoch 15/20
    18170/18170 [==============================] - 0s 13us/step - loss: 0.2405 - val_loss: 0.2380
    Epoch 16/20
    18170/18170 [==============================] - 0s 13us/step - loss: 0.2381 - val_loss: 0.2360
    Epoch 17/20
    18170/18170 [==============================] - 0s 13us/step - loss: 0.2360 - val_loss: 0.2338
    Epoch 18/20
    18170/18170 [==============================] - 0s 13us/step - loss: 0.2341 - val_loss: 0.2324
    Epoch 19/20
    18170/18170 [==============================] - 0s 13us/step - loss: 0.2322 - val_loss: 0.2304
    Epoch 20/20
    18170/18170 [==============================] - 0s 13us/step - loss: 0.2304 - val_loss: 0.2289
    Train on 18170 samples, validate on 2019 samples
    Epoch 1/20
    18170/18170 [==============================] - 0s 24us/step - loss: 2.0617 - acc: 0.3189 - val_loss: 2.0173 - val_acc: 0.3274
    Epoch 2/20
    18170/18170 [==============================] - 0s 8us/step - loss: 1.8914 - acc: 0.3275 - val_loss: 1.8148 - val_acc: 0.3274
    Epoch 3/20
    18170/18170 [==============================] - 0s 7us/step - loss: 1.7902 - acc: 0.3275 - val_loss: 1.7813 - val_acc: 0.3274
    Epoch 4/20
    18170/18170 [==============================] - 0s 7us/step - loss: 1.7751 - acc: 0.3275 - val_loss: 1.7695 - val_acc: 0.3274
    Epoch 5/20
    18170/18170 [==============================] - 0s 7us/step - loss: 1.7662 - acc: 0.3275 - val_loss: 1.7590 - val_acc: 0.3274
    Epoch 6/20
    18170/18170 [==============================] - 0s 7us/step - loss: 1.7585 - acc: 0.3280 - val_loss: 1.7515 - val_acc: 0.3284
    Epoch 7/20
    18170/18170 [==============================] - 0s 7us/step - loss: 1.7523 - acc: 0.3308 - val_loss: 1.7473 - val_acc: 0.3269
    Epoch 8/20
    18170/18170 [==============================] - 0s 7us/step - loss: 1.7477 - acc: 0.3319 - val_loss: 1.7431 - val_acc: 0.3314
    Epoch 9/20
    18170/18170 [==============================] - 0s 7us/step - loss: 1.7438 - acc: 0.3365 - val_loss: 1.7385 - val_acc: 0.3358
    Epoch 10/20
    18170/18170 [==============================] - 0s 7us/step - loss: 1.7364 - acc: 0.3453 - val_loss: 1.7227 - val_acc: 0.3616
    Epoch 11/20
    18170/18170 [==============================] - 0s 7us/step - loss: 1.7232 - acc: 0.3665 - val_loss: 1.7174 - val_acc: 0.3675
    Epoch 12/20
    18170/18170 [==============================] - 0s 7us/step - loss: 1.7167 - acc: 0.3723 - val_loss: 1.7143 - val_acc: 0.3680
    Epoch 13/20
    18170/18170 [==============================] - 0s 7us/step - loss: 1.7122 - acc: 0.3729 - val_loss: 1.7084 - val_acc: 0.3690
    Epoch 14/20
    18170/18170 [==============================] - 0s 7us/step - loss: 1.7067 - acc: 0.3730 - val_loss: 1.7009 - val_acc: 0.3695
    Epoch 15/20
    18170/18170 [==============================] - 0s 7us/step - loss: 1.7019 - acc: 0.3719 - val_loss: 1.6956 - val_acc: 0.3660
    Epoch 16/20
    18170/18170 [==============================] - 0s 8us/step - loss: 1.6954 - acc: 0.3724 - val_loss: 1.6884 - val_acc: 0.3720
    Epoch 17/20
    18170/18170 [==============================] - 0s 7us/step - loss: 1.6904 - acc: 0.3755 - val_loss: 1.6831 - val_acc: 0.3794
    Epoch 18/20
    18170/18170 [==============================] - 0s 7us/step - loss: 1.6873 - acc: 0.3748 - val_loss: 1.6783 - val_acc: 0.3789
    Epoch 19/20
    18170/18170 [==============================] - 0s 8us/step - loss: 1.6829 - acc: 0.3781 - val_loss: 1.6756 - val_acc: 0.3749
    Epoch 20/20
    18170/18170 [==============================] - 0s 7us/step - loss: 1.6787 - acc: 0.3790 - val_loss: 1.6736 - val_acc: 0.3754
    8908/8908 [==============================] - 0s 31us/step
    Classification score for NN:               precision    recall  f1-score   support
    
               1       0.28      0.08      0.13       925
               2       0.00      0.00      0.00       975
               3       0.00      0.00      0.00       150
               4       0.00      0.00      0.00       199
               5       0.00      0.00      0.00       828
               6       0.21      0.31      0.25      1625
               7       0.00      0.00      0.00      1247
               8       0.44      0.93      0.60      2959
    
       micro avg       0.37      0.37      0.37      8908
       macro avg       0.12      0.17      0.12      8908
    weighted avg       0.21      0.37      0.26      8908
    
    Classification score for SOAM1:               precision    recall  f1-score   support
    
               1       0.25      0.25      0.25       925
               2       0.23      0.21      0.22       975
               3       0.38      0.26      0.31       150
               4       0.50      0.46      0.48       199
               5       0.49      0.33      0.39       828
               6       0.32      0.32      0.32      1625
               7       0.31      0.23      0.27      1247
               8       0.61      0.76      0.68      2959
    
       micro avg       0.44      0.44      0.44      8908
       macro avg       0.39      0.35      0.36      8908
    weighted avg       0.42      0.44      0.42      8908
    
    data sample 50.0
    Train on 22712 samples, validate on 2524 samples
    Epoch 1/20
    22712/22712 [==============================] - 2s 66us/step - loss: 0.4400 - val_loss: 0.2956
    Epoch 2/20
    22712/22712 [==============================] - 0s 16us/step - loss: 0.2897 - val_loss: 0.2865
    Epoch 3/20
    22712/22712 [==============================] - 0s 15us/step - loss: 0.2852 - val_loss: 0.2846
    Epoch 4/20
    22712/22712 [==============================] - 0s 16us/step - loss: 0.2831 - val_loss: 0.2809
    Epoch 5/20
    22712/22712 [==============================] - 0s 16us/step - loss: 0.2723 - val_loss: 0.2649
    Epoch 6/20
    22712/22712 [==============================] - 0s 15us/step - loss: 0.2609 - val_loss: 0.2568
    Epoch 7/20
    22712/22712 [==============================] - 0s 15us/step - loss: 0.2515 - val_loss: 0.2468
    Epoch 8/20
    22712/22712 [==============================] - 0s 15us/step - loss: 0.2433 - val_loss: 0.2411
    Epoch 9/20
    22712/22712 [==============================] - 0s 15us/step - loss: 0.2380 - val_loss: 0.2366
    Epoch 10/20
    22712/22712 [==============================] - 0s 15us/step - loss: 0.2341 - val_loss: 0.2334
    Epoch 11/20
    22712/22712 [==============================] - 0s 16us/step - loss: 0.2316 - val_loss: 0.2312
    Epoch 12/20
    22712/22712 [==============================] - 0s 15us/step - loss: 0.2297 - val_loss: 0.2294
    Epoch 13/20
    22712/22712 [==============================] - 0s 16us/step - loss: 0.2277 - val_loss: 0.2265
    Epoch 14/20
    22712/22712 [==============================] - 0s 16us/step - loss: 0.2244 - val_loss: 0.2232
    Epoch 15/20
    22712/22712 [==============================] - 0s 15us/step - loss: 0.2218 - val_loss: 0.2212
    Epoch 16/20
    22712/22712 [==============================] - 0s 15us/step - loss: 0.2198 - val_loss: 0.2191
    Epoch 17/20
    22712/22712 [==============================] - 0s 13us/step - loss: 0.2176 - val_loss: 0.2169
    Epoch 18/20
    22712/22712 [==============================] - 0s 13us/step - loss: 0.2156 - val_loss: 0.2150
    Epoch 19/20
    22712/22712 [==============================] - 0s 13us/step - loss: 0.2140 - val_loss: 0.2136
    Epoch 20/20
    22712/22712 [==============================] - 0s 13us/step - loss: 0.2126 - val_loss: 0.2127
    Train on 22712 samples, validate on 2524 samples
    Epoch 1/20
    22712/22712 [==============================] - 1s 23us/step - loss: 2.0114 - acc: 0.3208 - val_loss: 1.8345 - val_acc: 0.3277
    Epoch 2/20
    22712/22712 [==============================] - 0s 8us/step - loss: 1.8036 - acc: 0.3275 - val_loss: 1.7827 - val_acc: 0.3277
    Epoch 3/20
    22712/22712 [==============================] - 0s 7us/step - loss: 1.7733 - acc: 0.3275 - val_loss: 1.7600 - val_acc: 0.3277
    Epoch 4/20
    22712/22712 [==============================] - 0s 7us/step - loss: 1.7416 - acc: 0.3285 - val_loss: 1.7273 - val_acc: 0.3391
    Epoch 5/20
    22712/22712 [==============================] - 0s 7us/step - loss: 1.7006 - acc: 0.3683 - val_loss: 1.6866 - val_acc: 0.3796
    Epoch 6/20
    22712/22712 [==============================] - 0s 7us/step - loss: 1.6714 - acc: 0.3896 - val_loss: 1.6756 - val_acc: 0.3831
    Epoch 7/20
    22712/22712 [==============================] - 0s 7us/step - loss: 1.6650 - acc: 0.3883 - val_loss: 1.6718 - val_acc: 0.3768
    Epoch 8/20
    22712/22712 [==============================] - 0s 7us/step - loss: 1.6602 - acc: 0.3874 - val_loss: 1.6693 - val_acc: 0.3784
    Epoch 9/20
    22712/22712 [==============================] - 0s 8us/step - loss: 1.6577 - acc: 0.3870 - val_loss: 1.6655 - val_acc: 0.3712
    Epoch 10/20
    22712/22712 [==============================] - 0s 7us/step - loss: 1.6558 - acc: 0.3870 - val_loss: 1.6605 - val_acc: 0.3732
    Epoch 11/20
    22712/22712 [==============================] - 0s 7us/step - loss: 1.6532 - acc: 0.3878 - val_loss: 1.6574 - val_acc: 0.3780
    Epoch 12/20
    22712/22712 [==============================] - 0s 8us/step - loss: 1.6519 - acc: 0.3872 - val_loss: 1.6575 - val_acc: 0.3764
    Epoch 13/20
    22712/22712 [==============================] - 0s 7us/step - loss: 1.6491 - acc: 0.3869 - val_loss: 1.6533 - val_acc: 0.3764
    Epoch 14/20
    22712/22712 [==============================] - 0s 7us/step - loss: 1.6465 - acc: 0.3873 - val_loss: 1.6519 - val_acc: 0.3780
    Epoch 15/20
    22712/22712 [==============================] - 0s 7us/step - loss: 1.6442 - acc: 0.3865 - val_loss: 1.6488 - val_acc: 0.3772
    Epoch 16/20
    22712/22712 [==============================] - 0s 7us/step - loss: 1.6420 - acc: 0.3877 - val_loss: 1.6455 - val_acc: 0.3800
    Epoch 17/20
    22712/22712 [==============================] - 0s 8us/step - loss: 1.6379 - acc: 0.3919 - val_loss: 1.6414 - val_acc: 0.3827
    Epoch 18/20
    22712/22712 [==============================] - 0s 7us/step - loss: 1.6377 - acc: 0.3904 - val_loss: 1.6370 - val_acc: 0.3859
    Epoch 19/20
    22712/22712 [==============================] - 0s 7us/step - loss: 1.6314 - acc: 0.3936 - val_loss: 1.6336 - val_acc: 0.3942
    Epoch 20/20
    22712/22712 [==============================] - 0s 7us/step - loss: 1.6267 - acc: 0.3996 - val_loss: 1.6319 - val_acc: 0.3966
    8908/8908 [==============================] - 0s 39us/step
    Classification score for NN:               precision    recall  f1-score   support
    
               1       0.31      0.06      0.10       925
               2       0.31      0.10      0.15       975
               3       0.00      0.00      0.00       150
               4       0.00      0.00      0.00       199
               5       0.00      0.00      0.00       828
               6       0.24      0.33      0.28      1625
               7       0.41      0.08      0.13      1247
               8       0.46      0.93      0.62      2959
    
       micro avg       0.40      0.40      0.40      8908
       macro avg       0.22      0.19      0.16      8908
    weighted avg       0.32      0.40      0.30      8908
    
    Classification score for SOAM1:               precision    recall  f1-score   support
    
               1       0.22      0.22      0.22       925
               2       0.25      0.24      0.24       975
               3       0.38      0.23      0.29       150
               4       0.47      0.43      0.45       199
               5       0.42      0.26      0.32       828
               6       0.33      0.36      0.34      1625
               7       0.33      0.25      0.28      1247
               8       0.63      0.77      0.69      2959
    
       micro avg       0.44      0.44      0.44      8908
       macro avg       0.38      0.34      0.36      8908
    weighted avg       0.42      0.44      0.43      8908
    
    data sample 60.0
    Train on 27254 samples, validate on 3029 samples
    Epoch 1/20
    27254/27254 [==============================] - 2s 65us/step - loss: 0.4424 - val_loss: 0.2928
    Epoch 2/20
    27254/27254 [==============================] - 0s 14us/step - loss: 0.2893 - val_loss: 0.2849
    Epoch 3/20
    27254/27254 [==============================] - 0s 13us/step - loss: 0.2849 - val_loss: 0.2819
    Epoch 4/20
    27254/27254 [==============================] - 0s 14us/step - loss: 0.2750 - val_loss: 0.2646
    Epoch 5/20
    27254/27254 [==============================] - 0s 14us/step - loss: 0.2619 - val_loss: 0.2580
    Epoch 6/20
    27254/27254 [==============================] - 0s 14us/step - loss: 0.2576 - val_loss: 0.2544
    Epoch 7/20
    27254/27254 [==============================] - 0s 14us/step - loss: 0.2526 - val_loss: 0.2473
    Epoch 8/20
    27254/27254 [==============================] - 0s 14us/step - loss: 0.2430 - val_loss: 0.2361
    Epoch 9/20
    27254/27254 [==============================] - 0s 14us/step - loss: 0.2344 - val_loss: 0.2296
    Epoch 10/20
    27254/27254 [==============================] - 0s 14us/step - loss: 0.2292 - val_loss: 0.2259
    Epoch 11/20
    27254/27254 [==============================] - 0s 14us/step - loss: 0.2260 - val_loss: 0.2235
    Epoch 12/20
    27254/27254 [==============================] - 0s 14us/step - loss: 0.2235 - val_loss: 0.2209
    Epoch 13/20
    27254/27254 [==============================] - 0s 14us/step - loss: 0.2212 - val_loss: 0.2187
    Epoch 14/20
    27254/27254 [==============================] - 0s 14us/step - loss: 0.2192 - val_loss: 0.2171
    Epoch 15/20
    27254/27254 [==============================] - 0s 14us/step - loss: 0.2165 - val_loss: 0.2134
    Epoch 16/20
    27254/27254 [==============================] - 0s 14us/step - loss: 0.2133 - val_loss: 0.2106
    Epoch 17/20
    27254/27254 [==============================] - 0s 14us/step - loss: 0.2108 - val_loss: 0.2084
    Epoch 18/20
    27254/27254 [==============================] - 0s 14us/step - loss: 0.2082 - val_loss: 0.2058
    Epoch 19/20
    27254/27254 [==============================] - 0s 14us/step - loss: 0.2059 - val_loss: 0.2043
    Epoch 20/20
    27254/27254 [==============================] - 0s 14us/step - loss: 0.2039 - val_loss: 0.2019
    Train on 27254 samples, validate on 3029 samples
    Epoch 1/20
    27254/27254 [==============================] - 1s 24us/step - loss: 1.9492 - acc: 0.3195 - val_loss: 1.7977 - val_acc: 0.3275
    Epoch 2/20
    27254/27254 [==============================] - 0s 9us/step - loss: 1.7790 - acc: 0.3275 - val_loss: 1.7619 - val_acc: 0.3275
    Epoch 3/20
    27254/27254 [==============================] - 0s 9us/step - loss: 1.7422 - acc: 0.3293 - val_loss: 1.7340 - val_acc: 0.3371
    Epoch 4/20
    27254/27254 [==============================] - 0s 9us/step - loss: 1.7162 - acc: 0.3413 - val_loss: 1.7175 - val_acc: 0.3516
    Epoch 5/20
    27254/27254 [==============================] - 0s 9us/step - loss: 1.7002 - acc: 0.3593 - val_loss: 1.7082 - val_acc: 0.3608
    Epoch 6/20
    27254/27254 [==============================] - 0s 9us/step - loss: 1.6880 - acc: 0.3752 - val_loss: 1.6987 - val_acc: 0.3816
    Epoch 7/20
    27254/27254 [==============================] - 0s 9us/step - loss: 1.6800 - acc: 0.3880 - val_loss: 1.6967 - val_acc: 0.3807
    Epoch 8/20
    27254/27254 [==============================] - 0s 9us/step - loss: 1.6728 - acc: 0.3918 - val_loss: 1.6852 - val_acc: 0.3866
    Epoch 9/20
    27254/27254 [==============================] - 0s 9us/step - loss: 1.6682 - acc: 0.3940 - val_loss: 1.6800 - val_acc: 0.3866
    Epoch 10/20
    27254/27254 [==============================] - 0s 8us/step - loss: 1.6631 - acc: 0.3945 - val_loss: 1.6755 - val_acc: 0.3882
    Epoch 11/20
    27254/27254 [==============================] - 0s 9us/step - loss: 1.6580 - acc: 0.3955 - val_loss: 1.6707 - val_acc: 0.3882
    Epoch 12/20
    27254/27254 [==============================] - 0s 9us/step - loss: 1.6514 - acc: 0.3959 - val_loss: 1.6646 - val_acc: 0.3886
    Epoch 13/20
    27254/27254 [==============================] - 0s 9us/step - loss: 1.6396 - acc: 0.3920 - val_loss: 1.6424 - val_acc: 0.3849
    Epoch 14/20
    27254/27254 [==============================] - 0s 9us/step - loss: 1.6164 - acc: 0.3963 - val_loss: 1.6270 - val_acc: 0.3873
    Epoch 15/20
    27254/27254 [==============================] - 0s 9us/step - loss: 1.6100 - acc: 0.4001 - val_loss: 1.6262 - val_acc: 0.3889
    Epoch 16/20
    27254/27254 [==============================] - 0s 9us/step - loss: 1.6076 - acc: 0.4031 - val_loss: 1.6213 - val_acc: 0.3972
    Epoch 17/20
    27254/27254 [==============================] - 0s 9us/step - loss: 1.6050 - acc: 0.4046 - val_loss: 1.6197 - val_acc: 0.3955
    Epoch 18/20
    27254/27254 [==============================] - 0s 10us/step - loss: 1.6051 - acc: 0.4053 - val_loss: 1.6237 - val_acc: 0.3962
    Epoch 19/20
    27254/27254 [==============================] - 0s 9us/step - loss: 1.6039 - acc: 0.4045 - val_loss: 1.6163 - val_acc: 0.3991
    Epoch 20/20
    27254/27254 [==============================] - 0s 9us/step - loss: 1.6016 - acc: 0.4068 - val_loss: 1.6178 - val_acc: 0.4001
    8908/8908 [==============================] - 0s 52us/step
    Classification score for NN:               precision    recall  f1-score   support
    
               1       0.34      0.15      0.20       925
               2       0.33      0.07      0.12       975
               3       0.00      0.00      0.00       150
               4       0.22      0.25      0.23       199
               5       0.00      0.00      0.00       828
               6       0.27      0.51      0.36      1625
               7       0.00      0.00      0.00      1247
               8       0.52      0.88      0.65      2959
    
       micro avg       0.41      0.41      0.41      8908
       macro avg       0.21      0.23      0.20      8908
    weighted avg       0.30      0.41      0.32      8908
    
    Classification score for SOAM1:               precision    recall  f1-score   support
    
               1       0.23      0.26      0.25       925
               2       0.25      0.21      0.23       975
               3       0.33      0.23      0.27       150
               4       0.46      0.32      0.38       199
               5       0.50      0.26      0.34       828
               6       0.33      0.33      0.33      1625
               7       0.30      0.21      0.25      1247
               8       0.61      0.80      0.69      2959
    
       micro avg       0.44      0.44      0.44      8908
       macro avg       0.38      0.33      0.34      8908
    weighted avg       0.42      0.44      0.42      8908
    
    data sample 70.0
    Train on 31797 samples, validate on 3534 samples
    Epoch 1/20
    31797/31797 [==============================] - 2s 65us/step - loss: 0.3939 - val_loss: 0.2859
    Epoch 2/20
    31797/31797 [==============================] - 0s 14us/step - loss: 0.2846 - val_loss: 0.2825
    Epoch 3/20
    31797/31797 [==============================] - 0s 14us/step - loss: 0.2799 - val_loss: 0.2705
    Epoch 4/20
    31797/31797 [==============================] - 0s 14us/step - loss: 0.2606 - val_loss: 0.2525
    Epoch 5/20
    31797/31797 [==============================] - 0s 14us/step - loss: 0.2476 - val_loss: 0.2414
    Epoch 6/20
    31797/31797 [==============================] - 0s 14us/step - loss: 0.2378 - val_loss: 0.2342
    Epoch 7/20
    31797/31797 [==============================] - 0s 14us/step - loss: 0.2322 - val_loss: 0.2288
    Epoch 8/20
    31797/31797 [==============================] - 0s 14us/step - loss: 0.2271 - val_loss: 0.2247
    Epoch 9/20
    31797/31797 [==============================] - 0s 14us/step - loss: 0.2236 - val_loss: 0.2217
    Epoch 10/20
    31797/31797 [==============================] - 0s 14us/step - loss: 0.2213 - val_loss: 0.2200
    Epoch 11/20
    31797/31797 [==============================] - 0s 14us/step - loss: 0.2192 - val_loss: 0.2178
    Epoch 12/20
    31797/31797 [==============================] - 0s 14us/step - loss: 0.2159 - val_loss: 0.2143
    Epoch 13/20
    31797/31797 [==============================] - 0s 14us/step - loss: 0.2136 - val_loss: 0.2128
    Epoch 14/20
    31797/31797 [==============================] - 0s 14us/step - loss: 0.2122 - val_loss: 0.2116
    Epoch 15/20
    31797/31797 [==============================] - 0s 14us/step - loss: 0.2110 - val_loss: 0.2103
    Epoch 16/20
    31797/31797 [==============================] - 0s 14us/step - loss: 0.2099 - val_loss: 0.2102
    Epoch 17/20
    31797/31797 [==============================] - 0s 14us/step - loss: 0.2088 - val_loss: 0.2090
    Epoch 18/20
    31797/31797 [==============================] - 0s 14us/step - loss: 0.2077 - val_loss: 0.2071
    Epoch 19/20
    31797/31797 [==============================] - 0s 14us/step - loss: 0.2061 - val_loss: 0.2057
    Epoch 20/20
    31797/31797 [==============================] - 0s 14us/step - loss: 0.2045 - val_loss: 0.2041
    Train on 31797 samples, validate on 3534 samples
    Epoch 1/20
    31797/31797 [==============================] - 1s 23us/step - loss: 1.9450 - acc: 0.2319 - val_loss: 1.7789 - val_acc: 0.3274
    Epoch 2/20
    31797/31797 [==============================] - 0s 8us/step - loss: 1.7716 - acc: 0.3275 - val_loss: 1.7703 - val_acc: 0.3274
    Epoch 3/20
    31797/31797 [==============================] - 0s 8us/step - loss: 1.7626 - acc: 0.3275 - val_loss: 1.7609 - val_acc: 0.3274
    Epoch 4/20
    31797/31797 [==============================] - 0s 8us/step - loss: 1.7526 - acc: 0.3277 - val_loss: 1.7530 - val_acc: 0.3299
    Epoch 5/20
    31797/31797 [==============================] - 0s 8us/step - loss: 1.7419 - acc: 0.3382 - val_loss: 1.7324 - val_acc: 0.3563
    Epoch 6/20
    31797/31797 [==============================] - 0s 8us/step - loss: 1.7099 - acc: 0.3654 - val_loss: 1.7112 - val_acc: 0.3738
    Epoch 7/20
    31797/31797 [==============================] - 0s 8us/step - loss: 1.6930 - acc: 0.3727 - val_loss: 1.6978 - val_acc: 0.3724
    Epoch 8/20
    31797/31797 [==============================] - 0s 8us/step - loss: 1.6844 - acc: 0.3755 - val_loss: 1.6927 - val_acc: 0.3746
    Epoch 9/20
    31797/31797 [==============================] - 0s 8us/step - loss: 1.6789 - acc: 0.3762 - val_loss: 1.6888 - val_acc: 0.3721
    Epoch 10/20
    31797/31797 [==============================] - 0s 8us/step - loss: 1.6726 - acc: 0.3776 - val_loss: 1.6815 - val_acc: 0.3746
    Epoch 11/20
    31797/31797 [==============================] - 0s 8us/step - loss: 1.6667 - acc: 0.3781 - val_loss: 1.6825 - val_acc: 0.3789
    Epoch 12/20
    31797/31797 [==============================] - 0s 8us/step - loss: 1.6607 - acc: 0.3809 - val_loss: 1.6705 - val_acc: 0.3783
    Epoch 13/20
    31797/31797 [==============================] - 0s 8us/step - loss: 1.6553 - acc: 0.3834 - val_loss: 1.6659 - val_acc: 0.3783
    Epoch 14/20
    31797/31797 [==============================] - 0s 8us/step - loss: 1.6510 - acc: 0.3838 - val_loss: 1.6615 - val_acc: 0.3809
    Epoch 15/20
    31797/31797 [==============================] - 0s 8us/step - loss: 1.6461 - acc: 0.3903 - val_loss: 1.6630 - val_acc: 0.3848
    Epoch 16/20
    31797/31797 [==============================] - 0s 8us/step - loss: 1.6430 - acc: 0.3935 - val_loss: 1.6556 - val_acc: 0.3950
    Epoch 17/20
    31797/31797 [==============================] - 0s 8us/step - loss: 1.6390 - acc: 0.3974 - val_loss: 1.6546 - val_acc: 0.3990
    Epoch 18/20
    31797/31797 [==============================] - 0s 8us/step - loss: 1.6374 - acc: 0.3979 - val_loss: 1.6536 - val_acc: 0.4007
    Epoch 19/20
    31797/31797 [==============================] - 0s 8us/step - loss: 1.6336 - acc: 0.4010 - val_loss: 1.6503 - val_acc: 0.4029
    Epoch 20/20
    31797/31797 [==============================] - 0s 8us/step - loss: 1.6318 - acc: 0.4030 - val_loss: 1.6491 - val_acc: 0.4035
    8908/8908 [==============================] - 1s 57us/step
    Classification score for NN:               precision    recall  f1-score   support
    
               1       0.39      0.13      0.20       925
               2       0.28      0.05      0.09       975
               3       0.00      0.00      0.00       150
               4       0.00      0.00      0.00       199
               5       0.00      0.00      0.00       828
               6       0.26      0.48      0.34      1625
               7       0.39      0.11      0.18      1247
               8       0.50      0.87      0.64      2959
    
       micro avg       0.41      0.41      0.41      8908
       macro avg       0.23      0.21      0.18      8908
    weighted avg       0.34      0.41      0.33      8908
    
    Classification score for SOAM1:               precision    recall  f1-score   support
    
               1       0.24      0.26      0.25       925
               2       0.23      0.22      0.23       975
               3       0.32      0.24      0.27       150
               4       0.52      0.36      0.43       199
               5       0.47      0.31      0.37       828
               6       0.34      0.33      0.34      1625
               7       0.32      0.23      0.27      1247
               8       0.62      0.76      0.68      2959
    
       micro avg       0.44      0.44      0.44      8908
       macro avg       0.38      0.34      0.35      8908
    weighted avg       0.42      0.44      0.43      8908
    
    data sample 80.0
    Train on 36340 samples, validate on 4038 samples
    Epoch 1/20
    36340/36340 [==============================] - 2s 65us/step - loss: 0.3916 - val_loss: 0.2886
    Epoch 2/20
    36340/36340 [==============================] - 1s 15us/step - loss: 0.2863 - val_loss: 0.2834
    Epoch 3/20
    36340/36340 [==============================] - 1s 14us/step - loss: 0.2826 - val_loss: 0.2756
    Epoch 4/20
    36340/36340 [==============================] - 1s 15us/step - loss: 0.2643 - val_loss: 0.2550
    Epoch 5/20
    36340/36340 [==============================] - 1s 14us/step - loss: 0.2505 - val_loss: 0.2455
    Epoch 6/20
    36340/36340 [==============================] - 1s 14us/step - loss: 0.2434 - val_loss: 0.2395
    Epoch 7/20
    36340/36340 [==============================] - 1s 15us/step - loss: 0.2377 - val_loss: 0.2340
    Epoch 8/20
    36340/36340 [==============================] - 1s 15us/step - loss: 0.2325 - val_loss: 0.2288
    Epoch 9/20
    36340/36340 [==============================] - 1s 15us/step - loss: 0.2267 - val_loss: 0.2229
    Epoch 10/20
    36340/36340 [==============================] - 1s 15us/step - loss: 0.2220 - val_loss: 0.2191
    Epoch 11/20
    36340/36340 [==============================] - 1s 14us/step - loss: 0.2187 - val_loss: 0.2162
    Epoch 12/20
    36340/36340 [==============================] - 1s 15us/step - loss: 0.2158 - val_loss: 0.2136
    Epoch 13/20
    36340/36340 [==============================] - 1s 14us/step - loss: 0.2135 - val_loss: 0.2113
    Epoch 14/20
    36340/36340 [==============================] - 1s 15us/step - loss: 0.2110 - val_loss: 0.2089
    Epoch 15/20
    36340/36340 [==============================] - 1s 14us/step - loss: 0.2085 - val_loss: 0.2067
    Epoch 16/20
    36340/36340 [==============================] - 1s 15us/step - loss: 0.2063 - val_loss: 0.2047
    Epoch 17/20
    36340/36340 [==============================] - 1s 14us/step - loss: 0.2046 - val_loss: 0.2031
    Epoch 18/20
    36340/36340 [==============================] - 1s 14us/step - loss: 0.2032 - val_loss: 0.2021
    Epoch 19/20
    36340/36340 [==============================] - 1s 14us/step - loss: 0.2018 - val_loss: 0.2012
    Epoch 20/20
    36340/36340 [==============================] - 1s 15us/step - loss: 0.2007 - val_loss: 0.1996
    Train on 36340 samples, validate on 4038 samples
    Epoch 1/20
    36340/36340 [==============================] - 1s 27us/step - loss: 1.9088 - acc: 0.3247 - val_loss: 1.7850 - val_acc: 0.3276
    Epoch 2/20
    36340/36340 [==============================] - 0s 10us/step - loss: 1.7774 - acc: 0.3275 - val_loss: 1.7572 - val_acc: 0.3276
    Epoch 3/20
    36340/36340 [==============================] - 0s 10us/step - loss: 1.7411 - acc: 0.3280 - val_loss: 1.7193 - val_acc: 0.3370
    Epoch 4/20
    36340/36340 [==============================] - 0s 9us/step - loss: 1.6969 - acc: 0.3640 - val_loss: 1.6701 - val_acc: 0.3856
    Epoch 5/20
    36340/36340 [==============================] - 0s 10us/step - loss: 1.6710 - acc: 0.3840 - val_loss: 1.6585 - val_acc: 0.3848
    Epoch 6/20
    36340/36340 [==============================] - 0s 9us/step - loss: 1.6590 - acc: 0.3831 - val_loss: 1.6490 - val_acc: 0.3787
    Epoch 7/20
    36340/36340 [==============================] - 0s 10us/step - loss: 1.6459 - acc: 0.3852 - val_loss: 1.6371 - val_acc: 0.3843
    Epoch 8/20
    36340/36340 [==============================] - 0s 10us/step - loss: 1.6359 - acc: 0.3859 - val_loss: 1.6286 - val_acc: 0.3853
    Epoch 9/20
    36340/36340 [==============================] - 0s 10us/step - loss: 1.6283 - acc: 0.3876 - val_loss: 1.6219 - val_acc: 0.3930
    Epoch 10/20
    36340/36340 [==============================] - 0s 10us/step - loss: 1.6241 - acc: 0.3903 - val_loss: 1.6196 - val_acc: 0.3908
    Epoch 11/20
    36340/36340 [==============================] - 0s 10us/step - loss: 1.6196 - acc: 0.3906 - val_loss: 1.6159 - val_acc: 0.3950
    Epoch 12/20
    36340/36340 [==============================] - 0s 10us/step - loss: 1.6164 - acc: 0.3939 - val_loss: 1.6154 - val_acc: 0.3903
    Epoch 13/20
    36340/36340 [==============================] - 0s 9us/step - loss: 1.6144 - acc: 0.3944 - val_loss: 1.6132 - val_acc: 0.3965
    Epoch 14/20
    36340/36340 [==============================] - 0s 9us/step - loss: 1.6120 - acc: 0.3971 - val_loss: 1.6089 - val_acc: 0.3990
    Epoch 15/20
    36340/36340 [==============================] - 0s 8us/step - loss: 1.6092 - acc: 0.3968 - val_loss: 1.6063 - val_acc: 0.3985
    Epoch 16/20
    36340/36340 [==============================] - 0s 8us/step - loss: 1.6080 - acc: 0.3987 - val_loss: 1.6056 - val_acc: 0.3975
    Epoch 17/20
    36340/36340 [==============================] - 0s 8us/step - loss: 1.6064 - acc: 0.3972 - val_loss: 1.6068 - val_acc: 0.3980
    Epoch 18/20
    36340/36340 [==============================] - 0s 8us/step - loss: 1.6061 - acc: 0.3986 - val_loss: 1.6108 - val_acc: 0.3950
    Epoch 19/20
    36340/36340 [==============================] - 0s 8us/step - loss: 1.6052 - acc: 0.3991 - val_loss: 1.6044 - val_acc: 0.3920
    Epoch 20/20
    36340/36340 [==============================] - 0s 8us/step - loss: 1.6032 - acc: 0.3992 - val_loss: 1.6062 - val_acc: 0.4000
    8908/8908 [==============================] - 1s 66us/step
    Classification score for NN:               precision    recall  f1-score   support
    
               1       0.37      0.13      0.20       925
               2       0.24      0.07      0.11       975
               3       0.00      0.00      0.00       150
               4       0.00      0.00      0.00       199
               5       0.00      0.00      0.00       828
               6       0.26      0.50      0.35      1625
               7       0.38      0.11      0.17      1247
               8       0.53      0.86      0.65      2959
    
       micro avg       0.42      0.42      0.42      8908
       macro avg       0.22      0.21      0.18      8908
    weighted avg       0.34      0.42      0.34      8908
    
    Classification score for SOAM1:               precision    recall  f1-score   support
    
               1       0.23      0.23      0.23       925
               2       0.24      0.22      0.23       975
               3       0.37      0.30      0.33       150
               4       0.51      0.38      0.44       199
               5       0.42      0.32      0.36       828
               6       0.31      0.33      0.32      1625
               7       0.28      0.20      0.23      1247
               8       0.62      0.75      0.68      2959
    
       micro avg       0.43      0.43      0.43      8908
       macro avg       0.37      0.34      0.35      8908
    weighted avg       0.41      0.43      0.42      8908
    
    data sample 90.0
    Train on 40882 samples, validate on 4543 samples
    Epoch 1/20
    40882/40882 [==============================] - 3s 67us/step - loss: 0.3769 - val_loss: 0.2815
    Epoch 2/20
    40882/40882 [==============================] - 1s 15us/step - loss: 0.2703 - val_loss: 0.2610
    Epoch 3/20
    40882/40882 [==============================] - 1s 15us/step - loss: 0.2591 - val_loss: 0.2531
    Epoch 4/20
    40882/40882 [==============================] - 1s 15us/step - loss: 0.2490 - val_loss: 0.2432
    Epoch 5/20
    40882/40882 [==============================] - 1s 15us/step - loss: 0.2419 - val_loss: 0.2364
    Epoch 6/20
    40882/40882 [==============================] - 1s 15us/step - loss: 0.2347 - val_loss: 0.2305
    Epoch 7/20
    40882/40882 [==============================] - 1s 15us/step - loss: 0.2294 - val_loss: 0.2259
    Epoch 8/20
    40882/40882 [==============================] - 1s 15us/step - loss: 0.2254 - val_loss: 0.2223
    Epoch 9/20
    40882/40882 [==============================] - 1s 15us/step - loss: 0.2214 - val_loss: 0.2179
    Epoch 10/20
    40882/40882 [==============================] - 1s 15us/step - loss: 0.2169 - val_loss: 0.2136
    Epoch 11/20
    40882/40882 [==============================] - 1s 15us/step - loss: 0.2134 - val_loss: 0.2113
    Epoch 12/20
    40882/40882 [==============================] - 1s 15us/step - loss: 0.2112 - val_loss: 0.2094
    Epoch 13/20
    40882/40882 [==============================] - 1s 15us/step - loss: 0.2095 - val_loss: 0.2083
    Epoch 14/20
    40882/40882 [==============================] - 1s 15us/step - loss: 0.2082 - val_loss: 0.2071
    Epoch 15/20
    40882/40882 [==============================] - 1s 15us/step - loss: 0.2070 - val_loss: 0.2057
    Epoch 16/20
    40882/40882 [==============================] - 1s 15us/step - loss: 0.2059 - val_loss: 0.2046
    Epoch 17/20
    40882/40882 [==============================] - 1s 16us/step - loss: 0.2049 - val_loss: 0.2037
    Epoch 18/20
    40882/40882 [==============================] - 1s 17us/step - loss: 0.2040 - val_loss: 0.2031
    Epoch 19/20
    40882/40882 [==============================] - 1s 17us/step - loss: 0.2032 - val_loss: 0.2022
    Epoch 20/20
    40882/40882 [==============================] - 1s 17us/step - loss: 0.2024 - val_loss: 0.2012
    Train on 40882 samples, validate on 4543 samples
    Epoch 1/20
    40882/40882 [==============================] - 1s 24us/step - loss: 1.9060 - acc: 0.3211 - val_loss: 1.7935 - val_acc: 0.3275
    Epoch 2/20
    40882/40882 [==============================] - 0s 9us/step - loss: 1.7887 - acc: 0.3275 - val_loss: 1.7770 - val_acc: 0.3275
    Epoch 3/20
    40882/40882 [==============================] - 0s 8us/step - loss: 1.7665 - acc: 0.3275 - val_loss: 1.7527 - val_acc: 0.3278
    Epoch 4/20
    40882/40882 [==============================] - 0s 8us/step - loss: 1.7457 - acc: 0.3318 - val_loss: 1.7400 - val_acc: 0.3348
    Epoch 5/20
    40882/40882 [==============================] - 0s 8us/step - loss: 1.7311 - acc: 0.3475 - val_loss: 1.7183 - val_acc: 0.3689
    Epoch 6/20
    40882/40882 [==============================] - 0s 8us/step - loss: 1.7022 - acc: 0.3696 - val_loss: 1.7022 - val_acc: 0.3632
    Epoch 7/20
    40882/40882 [==============================] - 0s 8us/step - loss: 1.6815 - acc: 0.3716 - val_loss: 1.6821 - val_acc: 0.3696
    Epoch 8/20
    40882/40882 [==============================] - 0s 8us/step - loss: 1.6714 - acc: 0.3746 - val_loss: 1.6723 - val_acc: 0.3742
    Epoch 9/20
    40882/40882 [==============================] - 0s 8us/step - loss: 1.6637 - acc: 0.3788 - val_loss: 1.6638 - val_acc: 0.3762
    Epoch 10/20
    40882/40882 [==============================] - 0s 8us/step - loss: 1.6539 - acc: 0.3798 - val_loss: 1.6554 - val_acc: 0.3821
    Epoch 11/20
    40882/40882 [==============================] - 0s 8us/step - loss: 1.6486 - acc: 0.3849 - val_loss: 1.6509 - val_acc: 0.3837
    Epoch 12/20
    40882/40882 [==============================] - 0s 8us/step - loss: 1.6455 - acc: 0.3864 - val_loss: 1.6505 - val_acc: 0.3905
    Epoch 13/20
    40882/40882 [==============================] - 0s 8us/step - loss: 1.6458 - acc: 0.3844 - val_loss: 1.6518 - val_acc: 0.3923
    Epoch 14/20
    40882/40882 [==============================] - 0s 9us/step - loss: 1.6421 - acc: 0.3869 - val_loss: 1.6434 - val_acc: 0.3898
    Epoch 15/20
    40882/40882 [==============================] - 0s 8us/step - loss: 1.6390 - acc: 0.3873 - val_loss: 1.6439 - val_acc: 0.3883
    Epoch 16/20
    40882/40882 [==============================] - 0s 8us/step - loss: 1.6357 - acc: 0.3895 - val_loss: 1.6386 - val_acc: 0.3916
    Epoch 17/20
    40882/40882 [==============================] - 0s 8us/step - loss: 1.6342 - acc: 0.3910 - val_loss: 1.6347 - val_acc: 0.3914
    Epoch 18/20
    40882/40882 [==============================] - 0s 8us/step - loss: 1.6303 - acc: 0.3906 - val_loss: 1.6387 - val_acc: 0.3960
    Epoch 19/20
    40882/40882 [==============================] - 0s 8us/step - loss: 1.6276 - acc: 0.3925 - val_loss: 1.6286 - val_acc: 0.4013
    Epoch 20/20
    40882/40882 [==============================] - 0s 8us/step - loss: 1.6249 - acc: 0.3937 - val_loss: 1.6295 - val_acc: 0.4037
    8908/8908 [==============================] - 1s 77us/step
    Classification score for NN:               precision    recall  f1-score   support
    
               1       0.32      0.20      0.25       925
               2       0.19      0.01      0.02       975
               3       0.00      0.00      0.00       150
               4       0.33      0.03      0.06       199
               5       0.00      0.00      0.00       828
               6       0.26      0.43      0.33      1625
               7       0.37      0.12      0.18      1247
               8       0.49      0.86      0.62      2959
    
       micro avg       0.40      0.40      0.40      8908
       macro avg       0.24      0.21      0.18      8908
    weighted avg       0.32      0.40      0.32      8908
    
    Classification score for SOAM1:               precision    recall  f1-score   support
    
               1       0.24      0.25      0.25       925
               2       0.25      0.21      0.23       975
               3       0.37      0.19      0.25       150
               4       0.53      0.51      0.52       199
               5       0.47      0.28      0.35       828
               6       0.34      0.35      0.35      1625
               7       0.35      0.29      0.32      1247
               8       0.63      0.78      0.70      2959
    
       micro avg       0.45      0.45      0.45      8908
       macro avg       0.40      0.36      0.37      8908
    weighted avg       0.44      0.45      0.44      8908
    
    data sample 99.0
    Train on 44971 samples, validate on 4997 samples
    Epoch 1/20
    44971/44971 [==============================] - 3s 67us/step - loss: 0.3675 - val_loss: 0.2857
    Epoch 2/20
    44971/44971 [==============================] - 1s 16us/step - loss: 0.2819 - val_loss: 0.2738
    Epoch 3/20
    44971/44971 [==============================] - 1s 15us/step - loss: 0.2582 - val_loss: 0.2498
    Epoch 4/20
    44971/44971 [==============================] - 1s 15us/step - loss: 0.2399 - val_loss: 0.2313
    Epoch 5/20
    44971/44971 [==============================] - 1s 15us/step - loss: 0.2282 - val_loss: 0.2258
    Epoch 6/20
    44971/44971 [==============================] - 1s 15us/step - loss: 0.2238 - val_loss: 0.2224
    Epoch 7/20
    44971/44971 [==============================] - 1s 15us/step - loss: 0.2196 - val_loss: 0.2175
    Epoch 8/20
    44971/44971 [==============================] - 1s 15us/step - loss: 0.2154 - val_loss: 0.2146
    Epoch 9/20
    44971/44971 [==============================] - 1s 15us/step - loss: 0.2128 - val_loss: 0.2124
    Epoch 10/20
    44971/44971 [==============================] - 1s 15us/step - loss: 0.2103 - val_loss: 0.2099
    Epoch 11/20
    44971/44971 [==============================] - 1s 15us/step - loss: 0.2080 - val_loss: 0.2079
    Epoch 12/20
    44971/44971 [==============================] - 1s 15us/step - loss: 0.2060 - val_loss: 0.2059
    Epoch 13/20
    44971/44971 [==============================] - 1s 15us/step - loss: 0.2043 - val_loss: 0.2046
    Epoch 14/20
    44971/44971 [==============================] - 1s 16us/step - loss: 0.2028 - val_loss: 0.2029
    Epoch 15/20
    44971/44971 [==============================] - 1s 15us/step - loss: 0.2015 - val_loss: 0.2020
    Epoch 16/20
    44971/44971 [==============================] - 1s 15us/step - loss: 0.2003 - val_loss: 0.2006
    Epoch 17/20
    44971/44971 [==============================] - 1s 15us/step - loss: 0.1992 - val_loss: 0.1993
    Epoch 18/20
    44971/44971 [==============================] - 1s 15us/step - loss: 0.1981 - val_loss: 0.1985
    Epoch 19/20
    44971/44971 [==============================] - 1s 15us/step - loss: 0.1972 - val_loss: 0.1978
    Epoch 20/20
    44971/44971 [==============================] - 1s 15us/step - loss: 0.1964 - val_loss: 0.1970
    Train on 44971 samples, validate on 4997 samples
    Epoch 1/20
    44971/44971 [==============================] - 1s 24us/step - loss: 1.8934 - acc: 0.3156 - val_loss: 1.7898 - val_acc: 0.3276
    Epoch 2/20
    44971/44971 [==============================] - 0s 9us/step - loss: 1.7710 - acc: 0.3275 - val_loss: 1.7512 - val_acc: 0.3276
    Epoch 3/20
    44971/44971 [==============================] - 0s 9us/step - loss: 1.7220 - acc: 0.3390 - val_loss: 1.7077 - val_acc: 0.3620
    Epoch 4/20
    44971/44971 [==============================] - 0s 8us/step - loss: 1.6900 - acc: 0.3771 - val_loss: 1.6896 - val_acc: 0.3850
    Epoch 5/20
    44971/44971 [==============================] - 0s 8us/step - loss: 1.6785 - acc: 0.3856 - val_loss: 1.6814 - val_acc: 0.3868
    Epoch 6/20
    44971/44971 [==============================] - 0s 8us/step - loss: 1.6706 - acc: 0.3852 - val_loss: 1.6745 - val_acc: 0.3876
    Epoch 7/20
    44971/44971 [==============================] - 0s 8us/step - loss: 1.6610 - acc: 0.3842 - val_loss: 1.6588 - val_acc: 0.3796
    Epoch 8/20
    44971/44971 [==============================] - 0s 8us/step - loss: 1.6497 - acc: 0.3808 - val_loss: 1.6447 - val_acc: 0.3784
    Epoch 9/20
    44971/44971 [==============================] - 0s 9us/step - loss: 1.6403 - acc: 0.3819 - val_loss: 1.6370 - val_acc: 0.3786
    Epoch 10/20
    44971/44971 [==============================] - 0s 8us/step - loss: 1.6334 - acc: 0.3845 - val_loss: 1.6289 - val_acc: 0.3810
    Epoch 11/20
    44971/44971 [==============================] - 0s 8us/step - loss: 1.6290 - acc: 0.3855 - val_loss: 1.6235 - val_acc: 0.3826
    Epoch 12/20
    44971/44971 [==============================] - 0s 9us/step - loss: 1.6256 - acc: 0.3880 - val_loss: 1.6210 - val_acc: 0.3860
    Epoch 13/20
    44971/44971 [==============================] - 0s 9us/step - loss: 1.6239 - acc: 0.3890 - val_loss: 1.6269 - val_acc: 0.3844
    Epoch 14/20
    44971/44971 [==============================] - 0s 8us/step - loss: 1.6207 - acc: 0.3895 - val_loss: 1.6159 - val_acc: 0.3874
    Epoch 15/20
    44971/44971 [==============================] - 0s 8us/step - loss: 1.6190 - acc: 0.3917 - val_loss: 1.6152 - val_acc: 0.3920
    Epoch 16/20
    44971/44971 [==============================] - 0s 8us/step - loss: 1.6177 - acc: 0.3930 - val_loss: 1.6134 - val_acc: 0.3942
    Epoch 17/20
    44971/44971 [==============================] - 0s 8us/step - loss: 1.6157 - acc: 0.3938 - val_loss: 1.6070 - val_acc: 0.3970
    Epoch 18/20
    44971/44971 [==============================] - 0s 8us/step - loss: 1.6109 - acc: 0.3987 - val_loss: 1.6008 - val_acc: 0.4006
    Epoch 19/20
    44971/44971 [==============================] - 0s 8us/step - loss: 1.6072 - acc: 0.4000 - val_loss: 1.6043 - val_acc: 0.4066
    Epoch 20/20
    44971/44971 [==============================] - 0s 9us/step - loss: 1.6055 - acc: 0.4014 - val_loss: 1.5960 - val_acc: 0.4046
    8908/8908 [==============================] - 1s 87us/step
    Classification score for NN:               precision    recall  f1-score   support
    
               1       0.31      0.16      0.21       925
               2       0.30      0.05      0.09       975
               3       0.00      0.00      0.00       150
               4       0.40      0.02      0.04       199
               5       0.00      0.00      0.00       828
               6       0.27      0.53      0.36      1625
               7       0.26      0.07      0.11      1247
               8       0.53      0.84      0.65      2959
    
       micro avg       0.41      0.41      0.41      8908
       macro avg       0.26      0.21      0.18      8908
    weighted avg       0.34      0.41      0.33      8908
    
    Classification score for SOAM1:               precision    recall  f1-score   support
    
               1       0.23      0.24      0.23       925
               2       0.24      0.23      0.23       975
               3       0.42      0.23      0.30       150
               4       0.49      0.49      0.49       199
               5       0.50      0.31      0.38       828
               6       0.35      0.31      0.33      1625
               7       0.33      0.21      0.26      1247
               8       0.60      0.80      0.69      2959
    
       micro avg       0.45      0.45      0.45      8908
       macro avg       0.39      0.35      0.36      8908
    weighted avg       0.42      0.45      0.43      8908
    
    

In [37]:

    
    
    #Evalution output for Neural network
    accuracy_list_NN
    

Out[37]:

    
    
    [0.33217332734620564,
     0.3865065110013471,
     0.3673102828917827,
     0.37415806017063313,
     0.39604849573417156,
     0.4137853614728334,
     0.4106421194431971,
     0.4150202065559048,
     0.4024472384373597,
     0.41030534351145037]

In [38]:

    
    
    #Evalution output for SOAM network
    accuracy_list_SOAM1
    

Out[38]:

    
    
    [0.41546924113156714,
     0.43780871127076787,
     0.4373596766951055,
     0.43769645262685225,
     0.4410642119443197,
     0.4389312977099237,
     0.43971710821733273,
     0.4296138302649304,
     0.45363718006286485,
     0.44611585092052086]

In [39]:

    
    
    #Saving output to a file
    with open('Accuracy_NN.txt', 'w') as f:
        print(accuracy_list_NN, file=f)
    with open('Accuracy_SOAM.txt', 'w') as f:
        print(accuracy_list_SOAM1, file=f)
    
    with open('F1_score_NN.txt', 'w') as f:
        print(F1_score_list_NN, file=f)
    with open('F1_score_SOAM1.txt', 'w') as f:
        print(F1_score_list_SOAM1, file=f)
    
    with open('Precision_Score_NN.txt', 'w') as f:
        print(Precision_list_NN, file=f)
    with open('Precision_Score_SOAM1.txt', 'w') as f:
        print(Precision_list_SOAM1, file=f)
        
    with open('Recall_Recall_NN.txt', 'w') as f:
        print(Recall_list_NN, file=f)
    with open('Recall_Recall_SOAM1.txt', 'w') as f:
        print(Recall_list_SOAM1, file=f)
    

In [40]:

    
    
    #output comparitive visualization
    from matplotlib.pyplot import figure
    plt.figure(figsize=(15, 5))
    plt.plot(accuracy_list_NN,label='NN')
    plt.plot(accuracy_list_SOAM1,label='SOAM1')
    #plt.plot([10,20], accuracy_list_SOAM3,label='SOAM2')
    plt.legend(loc='lower right')
    plt.xlabel("data fraction")
    plt.ylabel("Accuaracy Value")
    

Out[40]:

    
    
    Text(0, 0.5, 'Accuaracy Value')

![](__results___files/__results___45_1.png)

