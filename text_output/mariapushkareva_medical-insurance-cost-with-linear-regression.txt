Which Factors Influence the Price of Health Insurance?

![](https://i.imgur.com/zTnvOcb.jpg)

Many factors that affect how much you pay for health insurance are not within
your control. Nonetheless, it's good to have an understanding of what they
are. Here are some factors that affect how much health insurance premiums cost

  * **age:** age of primary beneficiary

  * **sex:** insurance contractor gender, female, male

  * **bmi:** Body mass index, providing an understanding of body, weights that are relatively high or low relative to height, objective index of body weight (kg / m ^ 2) using the ratio of height to weight, ideally 18.5 to 24.9

  * **children:** Number of children covered by health insurance / Number of dependents

  * **smoker:** Smoking

  * **region:** the beneficiary's residential area in the US, northeast, southeast, southwest, northwest

EDA and Visualizations

In [1]:

    
    
    import pandas as pd
    import numpy as np
    import matplotlib.pyplot as plt
    %matplotlib inline
    import seaborn as sns
    import warnings
    warnings.filterwarnings('ignore')
    

In [2]:

    
    
    df = pd.read_csv('../input/insurance.csv')
    df.head()
    

Out[2]:

| age | sex | bmi | children | smoker | region | charges  
---|---|---|---|---|---|---|---  
0 | 19 | female | 27.900 | 0 | yes | southwest | 16884.92400  
1 | 18 | male | 33.770 | 1 | no | southeast | 1725.55230  
2 | 28 | male | 33.000 | 3 | no | southeast | 4449.46200  
3 | 33 | male | 22.705 | 0 | no | northwest | 21984.47061  
4 | 32 | male | 28.880 | 0 | no | northwest | 3866.85520  
  
In [3]:

    
    
    df.shape
    

Out[3]:

    
    
    (1338, 7)

In [4]:

    
    
    df.describe()
    

Out[4]:

| age | bmi | children | charges  
---|---|---|---|---  
count | 1338.000000 | 1338.000000 | 1338.000000 | 1338.000000  
mean | 39.207025 | 30.663397 | 1.094918 | 13270.422265  
std | 14.049960 | 6.098187 | 1.205493 | 12110.011237  
min | 18.000000 | 15.960000 | 0.000000 | 1121.873900  
25% | 27.000000 | 26.296250 | 0.000000 | 4740.287150  
50% | 39.000000 | 30.400000 | 1.000000 | 9382.033000  
75% | 51.000000 | 34.693750 | 2.000000 | 16639.912515  
max | 64.000000 | 53.130000 | 5.000000 | 63770.428010  
  
In [5]:

    
    
    df.dtypes
    

Out[5]:

    
    
    age           int64
    sex          object
    bmi         float64
    children      int64
    smoker       object
    region       object
    charges     float64
    dtype: object

In [6]:

    
    
    df.isnull().sum()
    

Out[6]:

    
    
    age         0
    sex         0
    bmi         0
    children    0
    smoker      0
    region      0
    charges     0
    dtype: int64

We have 0 missing values which is very good. Now let's do EDA with some cool
graphs :) First we'll see how the charges are distributed according to given
factors

In [7]:

    
    
    sns.set(style='whitegrid')
    f, ax = plt.subplots(1,1, figsize=(12, 8))
    ax = sns.distplot(df['charges'], kde = True, color = 'c')
    plt.title('Distribution of Charges')
    

Out[7]:

    
    
    Text(0.5, 1.0, 'Distribution of Charges')

![](__results___files/__results___11_1.png)

This distribution is right-skewed. To make it closer to normal we can apply
natural log

In [8]:

    
    
    f, ax = plt.subplots(1, 1, figsize=(12, 8))
    ax = sns.distplot(np.log10(df['charges']), kde = True, color = 'r' )
    

![](__results___files/__results___13_0.png)

Now let's look at the charges by region

In [9]:

    
    
    charges = df['charges'].groupby(df.region).sum().sort_values(ascending = True)
    f, ax = plt.subplots(1, 1, figsize=(8, 6))
    ax = sns.barplot(charges.head(), charges.head().index, palette='Blues')
    

![](__results___files/__results___15_0.png)

So overall the highest medical charges are in the Southeast and the lowest are
in the Southwest. Taking into account certain factors (sex, smoking, having
children) let's see how it changes by region

In [10]:

    
    
    f, ax = plt.subplots(1, 1, figsize=(12, 8))
    ax = sns.barplot(x='region', y='charges', hue='sex', data=df, palette='cool')
    

![](__results___files/__results___17_0.png)

In [11]:

    
    
    f, ax = plt.subplots(1,1, figsize=(12,8))
    ax = sns.barplot(x = 'region', y = 'charges',
                     hue='smoker', data=df, palette='Reds_r')
    

![](__results___files/__results___18_0.png)

In [12]:

    
    
    f, ax = plt.subplots(1, 1, figsize=(12, 8))
    ax = sns.barplot(x='region', y='charges', hue='children', data=df, palette='Set1')
    

![](__results___files/__results___19_0.png)

As we can see from these barplots the highest charges due to smoking are still
in the Southeast but the lowest are in the Northeast. People in the Southwest
generally smoke more than people in the Northeast, but people in the Northeast
have higher charges by gender than in the Southwest and Northwest overall. And
people with children tend to have higher medical costs overall as well

Now let's analyze the medical charges by age, bmi and children according to
the smoking factor

In [13]:

    
    
    ax = sns.lmplot(x = 'age', y = 'charges', data=df, hue='smoker', palette='Set1')
    ax = sns.lmplot(x = 'bmi', y = 'charges', data=df, hue='smoker', palette='Set2')
    ax = sns.lmplot(x = 'children', y = 'charges', data=df, hue='smoker', palette='Set3')
    

![](__results___files/__results___22_0.png)

![](__results___files/__results___22_1.png)

![](__results___files/__results___22_2.png)

Smoking has the highest impact on medical costs, even though the costs are
growing with age, bmi and children. Also people who have children generally
smoke less, which the following violinplots shows too

In [14]:

    
    
    f, ax = plt.subplots(1, 1, figsize=(10, 10))
    ax = sns.violinplot(x = 'children', y = 'charges', data=df,
                     orient='v', hue='smoker', palette='inferno')
    

![](__results___files/__results___24_0.png)

In [15]:

    
    
    ##Converting objects labels into categorical
    df[['sex', 'smoker', 'region']] = df[['sex', 'smoker', 'region']].astype('category')
    df.dtypes
    

Out[15]:

    
    
    age            int64
    sex         category
    bmi          float64
    children       int64
    smoker      category
    region      category
    charges      float64
    dtype: object

In [16]:

    
    
    ##Converting category labels into numerical using LabelEncoder
    from sklearn.preprocessing import LabelEncoder
    label = LabelEncoder()
    label.fit(df.sex.drop_duplicates())
    df.sex = label.transform(df.sex)
    label.fit(df.smoker.drop_duplicates())
    df.smoker = label.transform(df.smoker)
    label.fit(df.region.drop_duplicates())
    df.region = label.transform(df.region)
    df.dtypes
    

Out[16]:

    
    
    age           int64
    sex           int64
    bmi         float64
    children      int64
    smoker        int64
    region        int64
    charges     float64
    dtype: object

In [17]:

    
    
    f, ax = plt.subplots(1, 1, figsize=(10, 10))
    ax = sns.heatmap(df.corr(), annot=True, cmap='cool')
    

![](__results___files/__results___27_0.png)

No correlation, except with the smoking

Linear Regression

In [18]:

    
    
    from sklearn.model_selection import train_test_split as holdout
    from sklearn.linear_model import LinearRegression
    from sklearn import metrics
    x = df.drop(['charges'], axis = 1)
    y = df['charges']
    x_train, x_test, y_train, y_test = holdout(x, y, test_size=0.2, random_state=0)
    Lin_reg = LinearRegression()
    Lin_reg.fit(x_train, y_train)
    print(Lin_reg.intercept_)
    print(Lin_reg.coef_)
    print(Lin_reg.score(x_test, y_test))
    
    
    
    -11661.983908824413
    [  253.99185244   -24.32455098   328.40261701   443.72929547
     23568.87948381  -288.50857254]
    0.7998747145449959
    

The result we got is good enough, but we can try to improve it a bit by
reducing unimportant features later

Ridge Regression

In [19]:

    
    
    from sklearn.linear_model import Ridge
    Ridge = Ridge(alpha=0.5)
    Ridge.fit(x_train, y_train)
    print(Ridge.intercept_)
    print(Ridge.coef_)
    print(Ridge.score(x_test, y_test))
    
    
    
    -11643.440927495818
    [ 2.53893751e+02 -2.15112284e+01  3.28339566e+02  4.44238477e+02
      2.35009674e+04 -2.89027871e+02]
    0.799698963206314
    

Lasso Regression

In [20]:

    
    
    from sklearn.linear_model import Lasso
    Lasso = Lasso(alpha=0.2, fit_intercept=True, normalize=False, precompute=False, max_iter=1000,
                  tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')
    Lasso.fit(x_train, y_train)
    print(Lasso.intercept_)
    print(Lasso.coef_)
    print(Lasso.score(x_test, y_test))
    
    
    
    -11661.838929039533
    [ 2.53991436e+02 -2.34569821e+01  3.28389438e+02  4.43587436e+02
      2.35676136e+04 -2.88340296e+02]
    0.7998690236224706
    

Random Forest Regressor

In [21]:

    
    
    from sklearn.ensemble import RandomForestRegressor as rfr
    x = df.drop(['charges'], axis=1)
    y = df.charges
    Rfr = rfr(n_estimators = 100, criterion = 'mse',
                                  random_state = 1,
                                  n_jobs = -1)
    Rfr.fit(x_train,y_train)
    x_train_pred = Rfr.predict(x_train)
    x_test_pred = Rfr.predict(x_test)
    
    print('MSE train data: %.3f, MSE test data: %.3f' % 
          (metrics.mean_squared_error(x_train_pred, y_train),
           metrics.mean_squared_error(x_test_pred, y_test)))
    print('R2 train data: %.3f, R2 test data: %.3f' % 
          (metrics.r2_score(y_train,x_train_pred, y_train),
           metrics.r2_score(y_test,x_test_pred, y_test)))
    
    
    
    MSE train data: 3630549.354, MSE test data: 19737210.132
    R2 train data: 0.971, R2 test data: 0.877
    

In [22]:

    
    
    plt.figure(figsize=(8,6))
    
    plt.scatter(x_train_pred, x_train_pred - y_train,
              c = 'gray', marker = 'o', s = 35, alpha = 0.5,
              label = 'Train data')
    plt.scatter(x_test_pred, x_test_pred - y_test,
              c = 'blue', marker = 'o', s = 35, alpha = 0.7,
              label = 'Test data')
    plt.xlabel('Predicted values')
    plt.ylabel('Actual values')
    plt.legend(loc = 'upper right')
    plt.hlines(y = 0, xmin = 0, xmax = 60000, lw = 2, color = 'red')
    

Out[22]:

    
    
    <matplotlib.collections.LineCollection at 0x7fb5056c7d68>

![](__results___files/__results___38_1.png)

In [23]:

    
    
    print('Feature importance ranking\n\n')
    importances = Rfr.feature_importances_
    std = np.std([tree.feature_importances_ for tree in Rfr.estimators_],axis=0)
    indices = np.argsort(importances)[::-1]
    variables = ['age', 'sex', 'bmi', 'children','smoker', 'region']
    importance_list = []
    for f in range(x.shape[1]):
        variable = variables[indices[f]]
        importance_list.append(variable)
        print("%d.%s(%f)" % (f + 1, variable, importances[indices[f]]))
    
    # Plot the feature importances of the forest
    plt.figure()
    plt.title("Feature importances")
    plt.bar(importance_list, importances[indices],
           color="y", yerr=std[indices], align="center")
    
    
    
    Feature importance ranking
    
    
    1.smoker(0.601678)
    2.bmi(0.218751)
    3.age(0.136942)
    4.children(0.021042)
    5.region(0.015209)
    6.sex(0.006377)
    

Out[23]:

    
    
    <BarContainer object of 6 artists>

![](__results___files/__results___39_2.png)

Polynomial Regression

In [24]:

    
    
    from sklearn.preprocessing import PolynomialFeatures
    x = df.drop(['charges', 'sex', 'region'], axis = 1)
    y = df.charges
    pol = PolynomialFeatures (degree = 2)
    x_pol = pol.fit_transform(x)
    x_train, x_test, y_train, y_test = holdout(x_pol, y, test_size=0.2, random_state=0)
    Pol_reg = LinearRegression()
    Pol_reg.fit(x_train, y_train)
    y_train_pred = Pol_reg.predict(x_train)
    y_test_pred = Pol_reg.predict(x_test)
    print(Pol_reg.intercept_)
    print(Pol_reg.coef_)
    print(Pol_reg.score(x_test, y_test))
    
    
    
    -5325.881705252252
    [ 0.00000000e+00 -4.01606591e+01  5.23702019e+02  8.52025026e+02
     -9.52698471e+03  3.04430186e+00  1.84508369e+00  6.01720286e+00
      4.20849790e+00 -9.38983382e+00  3.81612289e+00  1.40840670e+03
     -1.45982790e+02 -4.46151855e+02 -9.52698471e+03]
    0.8812595703345225
    

Awesome! :)

In [25]:

    
    
    ##Evaluating the performance of the algorithm
    print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_test_pred))
    print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_test_pred))
    print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_test_pred)))
    
    
    
    Mean Absolute Error: 2824.4950454776595
    Mean Squared Error: 18895160.09878044
    Root Mean Squared Error: 4346.856346692451
    

In [26]:

    
    
    ##Predicting the charges
    y_test_pred = Pol_reg.predict(x_test)
    ##Comparing the actual output values with the predicted values
    df = pd.DataFrame({'Actual': y_test, 'Predicted': y_test_pred})
    df
    

Out[26]:

| Actual | Predicted  
---|---|---  
578 | 9724.530000 | 12101.156323  
610 | 8547.691300 | 10440.782266  
569 | 45702.022350 | 48541.022951  
1034 | 12950.071200 | 14140.067522  
198 | 9644.252500 | 8636.235727  
981 | 4500.339250 | 5072.787029  
31 | 2198.189850 | 3090.494817  
1256 | 11436.738150 | 13171.361938  
1219 | 7537.163900 | 9187.612192  
1320 | 5425.023350 | 7496.320857  
613 | 6753.038000 | 6653.904925  
1107 | 10493.945800 | 11893.766490  
1263 | 7337.748000 | 9291.317273  
406 | 4185.097900 | 5326.271479  
795 | 18310.742000 | 25726.734553  
970 | 10702.642400 | 12643.360147  
824 | 12523.604800 | 13099.011032  
141 | 3490.549100 | 5336.149644  
1173 | 6457.843400 | 8680.680007  
1042 | 33475.817150 | 27696.731856  
966 | 23967.383050 | 27202.661385  
467 | 12643.377800 | 14490.242822  
1098 | 23045.566160 | 10998.132160  
757 | 23065.420700 | 29566.723915  
1097 | 1674.632300 | 3611.941947  
319 | 4667.607650 | 6215.673009  
1286 | 3732.625100 | 3077.366908  
459 | 7682.670000 | 9772.813467  
5 | 3756.621600 | 5085.856146  
517 | 8413.463050 | 10696.238443  
... | ... | ...  
535 | 6067.126750 | 7852.517542  
853 | 11729.679500 | 12500.224437  
1014 | 5383.536000 | 6780.484833  
1186 | 37465.343750 | 35020.456915  
215 | 7371.772000 | 9353.329096  
1046 | 7325.048200 | 7794.117226  
986 | 8410.046850 | 10584.144595  
489 | 10461.979400 | 12489.567570  
968 | 3279.868550 | 4999.229874  
1160 | 7727.253200 | 9297.870472  
792 | 2731.912200 | 3299.083436  
1224 | 6858.479600 | 8156.022661  
465 | 19521.968200 | 26671.512279  
251 | 47305.305000 | 42890.820207  
1017 | 3987.926000 | 5950.457712  
1239 | 3238.435700 | 3930.573984  
427 | 7323.734819 | 3192.996621  
295 | 1704.568100 | 2778.118484  
820 | 7445.918000 | 9919.906707  
1335 | 1629.833500 | 2709.177041  
884 | 4877.981050 | 6174.548189  
326 | 3561.888900 | 5019.195968  
1109 | 8605.361500 | 9777.556588  
783 | 24520.264000 | 30818.987409  
668 | 45710.207850 | 40988.027774  
1084 | 15019.760050 | 16712.196281  
726 | 6664.685950 | 8654.565461  
1132 | 20709.020340 | 12372.050609  
725 | 40932.429500 | 41465.617268  
963 | 9500.573050 | 10941.780705  
  
268 rows Ã 2 columns

**Conclusion:** like we previously noticed **smoking** is the greatest factor
that affects medical cost charges, then it's **bmi** and **age**. **Polynomial
Regression** turned out to be the best model

